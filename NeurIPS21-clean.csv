id,url,decision,ratings,confidence,text,Filename,helpful
0,https://openreview.net/forum?id=MAWgLrYvMs0,1,7,3,"We acknowledge the ethical issues raised in section 5.2 and the demographic reporting in Figure 5. Since authors use public information for accountability purposes, consent and privacy concerns seem to be minimized, and authors had furthermore reported passing IRB requirements.
Although it seems that authors focus on the declared intended use highlighted in 5.1, it is possible for others to imagine potentially harmful use cases for this dataset, including using the information for targeted harassment or violence against specific police officers. Since the dataset could potentially reveal police identities, authors should think seriously about how to restrict and communicate throughout the data distribution process. Proposals such as a data license [1] may be a useful framework for thinking about how to approach this task, in addition to setting terms of use or requiring explicit requests for access.
[1] Benjamin, Misha, et al. ""Towards standardization of data licenses: The montreal data license."" arXiv preprint arXiv:1903.12262 (2019).",21-0.txt,0
0,https://openreview.net/forum?id=MAWgLrYvMs0,1,8,3,"In this paper the authors provide a new dataset, based on Freedom of Information Act requests, covering complaints against the Chicago Police Department. The authors detail how they process and clean the data and present an exploratory analysis based on the data. All reviewers found the paper to be a valuable contribution to the field. That said, given the sensitivity of this area (and open challenges raised by multiple reviewers), in my opinion acceptance is conditional on an ethics review.

Flagged for an additional ethics review because of data access and privacy issues.",21-1.txt,0
0,https://openreview.net/forum?id=MAWgLrYvMs0,1,8,3,"Line 211 f.: “and set as the end date of each assignment, the day immediately preceding the start date of the following assignment” --> the comma after “assignment” seems to be misplaced",21-2.txt,0
0,https://openreview.net/forum?id=MAWgLrYvMs0,1,7,3,"Very minor comment: the Jupyter notebooks in current version of the repo are ""un-run"" versions. The authors may want to consider generating some examples as HTML or PDF for some interested users.
I was also curious if there was a strong reason for not providing the final data files directly in the repo (e.g., to make sure users have the most up-to-date version?). This may be useful for some dataset users.",21-3.txt,0
1,https://openreview.net/forum?id=QzNHE7QHhut,1,6,3,"Because this is biometric information that could be perceived as sensitive, we recommend authors restrict distribution and monitor those making use of the dataset (ie. restricting access by request form, restricting possible use cases for the data, terms of use contract, etc.). Proposals such as a data license [1] may be a useful framework for thinking about how to approach this task.
There seems to be informed written consent, anonymization, participant compensation and reporting to the involved institutional review board (IRB). Authors also seem to have already reported demographic disparities (see Figure 1), and included an ethics review section (A.3), so we feel as though other issues have been acknowledged and adequately addressed.
[1] Benjamin, Misha, et al. ""Towards standardization of data licenses: The montreal data license."" arXiv preprint arXiv:1903.12262 (2019).",21-4.txt,0
1,https://openreview.net/forum?id=QzNHE7QHhut,1,9,3,"This submission puts forward the largest dataset of fNIRS recordings labelled by mental workload indicators. The paper is well written and the dataset is well documented. Furthermore, the authors include a benchmark and a comprehensive and commendable discussion of the auditability of the dataset and the importance of including a range of diverse participants in the data collection process. This work can have significant impact within the BCI community. For this reason I recommend acceptance.

Flagged for an additional ethics review because this dataset is collected from humans.",21-5.txt,0
1,https://openreview.net/forum?id=QzNHE7QHhut,1,7,2,"In Section 3.2, Cross-subject pipelines, you might also include recent work on contrastive learning to improve cross-subject generalization in biosignal classification: JY Cheng, et al. “Subject-aware contrastive learning for biosignals,” arXiv:2007.04871 [cs.LG]
In Figure B.2: For clarity, it would be useful to label the axes as “mean accuracy”",21-6.txt,0
2,https://openreview.net/forum?id=7Bywt2mQsCe,1,8,4,"For dataset papers including some level of human participation, it is important to report the details of the group of involved participants. This includes the expertise level, and demographics of the involved humans, as well as information on the conditions and context of their contribution (ie. payment, experimental set up details, etc.). If necessary, authors should submit the study to their institutional review board (IRB) process to see if they are exempt or need to take further measures.",21-7.txt,0
2,https://openreview.net/forum?id=7Bywt2mQsCe,1,6,4,"This paper presents the MATH dataset that consists of math solving problems of high-school competition level. Overall, all reviewers acknowledge the significance and quality of the dataset as well as the newly-posed challenges. Although some concerns are also raised regarding the paper writing and clarification in some aspects, I think overall the paper provides enough contribution for the dataset track.

Flagged for an additional ethics review because this is a study with human participants, and the concerns raised by reviewers where not sufficiently addressed.",21-8.txt,0
3,https://openreview.net/forum?id=G1muTb5zuO7,1,6,4,"We thank the authors for their clarifications. It is clear from the discussion between the reviewers and the authors that the authors acknowledged and had a fine interaction about the ethics issues. If this discussion is also reflected in the paper itself, we don't see any additional issues with publishing this work.",21-9.txt,1
3,https://openreview.net/forum?id=G1muTb5zuO7,1,8,3,"The paper presents a benchmark dataset of annotations as to the morality or lack thereof in 25 text-based adventure games (drawn from the existing Jericho benchmark) for every possible action that a player can take, with a goal of training an agent to perform well on the game while simultaneously making moral choices. A fine-tuned pretrained language model approach is provided as a baseline approach. The annotation process is clear, and the motivation is excellent. In addition, the annotations represent a significant amount of work, as they are quite dense. The result from the provided baseline that policy shaping is more effective than reward shaping is interesting and worthy of future exploration.

Several reviewers raise the concern that the core concept of ""morality"" used for annotation here is treated as self-evident and innate, when it is well demonstrated that this is inaccurate. The set of annotators is drawn from a very specific pool, which may serve to focus existing cultural biases, and background information that would help clarify these biases is lacking. Additionally, the work is not well situated with respect to existing text game-playing agents, making the provided baseline difficult to evaluate. However, the benchmark represents a significant amount of work on a difficult problem, and the authors have engaged effectively with reviewers on the content of the paper. Authors should ensure that these discussions are incorporated in subsequent versions of the work. In particular, the authors should clearly discuss the contextual nature of the ethical judgments represented, acknowledging the use limitations and lack of objective certainty that implies.",21-10.txt,1
3,https://openreview.net/forum?id=G1muTb5zuO7,1,5,5,"This paper requires a greatly expanded limitations discussion that addresses the contextuality and cultural bounded-ness of moral reasoning, the limitations of using graduate students to code moral decisions, and the moral frame in which game play occurs.",21-11.txt,1
3,https://openreview.net/forum?id=G1muTb5zuO7,1,6,5,"Would like to note that Jiminy Cricket is a proxy swear/oath for ""Jesus Christ"" and not just the cartoon character (though I'm sure the authors are aware of this).",21-12.txt,1
3,https://openreview.net/forum?id=G1muTb5zuO7,1,7,3,"While this does not affect the scientific value of the contribution, I think there is a natural question that the reader may ask themselves (at least, I did) and should be answered in the paper: why Infocom games? These are games written in a quite ancient programming language, and their copyright is unclear as they are abandonware (as the authors rightly point out in the supplementary material). Wouldn't it have been easier to use more modern games, written in more modern languages like Inform, and which are often open-source (or permission could be seeked from the authors when they aren't)? I would like to know (and I suppose, many interested readers would) what the reason for this choice is: is it that modern games tend to be shorter or more linear than Infocom games?
In any research dealing with language, we should specify what language we are using - the paper should explicitly say somewhere that this is a dataset of English games and all the models operate on English.
I spotted a couple of minor typos/inconsistencies:
l. 74: markov -> Markov
l. 116-117: ""For instance, in Figure 1 an agent decides to take an object from a non-player character, who resists the attempted theft"" -> according to the figure, the NPC had been killed, so they could not resist the theft.",21-13.txt,1
4,https://openreview.net/forum?id=zfMtM7HZGLT,1,7,4,"It seems the authors have addressed the following issues in the “Limitations and broader impact” section of their paper:
Use of the depreciated Duke MTMC dataset
Acknowledgement for potential for surveillance use
Acknowledgement of privacy and consent violations in data collection regarding identification objectives
Acknowledgement of demographic representation issues with large human-centered datasets
They should add citations and further elaborate on the risks of demographic homogeneity in these datasets [1] but otherwise that section adequately covers the topics we hoped to see them mention explicitly. If relevant, authors should also consider any potential data distribution or copyright limitations that may interfere with their ability to make use of the dataset and should be conscientious in disseminating their models responsibly.
[1] Merler, Michele, et al. ""Diversity in faces."" arXiv preprint arXiv:1901.10436 (2019).",21-14.txt,0
4,https://openreview.net/forum?id=zfMtM7HZGLT,1,6,5,"This paper studies the performance of re-ID models on corrupted person images with a new benchmark. Generally, all reviewers acknowledge the motivation and importance of the study, and find it comprehensive enough to serve as a good benchmark paper. Apart from some correctable writing and clarity issues, a critical concern raised by a reviewer is the ethical issue of containing a previous dataset (DukeMTMC) which is already retracted. The authors properly addressed this issue by omitting DukeMTMC from the work. The reviewer was satisfied with this action and raised the score to acceptance.
Now the paper is supported by all reviewers, thus I recommend acceptance.

Flagged for an additional ethics review because this is a study on person reidentification.",21-15.txt,0
5,https://openreview.net/forum?id=b3Zoeq2sCLq,1,7,4,"It is unclear if this dataset was collected with informed consent - although volunteers signed an agreement for open-sourcing their data. Depending on the open-source licenses being used, such cases can still be ambiguous in the context of machine learning [1]. If possible, it would be great for authors to confirm having gone through some internal institutional review board (IRB) process, or otherwise being deemed exempt.
As this dataset involves sensitive biometric data, we encourage authors to restrict data distribution to prevent malicious use (ie. restricting access by request form, restricted possible use cases for the data, terms of use contract, etc.). Even though anonymized, it is important for authors to monitor how such a dataset is being used and leveraged by other parties and control outsider access to this information.
Although anonymized, malicious actors could still potentially reconstruct certain personal details. It is thus important for authors to include some explicit reflection on the potential risks of the dataset, including the potential for de-anonymization and its use in the development of applications for surveillance.
[1]https://creativecommons.org/2021/03/04/should-cc-licensed-content-be-used-to-train-ai-it-depends/",21-16.txt,0
5,https://openreview.net/forum?id=b3Zoeq2sCLq,1,8,3,"This paper introduces a speech recognition dataset for eight standard sub-dialects of Mandarin.

The paper is well-written. The overall feedback from the reviewers were positive. However, reviewers pointed out some of the concerns and weaknesses of the paper. I think the authors addressed some of them well during the rebuttal. I recommend the authors to address them all.

The Reviewer haxc and 91VF have raised some ethical concerns about this paper. The paper collected the speech dataset from the volunteers and certain details about the data collection procedure are not very clear in the paper as pointed out by the reviewers. I would recommend this paper for an ethical review.

Flagged for an additional ethics review because it is not sufficiently clear how volunteers were recruited or incentivized to participate in this study or what protocols were used to get their consent to include their speech in this dataset.",21-17.txt,0
5,https://openreview.net/forum?id=b3Zoeq2sCLq,1,7,3,"You should probably cite: https://openreview.net/forum?id=R8CwidgJ0yT
Generally this is really cool work and I'd like for it to get published. Things your team could do that would change my rating and help push it up above publishing threshold:
Align the audio and transcription to enable ASR (or convince me it's not necessary)
More detail on label quality to help future users
Hosting and maintenance plan
Noisy vs. clean on dialects (I know this is a big lift and may not be possible)
Use a CC-BY license
This has the possibility of being a 6-8 with some additional work.
I've upgraded to 7.",21-18.txt,0
6,https://openreview.net/forum?id=6nblryHxVbO,1,4,2,"In this paper the authors present a new dataset for wildfires spread with features including weather and topography. Further, they offer relatively simple baselines to predict the spread of wildfires. While there was a wide range of perspectives from reviewers, all reviewers largely agreed with the construction of the dataset as well as the clarity and validity of the work. While there were some concerns about the breadth of study in the ML community and the need for growth of research in this area (both for even more data and more models), the paper still contributes enough for it to be valuable to the academic community. As such, and after consulting with a second AC, I recommend acceptance and also encourage the authors to expand the paper in the ways suggested by the reviewers.",21-19.txt,0
6,https://openreview.net/forum?id=6nblryHxVbO,1,7,5,"Wildfire is extremely important and so the frame is on point. I don't understand why these are the variables that are included at the expense of others though. Are there additional people or papers who are dissatisfied with the current state of affairs so that they can cite/subtantiate this comment, which seems to be the premise of the dataset collection? ""While such [physics-based ]models are widely used, prediction of fire spread can be improved by a large set of covariates""",21-20.txt,0
6,https://openreview.net/forum?id=6nblryHxVbO,1,4,4,"For the evaluation with neural network, it may be useful to compare different backbone architectures.
Please compare with other open-source datasets for similar problems.
Will there be an official leader board maintained by the creators of the dataset?",21-21.txt,0
7,https://openreview.net/forum?id=tyn3MYS_uDT,1,6,4,"This scores for this paper are still marginal. However, the authors have addressed and responded to many of the issues raised by reviewers including expanding a related work section to include relevant prior literature in this area. After considering the author rebuttal and discussing with another AC, and given this important problem space for which quality evaluation datasets would be valuable, I recommend acceptance.",21-22.txt,0
7,https://openreview.net/forum?id=tyn3MYS_uDT,1,6,2,"The authors provide a dataset for off policy evaluation in bandit models. The motivation and writing could use a bit of work since its not immediately clear how the benchmark provides something significantly new and needed in the field.
Update after Rebuttal: I have increased the score to 6 based on the points raised by the authors during the rebuttal.",21-23.txt,0
7,https://openreview.net/forum?id=tyn3MYS_uDT,1,6,4,"At line 188, it is mentioned that alternative software packages cannot be used to implement some advanced OPE estimators. Does it mean that (unlike OBP) they do not provide interfaces flexible enough for implementing such advanced OPE estimators?
At the end of Section 6, an assumption on the reward structure is mentioned as a limitation. As far as I understood, this is a limitation of the experimental results presented in the paper and not an inherit limitation of OBD. Is that correct?",21-24.txt,0
8,https://openreview.net/forum?id=vzb0f0TIVlI,1,6,3,The reviews are mostly positive. The dataset is much larger than existing ones and is likely to be a valuable resource. The authors have addressed the concerns satisfactorily. I am happy to recommend acceptance.,21-25.txt,0
8,https://openreview.net/forum?id=vzb0f0TIVlI,1,6,4,"About text type caption text.
In real-life applications, what is the use case that we will need to use video-text spotting methods to get the caption texts? Wouldn't ASR be easier?
In some cases, there could be caption text in videos without audio. Would it be more useful to annotate caption texts that are not extractable by ASR differently?",21-26.txt,0
9,https://openreview.net/forum?id=bLBIbVaGDu,1,6,3,"I would gladly have decided ""7: Good paper, accept"" instead of ""6: Marginally above acceptance threshold"" if Table 1 have provided a stronger overview and comparison w.r.t. to other existing datasets. I'd also like to know why other datasets (especially, iSAID) are not considered.",21-27.txt,0
9,https://openreview.net/forum?id=bLBIbVaGDu,1,7,3,"In 1(b) of checklist, ""more effective methods need to be designed"" is not the limitation of the introduced dataset.
I think that the privacy and security problems should be described in 1(c) of checklist.
more analysis of experiments results can be added to make contributions to the deep learning community.",21-28.txt,0
10,https://openreview.net/forum?id=Fkpr2RYDvI1,1,7,4,"The reviewers agreed that this is an interesting idea that was clearly presented. There were a number of minor concerns from the reviewers regarding how AI-human collaboration affects dataset bias, but these were clarified in the discussion. Exciting and promising direction. I recommend it is accepted.",21-29.txt,0
10,https://openreview.net/forum?id=Fkpr2RYDvI1,1,7,4,"Other questions: Lines 148-151: Questions about few-shot examples:
Q6: How were the few shot examples authored? Did humans generate attributes and bios from scratch, or was a different procedure followed? Please elaborate.
Q7: Were the same few shot examples used for all the biographies in the dataset for a given notability type?
Q8: Were any experiments conducted on the number of few shot examples needed for reasonable performance or on the effect of using different sets of few-shot examples to generate the bios?
Q9: Will all of the human authored few shot examples be included in the dataset release?",21-30.txt,0
10,https://openreview.net/forum?id=Fkpr2RYDvI1,1,7,4,"The main stated reasons for creating SynthBio (in comparison to WikiBio) is a) to sidestep the problem of memorisation of large LMs and b) to control for noise and biases of naturalistic data. These should be weighed against the cost of creating the dataset (and tradeoffs between size and quality) as opposed to e.g. editing/augmenting the original WikiBio. This is the first missing comparison: what if instead of LM-generated attributes/bios, the annotators corrected the original Wikipedia bios? This of course would leave the problems of skewed attribute distributions, but that would be possible to account for with different test splits each with different criteria (geography, gender, occupation). Beyond the comparison to editing WikiBio, it is interesting that the results in section 4 (lines 205-210) suggest that these biases of WikiBio don't seem to affect the performance of the trained models w.r.t these attributes (of course it's still a useful diagnostic). Finally, regarding the element of memorisation, there is a missing comparison with a non-fine-tuned version of T5. This would demonstrate how much of the performance on WikiBio is due to pure memorisation (the kind shown in Table 2) compared to memorisation due to overfitting.
Regarding the edits done by the annotators, it would be good to get an idea of the distribution of edit types (deleting hallucinated information, adding missing information, fixing grammar or typos, etc).",21-31.txt,0
10,https://openreview.net/forum?id=Fkpr2RYDvI1,1,7,3,"The notes are given humbly to improve the work. With its primary goal, it can be improved into a strong and influential study. I suggest changing the case study to a less complicated one where you can accurately analyze the impact of your work.",21-32.txt,0
11,https://openreview.net/forum?id=bgWHz41FMB7,1,5,3,"This paper presents the RAFT which is a real-world few-shot text classification benchmark containing 11 sub tasks from different domains. This paper received borderline scores of (5,5,6,6). While all reviewers basically acknowledged that the dataset and the tasks are well-designed, the main concern was the lack of baselines other than GPT-3 (the result on GPT-3 itself is appreciated, though). Given the review, the authors updated the paper with the results on many other baseline models, which was appreciated and two reviewers agreed to raise the score from 5 to 6, although one of them is not reflected in the system. Thus all reviewers are now in the positive side.
Overall, there is not yet a particularly strong support for this paper, the AC considers it has no critical weakness and above the acceptance threshold.",21-33.txt,0
11,https://openreview.net/forum?id=bgWHz41FMB7,1,6,3,"I do not consider it a weakness, but the number of examples seems arbitrary at the moment – even though you refer to ""authors’ experience with users of the classification tool."" If you are interested in a few-shot evaluation and 50 is a limit, why not provide other train sets with, e.g., 5, 10, and 25 examples? I expect people to use the benchmark in this manner anyway. Moreover, the inclusion of Banking77 with a larger number of classes than training examples is controversial – it is largely a zero-shot scenario where different architectures are supposed to work well.
Typo:
There is a space before the dot in #144.",21-34.txt,1
12,https://openreview.net/forum?id=K7ke_GZ_6N,1,7,3,"The paper presents a new form of datasheets specific to artistic datasets, with the intent of addressing a set of standard questions pertaining to the decisions made by the curators of those datasets. The contribution beyond the seminal ""Datasheets for datasets"" work is domain specificity--a number of questions are raised that are specific to creative endeavors, e.g., provenance. The datasheet and two example use cases are presented.

The paper is well written and described a topic that is appropriate for the special track, but relatively niche. The set of topics covered by the datasheet is well chosen and thorough, and the work fits into the highly important area of data ethics; the existence of the relevant datasheet may help creators of such datasets to seriously consider some of the ethical implications of their work. In terms of weaknesses, the paper could be better tied into the literature, which would help tie it into the broader machine learning discussion on data ethics. More critically, reviewers are concerned that the expertise necessary to correctly fill in the datasheet is extremely broad, to the point that it may provide dataset curators with a false sense of awareness of individual topics when additional domain expertise is really required.

Overall, reviewers agree that this is a clear paper on an important, if domain limited, topic. The authors' responses to the concerns raised are detailed and thorough.",21-35.txt,1
12,https://openreview.net/forum?id=K7ke_GZ_6N,1,5,4,"The authors state that they are integrating the framework proposed in Datasheets for Datasets, which is interesting, but not sufficient to represent a breakthrough in the field. A literature review on ethical data labeling is missing; for this purpose, some of the most interesting works published on conferences such as FAccT or AIES could have been cited (for example: https://datanutrition.org/, https://doi.org/10.1145/3025453.3026044, https://doi.org/10.1145/3442188.3445940). To facilitate understanding about the workflow, the authors could have included a summary table of the sections they modified from the reference work (Datasheets for Datasets).",21-36.txt,1
12,https://openreview.net/forum?id=K7ke_GZ_6N,1,7,4,"It would be great if the authors could provide the questions in one place for those who want to use artsheets. Extra cool points if the authors put it in a format that is convenient for art dataset curators, for example taking into consideration where the data is housed (e.g., Github).",21-37.txt,1
12,https://openreview.net/forum?id=K7ke_GZ_6N,1,7,5,This paper suggests an ambitious undertaking for any dataset curator. Suggest how curators might make tradeoffs between the effort required to fully complete this datasheet and the risks of failing to do so.,21-38.txt,1
13,https://openreview.net/forum?id=KVMS3fl4Rsv,1,4,3,"The authors are proposing a dataset of spiking neural data recorded primarily from the motor areas of the primate brain while the primates are completing various reaching tasks. The neural recordings are also supplemented with hand kinematics data for the ground truth behavioural outcome of the neural activity. The authors have put together a nice evaluation challenge with private heldout data that will never be accessible to the users of their datasets, but which will be used privately on their servers for evaluating the submitted models. The dataset/leaderboard/competition setup can be a great way to push forward the developments of more accurate unsupervised models of spike activity.

The reviews are very mixed (4, 7, 5, 7) and all reviewers have quite low confidence. The main drawbacks brought up by the reviewers are:

Wrong/too-broad use of the term Latent Variable Model (LVM)
Lack of more traditional to the ML literature evaluation measures for the learnt latent spaces
Lack of statistical analysis of the results - are there statistically meaningful differences?
Is it fair for the competition to allow users access to the behavioural data - what if some models use this supervision signals while others don't?
The authors have addressed all of these points reasonably in my opinion during the discussion period. They have agreed to change how they refer to the baselines (not LVMs), they have added statistical tests, they have added a fair discussion on how the more traditional analysis of the learnt latents is not applicable in their use case, and how access to the behavioural data does not help these models pass all of their evaluation tests.

Having read the paper myself, and coming from the neuroscience background, I believe that the paper merits publication. It will be a great resource to encourage competition in developing good unsupervised learning methods for spiking neural data (which is being collected in more and more volume with the emergence of the better large scale recording methods), and gaining a better understanding of the latent dynamics in these populations of neurons. For this reason, I recommend acceptance.",21-39.txt,1
13,https://openreview.net/forum?id=KVMS3fl4Rsv,1,7,3,"""MC_Maze as a Neuroscience MNIST"" is repeated several times. MNIST is important to the history of deep learning of course, but IMO these days it is seen as an over-used / unrealistic benchmark with rampant over-fitting to the test set.",21-40.txt,1
13,https://openreview.net/forum?id=KVMS3fl4Rsv,1,5,2,It would be nice to see the discrete latents associated with SLDS on the provided data to see if some discrete interpretable latents are being observed on it.,21-41.txt,1
14,https://openreview.net/forum?id=NfTU-wN8Uo,1,6,5,"This work constructs the largest annotated 10 German abusive language comment datasets and performs a comprehensive analysis. Although reviewers raise their concerns from different aspects, the authors have made convincing replies. I recommend it is accepted, and authors should continue to refine the draft based on the rebuttal comments.",21-42.txt,0
14,https://openreview.net/forum?id=NfTU-wN8Uo,1,6,3,"The authors should reconsider the placement of Figure 3 in between of two paragraphs, neither of which relates to Figure 3.
For the format of this submission, I would have expected more detailed documentation of the data labeling, and less focus on the preliminary experiments (which also are not a strength of the paper...)",21-43.txt,0
15,https://openreview.net/forum?id=QVeMBoRXAo_,1,5,4,"The paper introduces a new dataset of ~9.5k images of churches annotated with fine-grained and hierarchical labels of architectural style. Reviewers felt that the dataset was novel and well-described, and would be an interesting testbed for fine-grained recognition. The primary weakness mentioned by reviewers was that the paper does not provide sufficient baseline classification results on the dataset. The authors provided additional results in their response to Reviewer roSv. In addition, the paper does not justify the utility of the bounding box annotations. On the whole, the AC feels that the benefits of this dataset for fine-grained classification outweigh the weaknesses. Congratulations on having your paper accepted to the NeurIPS 2021 Datasets & Benchmarks Track! The authors are encouraged to take the feedback from reviewers into account when preparing the final version of the paper.",21-44.txt,1
16,https://openreview.net/forum?id=vhjsBtq9OxO,1,9,4,"The data provided in the paper is of relevance for the NeurIPS data track. Based on reviewers opinions, and in particular after discussion with the authors, the paper achieves the minimum score required for publication at NeurIPS data track.",21-45.txt,0
16,https://openreview.net/forum?id=vhjsBtq9OxO,1,8,3,"The paper is a resubmission from Round 1 that has been significantly improved based on the comments of the reviewers of Round 1. In my opinion, the comments have been carefully addressed, and the dataset could be used by researchers to investigate this important problem. My main limitation concerns the relevance of the dataset for the machine learning community as discussed in the Weakness section, and I would be happy to hear from the authors about the technical barriers connected to the dataset they identified in terms of methodology, beyond the application domain.
Update after discussions with authors: I have reconsidered my position regarding the relevance of the dataset for the conference (see comments below).
Other comments
The first sentence states that mosquitoes are not present in Antarctica, with a reference to a 2013 publication. Is there any more up-to-date publication to confirm this is still the case with the current rise of temperature?
According to WHO website, numbers for malaria are '219 millions' and 400 000 deaths, in disagreement with the numbers reported in the introduction.
Given a system that accurately detect and classify mosquitoes from a sound recording trained using the dataset, what would be the typical use of such a system in real-world conditions? In uncontrolled environment, it seems very unlikely that a mosquito will fly close enough to the microphone to trigger a detection. In that case, would you say the dataset would only be useful in the same conditions as the data collection, i.e., capturing mosquitoes and recording them?",21-46.txt,0
16,https://openreview.net/forum?id=vhjsBtq9OxO,1,8,4,"it would be beneficial to include visual examples of your data. This could be spectrograms highlighting the differences between audio clips with and without mosquitos, or different kinds of mosquitos. This would fit into the supplementary materials.",21-47.txt,0
17,https://openreview.net/forum?id=lwrPkQP_is,1,6,3,"This paper proposes a benchmark for evaluating unsupervised RL agents - i.e. agents that learn from intrinsically generated reward signals or other kinds of pre-training before being exposed to the final task. Developing such endogenous RL agents is important, however progress is hard to measure without a comprehensive benchmark, so this submission is very well placed. All of the reviewers agreed that this is a solid submission, evaluating a large number of algorithms on a good set of tasks. For this reason I recommend acceptance.",21-48.txt,0
17,https://openreview.net/forum?id=lwrPkQP_is,1,7,4,"Minor comments:
Why is the standard error not visualized in figure 4 of the paper? They are important to visualize because the validity of 1-7 observations depends on them.
In line 79, “with intrinsic rewards
rtext
” should be replaced by “with intrinsic rewards
rtint
.
The paper says: ““This suggests that we are still far as a community from efficient generalization in deep RL.” It is a bit strong of a claim. Does it need to be said?
It would be interesting to see the learning curve of the unsupervised approaches, especially during the fine-tuning process. It would be great to include these in the paper, even if just in the appendix.",21-49.txt,0
17,https://openreview.net/forum?id=lwrPkQP_is,1,6,4,"I would have appreciated more details on how the agents are learning. The results focus pretty much on the summarized end result but that intermittent training process is quite helpful to understand for readers/reviewers. I would have appreciated that as a reader.
One related topic/work that might be worth discussing is the implications of sim2real gap issues that generally plague RL. This doesn't come up anywhere, and it would be useful to discuss that.
In the presentation of the benchmark, I would have appreciated calling out the metrics a bit more in detail. Drawing out the attention that would be useful and important because a benchmark is more than just its tasks and algorithms. It is how we measure/assess ""performance.""",21-50.txt,0
17,https://openreview.net/forum?id=lwrPkQP_is,1,7,5,About observation 3 of the experiments: you say that the performance gap from state-based vs pixel-based tasks is due to the quality of the exploration mechanism. Couldn't it just be a matter of the underlying DDPG-like agent which is less efficient at learning in pixel settings w.r.t. state settings ?,21-51.txt,0
18,https://openreview.net/forum?id=Jul-uX7EV_I,1,7,3,This paper proposes a new dataset for generating texts that describe tables in CS articles. There was broad consensus among the reviewers that this work is interesting and contributes to the field of data-to-text generation in a new and challenging domain that requires performing reasoning over the tables.,21-52.txt,0
18,https://openreview.net/forum?id=Jul-uX7EV_I,1,7,4,"In CS fields, especially ML and CL, the bold letters are often used to show the best performer in the results table. It might not be the case with other domains. It would be interesting to see without the [BOLD] tag how the performance drops, and if it does how generalizable is dataset to another domain where bold letters aren't the norm.",21-53.txt,0
18,https://openreview.net/forum?id=Jul-uX7EV_I,1,7,3,"Given that evaluation with automatic metrics seem not sufficient, it appears difficult for future models to incorporate this dataset into their analysis without significant resource commitment in terms of expert human scorers (plus this opens up concerns of the scorers being different from those who scored the models in this work). This may be a problem for the general dataset-to-text generation domain, but I'm curious what the authors think of the difficulty of dataset adoption and evaluation.",21-54.txt,0
19,https://openreview.net/forum?id=Ayf90B1yESX,1,7,4,"One of the potential ways in which synthetic data generation can be helpful is for ensuring the privacy of the patients. Usually, the hospitals are not willing to share the original patient data (even de-identified). Hence, if it can be shown that the ground truth data distribution is comparable, generating synthetic data and sharing it in public can be really helpful.",21-55.txt,0
20,https://openreview.net/forum?id=bNL5VlTfe3p,1,6,3,"The paper develops a 3D scanning setup and rendering pipeline that allows for near pixel-perfect matches between photographs of the scene and renders of the scene. The paper argues that this enables researchers to develop models for various 3D tasks on simulated data generated from their pipeline, and introduces three benchmarks based on simulated data. All reviewers agreed that the task is important, and that the careful co-design of the scanning hardware and rendering pipeline are technically impressive. Reviewers were initially put-off by the small number of actual scans provided, but after discussion agreed that the emphasis and true value of the work are not the scans of actual objects, but instead the data generation pipeline that allows for generating highly realistic synthetic data. In the end all reviewers were marginally in favor of accepting the paper. The AC agrees with the conclusion reached by the reviewers, and thinks that the synthetic data generated from the proposed pipeline is interesting and likely to be useful. Congratulations on having your paper accepted to the NeurIPS 2021 Datasets & Benchmarks Track!

When preparing the camera-ready version of the paper, authors are strongly encouraged to take into consideration the feedback from reviewers. In particular, the AC agrees with reviewers that the title of the paper should be changed (as the authors have already agreed to do); the authors should also consider revising the paper to better emphasize the importance of the synthetic data generated by the pipeline.",21-56.txt,0
20,https://openreview.net/forum?id=bNL5VlTfe3p,1,6,4,"L49: difference between 3 precisely-machined calibration objects and 7 color-textured 3D printed objects? any considerations for choosing those particular objects as the testset? are they diverse enough as testset to evaluate the generalization of new reconstruction models?
L103: ""We are not aware of any existing work able to achieve a faithfulness comparable to our approach, especially on geometrically complex objects"" --- this is a bold claim
L250: what's the resolution of 250 samples? bitdepth? Is it too small as training set? what the distribution it can represent? could it too restrictive to generalize only to the CAD models from ABC dataset?
Table 1: CNN performs too bad on normal direction, have you consider surface normal loss additional to the depth loss?
Fig 11: it looks like denoised depth introduces large positive error (red points). is it expected?
L254: can I understand the Shape completion task considered here is grayscale image (diffusely illuminated rendering) guided depth completion?
L269 & Fig 14: the generalization is shown from synthetic to real on CAD models, how about to the real objects?
I run the ipynb for simulating scans, decoding and reconstruction. It’s a very nice and smooth experience. Any idea for why the recon for vase misses point clouds near the neck?",21-57.txt,0
21,https://openreview.net/forum?id=fgFBtYgJQX_,1,7,2,"This paper proposes a very performant physics engine for robotics implemented on the GPU. The main benefit of this approach is that it enables end-to-end GPU training, which reduces the CPU to GPU communication and transfers. The speedups are measured on a wide range of RL tasks.

The paper is well-written and the overall feedback from the reviewers were positive. However, reviewers raised some concerns in their reviews. I recommend the authors to take those comments into account and try to address them in the camera-ready version of this paper.",21-58.txt,0
22,https://openreview.net/forum?id=VhIIQBm00VI,1,8,4,"This paper presents a benchmark dataset for few-shot learning based on the GLUE benchmark for natural langauge understanding. While there exist previous benchmark datasets, this work presents a new benchmark with significant novel contributions over the previous datasets. This would be a useful resource for NLP researchers.",21-59.txt,0
22,https://openreview.net/forum?id=VhIIQBm00VI,1,7,3,"Since NLP and NLU are not just about texts, I would recommend paying attention to tasks and benchmarks on other types of data that fall within the scope.",21-60.txt,0
22,https://openreview.net/forum?id=VhIIQBm00VI,1,7,3,"I have some questions for the authors:
What is the sampling method of training and test data? Did you claim the seed of the random sampling method? Do you have a better way of splitting the data if we want to make the sampled data more challenging/discriminative.
Are the educational background and the first language of annotators the same?
For tuning the model in a task-specific way, some models need to add extra layers. It will increase the size of model parameters. Which type of model capacity did you report in Table 5? Original or Modified models?
There are some typos in the paper:
Line 42: spans.^2. -> spans^2.
Line 81: exiting -> existing",21-61.txt,0
23,https://openreview.net/forum?id=mPducS1MsEK,1,7,5,"This paper provides a useful taxonomy and list of seemingly obvious failures in ML evaluations. After the author discussion period, all authors gave acceptance scores, and I agree with this suggestion. I encourage the authors to address the criticisms given in the individual reviews, using appendices for any details they did not spell out due to space constraints or other details they would like to add based on the reviewer comments.",21-62.txt,0
23,https://openreview.net/forum?id=mPducS1MsEK,1,7,4,"The descriptions of some failure modes could be clearer and/or more rigorous. I was sometimes relying primarily on the examples illustrating the failure mode to understand it’s definition better.
For 3.1 Algorithms, how do pressures from conferences to release code and consider reproducibility affect this failure mode?
Also 3.1, I would like to see more on the issue undocumented details. So many papers lack key details on every aspect of how experiments are conducted.
3.2 Label Errors is very brief. It may be worth discussing additional issues here such as how label errors relate to bias and fairness.
3.2 starting on Line 153: “Relatedly, [72] find that the removal of poor data from machine translation test sets over time has significantly contributed to the observed performance improvement in scores over the past decade.” This seems like it should be a separate issue from “Contaminated Data” as it is more specifically related to the benchmark drifting and becoming easier over time.
Where does the following failure mode sit in your taxonomy? Sometimes algorithms perform so well on a benchmark that it appears the benchmark is just too easy (this can also lead to the inability to detect performance differences if multiple algorithms all do really well). One scenario where this occurs is when an old benchmark dataset is repurposed for a newly introduced problem variation. Does this fit somewhere in 3.4 Comparison to inadequate baselines? Or is it more similar to some test set size issue?
Could you speak more to the benefits and drawbacks of the choices made in this paper regarding how the taxonomy is structured (e.g., separating internal and external at the highest level)? In other words, what benefits and drawbacks come with the specific perspective of this work?
I would like to see Section 5 expanded if extra space is given to accepted papers.
I like Figure 2 a lot, but I don’t know if it justifies the space it occupies. Maybe it could just be smaller.",21-63.txt,0
23,https://openreview.net/forum?id=mPducS1MsEK,1,6,4,"I have a minor comment about section 4.4.1 (""Reliance on simple heuristics"").
I think the section title doesn't adequately describe the problem. Presumably, if a task could genuinely be solved with simple heuristics, that's not a problem (and maybe it's even good news). The problem that the authors describe in that section is rather that task performance was evaluated on inappropriate datasets, resulting in simple heuristics appearing to succeed when actually, when evaluated on appropriate datasets, they don't. Maybe the failure mode could be re-characterized as reliance on 'inappropriate' heuristics (something along those lines).
This raises a bigger issue about whether a task definition should include notions of what the 'appropriate' heuristics or datasets for it should be. I understand why the authors chose to leave out details like these from the definition of a task. However, it seems like specifying these details could eventually be important if we want to understand what, in the context of a specific task, counts as a ""transfer"" or a ""distribution shift"". I get that there's no room to address this in this paper, so I'm offering it as a suggestion for future work.",21-64.txt,0
24,https://openreview.net/forum?id=mbW_GT3ZN-,1,8,2,"The authors introduce CREAK, which is a benchmark for commonsense reasoning about entity knowledge, collecting true and false information about entities with crowdsourcing. The reviewers agree that this dataset would be of a valuable contribution for various NLP tasks. I strongly recommend acceptance of this paper.",21-65.txt,0
25,https://openreview.net/forum?id=fnuAjFL7MXy,1,7,4,"This paper presents the MET dataset, a cleanly-labeled large-scale image dataset in the artwork domain for the study of instance-level recognition in the domain of artworks. While reviewers agree that the dataset is well-constructed and the paper is clearly written, some raised the concerns that the scope of the dataset is somehow overlapping with GLDv2, and that the dataset seems to be of limited interest to the NeurIPS audiences because of it domain-specific nature. After the authors' response, one reviewer agreed to raise the score from 5 to 6 (borderline accept), although it is not reflected in the system. Then now the all four reviewers are in the positive side, two of them with clear accept.
I, the AC, agree that this paper will fit better to vision or multimedia conferences. Nonetheless, since there is no severe weakness in the paper, I think it is ready to be presented at this track.",21-66.txt,0
25,https://openreview.net/forum?id=fnuAjFL7MXy,1,7,3,"In terms of how to improve the paper, please refer to the above sections. More proper venues for this paper inclue ICMR, ACM Multimedia, and other vision related venues etc.",21-67.txt,0
26,https://openreview.net/forum?id=rH8yliN6C83,1,7,4,"typos: L62: on the human pose estimation dataset → on a …
L91, L123: facilitate the research → facilitate research
L122: makes them difficult → makes it difficult
L123: regarding → considering
L134: biological a prior → a biological prior
L211: start → starting",21-68.txt,0
26,https://openreview.net/forum?id=rH8yliN6C83,1,7,3,"It would be great if the final version would have the following items:
The evaluation method for the experiments is not mentioned (It is use mAP, but how it was used it is not specified).
The baseline models are not detailed at all. A very short description should be introduced.",21-69.txt,0
27,https://openreview.net/forum?id=wCu6T5xFjeJ,1,5,5,"This paper presents a benchmark for text retrieval tasks, using 18 already existing datasets from diverse domains and task complexities, and covering a large variety of models used to showcase retrieval and ranking performance especially in a transfer learning setting. Overall, reviewers thought the work was strong in the diversity of included tasks, models, and usability; reviewers also praised the relevance with respect to zero-shot approaches. Concerns included overall novelty, distinction between relevance and ranking, and the fact that models were trained on different corpora at times. However, the reviewers ultimately have found this paper to be an extremely strong benchmark that will be immediately useful in the field.",21-70.txt,1
27,https://openreview.net/forum?id=wCu6T5xFjeJ,1,8,5,The authors should re-visit the word overlap experiment to show task complexity/overlap. Rather use the real estate to show transfer performance when the models are trained on a task (apart from MSMarco). The word overlap experiment is slightly misleading and doesnt provide extra intuition about task complexity.,21-71.txt,1
27,https://openreview.net/forum?id=wCu6T5xFjeJ,1,8,4,"The authors convinced me that nDCG@10 is also a reasonable choice to report. Therefore, you can ignore it as one of the weaknesses I raised (I keep it there to understand their answer for that issue)",21-72.txt,1
28,https://openreview.net/forum?id=q-8h8-LZiUm,1,7,4,"This work presents KLUE, which provides a set of benchmarks for natural language understanding. All reviewers agree this is high-quality work, and that the process of creating data was done in a thoughtful manner, with attention to detail. High-quality datasets in a multitude of datasets are indispensable and this work will contribute to work on low resource languages in general.",21-73.txt,0
28,https://openreview.net/forum?id=q-8h8-LZiUm,1,8,4,"Line 136: superGLUE -> SuperGLUE
Possible missing reference for the whole word masking strategy (line 258). And also, what tool was used for Korean word segmentation for WWM?
Unlike most of other NLU benchmarks, there is no overall score in KLUE. It is quite hard to compare different PLMs as a whole. If this is not designed, I'd suggest adding an overall score to your benchmark leaderboard.
line 292: The illustration 'a few interesting observation' is somewhat confusing, as these observations are quite normal and expected.
Will the pre-training model and data also be released to the public?",21-74.txt,0
29,https://openreview.net/forum?id=j6NxpQbREA1,1,6,5,"The paper is very clear in its arguments against the over-reliance of machine learning on benchmarking, which it claims as used as inadequate proxies for generalized learning. This core argument is supported by historical background and suggestions for improvement, although some of these suggestions could be moved to the main body of the paper or otherwise reorganized for increased visibility.

The reviewers provided very thorough reviews. They are in agreement that this paper is well done and covers an important topic that is relevant to the track. The authors' responses to the reviews were detailed and the resulting conversation makes it clear that the paper has the potential to spark important discussions in the community.",21-75.txt,1
29,https://openreview.net/forum?id=j6NxpQbREA1,1,5,3,"I reiterate that my goal in writing this review is not to nitpick or critique, but to offer places where the argument can be improved because it is important and deserving of a platform. But I want it to be more definitively able to occupy the space it is creating when it gets there.",21-76.txt,1
29,https://openreview.net/forum?id=j6NxpQbREA1,1,7,4,"Overall, the paper is really good, and the recommendations for moving forward with benchmarks provide a lot of pizzazz. Where I'm not 100% sold is the idea that the mentioned benchmarks are ""general purpose"" in the sense of the overarching goal of AGI rather than general purpose in a narrow AI specialty, even the authors mention that AGI is underspecified. For example, is computer vision one piece of AGI, or do we get to an intelligent agent that can understand its environment another way? Another reason I'm not sold is that the authors could have made the claim that the promise of current benchmarks is overhyped and not rooted in practical task specifications without making the claim that these are general purpose benchmarks and tying them to AGI. The AGI conversation somewhat seems to fall apart very easily (to me because I also read a lot on AGI although I don't care to touch it with a ten foot pole because it raises more questions than it answers) when considering benchmarks for ""narrow AI"".",21-77.txt,1
29,https://openreview.net/forum?id=j6NxpQbREA1,1,7,4,"Please use the latex template designed for the Datasets and Benchmarks track (page 1: “NeurIPS 2020 Workshop: ML Retrospectives, Surveys & Meta-analyses (ML-RSA)” seems to be a holdover from a previous submission).
Missing space in section 3.2 “well-defined kind of generalizability.Furthermore,”
Same in 4.1.1. “are not much more carefully selected.The benchmark”, “(lacking in particular any exploration of pragmatics[Qiu et al., 2020], or the proper handling of negation[Ettinger, 2020])”",21-78.txt,1
30,https://openreview.net/forum?id=hjsJraoqn1Y,1,8,3,"The data and methods provided in the paper are of relevance for the NeurIPS data track. Based on reviewers opinions, and in particular after discussion with the authors, the paper achieves the minimum score required for publication at NeurIPS data track.",21-79.txt,0
32,https://openreview.net/forum?id=pX4x8f6Km5T,1,6,2,"This paper presents a dataset for complex question answering. The dataset is significantly larger than previous datasets, and this problem space is underexplored but important. Reviewers' comments were addressed well in the rebuttals. All reviewers agree that this paper has merit and potential contribution for future research in this topic.",21-80.txt,0
33,https://openreview.net/forum?id=zQIvkXHS_U5,1,7,3,"This submission proposes a manipulation benchmark, with the focus of dealing with different objects in 3D simulation. All of the reviewers have rated the submission very highly, and agreed that it will be useful for a broad range of disciplines (e.g. robotics, RL). The paper is well written, the dataset is well documented, and benchmark results are provided. For this reason I recommend acceptance of this submission.",21-81.txt,0
34,https://openreview.net/forum?id=NYgt9vcdyjm,1,7,3,"This is an ambitious project and impressive array of datasets. I also think the Python toolkit will be useful for practitioners. However, the lack of detail in dataset construction and processing, combined with the lack of systematic benchmarking or even specification of evaluation criteria for each task, limit the impact of the work as currently presented.",21-82.txt,0
34,https://openreview.net/forum?id=NYgt9vcdyjm,1,6,5,"My suggestion to the authors is to think first what is actually the challenges in graph generation and transformation. A simple summarization of datasets in previous works doesn't make much sense for the community. It would be more important to find out how datasets or benchmarks can cover limitations in previous works of graph generation and transformation, and contribute to further research in this domain.",21-83.txt,0
34,https://openreview.net/forum?id=NYgt9vcdyjm,1,7,4,"As the authors point out in the main text, the problem of graph generation and transformation may differ in contexts and domains. Therefore, it is suggested to compare the implementations of graph generation and transformation in different domains, so as to allow readers to understand the limitations and difficulties. In the current version, some baselines methods target at specific domains, e.g., MoFlow for molecule graph generation, while some others are for general domains, e.g., GraphRNN. A comparative study may be helpful for better showing the difference in different domains.
Deeper discussions on the experimental results should be provided, in particular, how do existing work differs from each other and how different metrics affect model performance. It is interesting for me to discuss the choice of evaluation metrics, which seems are most based on simple statistics and heuristics. From Table 2, it is observed that many existing baselines have already achieved quite promising performance in terms of the concerned three metrics; does it mean that the generation quality is good enough, or may other domain-specific metrics be adopted? The authors may also outline several future research directions that allow for improvements.
Minor edits:
In Line 322, what is ""label graph""?
In Line 352, ""Fig. 2"" should be Table 2.",21-84.txt,0
35,https://openreview.net/forum?id=ByiVJWsHVKc,1,7,4,"The reviews are mostly positive. Although the dataset has a small number of labels, it can be a useful resource for research on self-supervised learning. The diversity of coverage is definitely a big plus. I am thus happy to accept i.",21-85.txt,1
35,https://openreview.net/forum?id=ByiVJWsHVKc,1,4,5,Overall the paper is good and thanks to the authors to bring this out to the community in general for improvement of self and semi supervised algorithms in the Autonomous driving space.,21-86.txt,1
37,https://openreview.net/forum?id=TSvj5dmuSd,1,7,3,"All reviewers appreciated the contributions of this work, in particular the diversity of data and domains. Two reviewers do note that more information on hyparams etc. ought to be included, in particular for reproducibility purposes. Several reviewers note that the dataset is in French, and that this may limit adoption of the benchmark; on this point I disagree---we need more datasets in languages other than English (even for similarly high resources European languages). To me, it is a strength of the work that it does not focus on English. This being said, it would be nice if you included a bit more on the unique properties of French (speech) that might be relevant for understanding performance on the benchmark (if there are any). All in all, the work is strong and is likely to be recognized as a good quality SSL speech benchmark.",21-87.txt,0
38,https://openreview.net/forum?id=TAXFsg6ZaOl,1,6,4,"The paper introduces a new dataset of DeepFake video, where each instance may have any combination of real or fake audio or video. Initial reviews were mixed, but the author responses and paper revisions were able to address all reviewer concerns and in the end all reviewers thought that the paper was above the bar for acceptance. Congratulations on having your paper accepted to the NeurIPS 2021 Datasets & Benchmarks Track!",21-88.txt,0
38,https://openreview.net/forum?id=TAXFsg6ZaOl,1,7,4,"Some typos:
line 303: FakeAVCelebwith -> FakeAVCeleb with
line 305: FakeAVCelebis -> FakeAVCeleb is
line 328: FakeAVCelebis -> FakeAVCeleb is
line 330: indiidual -> individual
line 334: FakeAVCelebin -> FakeAVCeleb in
line 350: FakeAVCelebby -> FakeAVCeleb by
line 355: FakeAVCelebstill -> FakeAVCeleb still",21-89.txt,0
38,https://openreview.net/forum?id=TAXFsg6ZaOl,1,6,2,"The current version of FakeAVCeleb only contains videos on celebrities. Indeed, celebrities are the most common objects in Deepfakes. However, it would be better if the dataset could include more diverse objects (i.e., not only celebrities but ordinary people) in the future. Also, the languages of speaking and the fake audio generation methods can be further expanded.
Why are existing Deepfake datasets racially biased? For example, DeeperForensics contains actors from 26 countries, covering four typical skin tones: white, black, yellow, brown.
What about the lip-unsynced cases? Can they be recognized as fakes? The authors may discuss this.
Line 95, 96 Typo: ""UADF"" -> ""UADFV"".
Line 184, 303, 355 Typo: a missing space after ""FakeAVCeleb"".
Line 322, 330 Typo: What are Figure 5 and Figure 6 in the main paper?
Line 330 Typo: ""indiidual"" -> ""individual"".",21-90.txt,0
39,https://openreview.net/forum?id=uXa9oBDZ9V1,1,9,3,"The paper studies VQA over abstract diagrams, and introduces new datasets about abstract icons and question answering over diagrams of icons. All reviewers found the task novel and interesting, felt that the datasets were likely to be useful to the community, and thought that the paper was well-written. Congratulations on having your paper accepted to the NeurIPS 2021 Datasets & Benchmarks Track!",21-91.txt,0
39,https://openreview.net/forum?id=uXa9oBDZ9V1,1,6,4,"Extensive work! Just some formatting feedbacks:
Figure 5: if possible, it would be better to make the text and label larger.
Line #103: there should be more space between the table and the title of section 3.1. Add some space or move the table to the top/bottom of this page.",21-92.txt,0
39,https://openreview.net/forum?id=uXa9oBDZ9V1,1,7,4,"Fig 2 (a) is missing an y-axis label.
It would be great to see how a random baseline performs in Table 6.
In Table 6, are the ""human"" numbers an average of all groups? It would be great if the numerical version of Fig 8 was also provided.",21-93.txt,0
40,https://openreview.net/forum?id=jI_BbL-qjJN,1,5,5,"This paper presents a benchmark dataset for hate speech detection, using an IR-based approach. Reviewers found the contribution compelling, though questions several aspects of novelty with respect to previous datasets, and methodology.",21-94.txt,1
40,https://openreview.net/forum?id=jI_BbL-qjJN,1,8,3,"About subjectivity vs. ambiguity: it is true that subjectivity may partly explain the observed uncertainty but I think the annotator’s disagreement may also be due to some forms of aleatoric uncertainty, i.e. an intrinsic uncertainty that leads to a non null p(y|x) for a given tweet x. For instance, a tweet might be intrinsically ambiguous because of some missing information about the context, the author, etc. and the conditional probability might be equal to e.g. p(y|x)=0.6. In such cases, some annotators will say y=1 while others will say y=0 not because they bring a subjective judgement but rather because they have to chose randomly between 1 and 0 . It is very important to keep such ambiguous tweets in the training set because the ambiguous region around the theoretical decision frontier is the most difficult to learn. With this in mind, the proposed approach to select samples (based on pooling or  most positives » samples) might be problematic. Indeed, they are likely to not select such ambiguous tweets and focus on the less ambiguous ones. Classifiers trained on this type of data will then tend to make « black-and-white fallacies » when they encountered ambiguous tweets.
Minors: «  identifies langauge » —>«  identifies language »",21-95.txt,1
40,https://openreview.net/forum?id=jI_BbL-qjJN,1,9,5,"Few corrections:
“representative coverage for cost-savings, yielding a biased dataset whose whose distribution diverges”
“incites violates against a demographic group, and identifies that target group”
In more than one case appears: “For a tweet, if an annotator identifies langauge that is derogatory toward or”",21-96.txt,1
41,https://openreview.net/forum?id=Q9SKS5k8io,1,6,4,"The paper proposes a comprehensive benchmark for weak supervision, consisting of 22 datasets for classification and sequence tagging from various sources (text, videos). It also provides implementations for popular WS methods and an extensive analysis of their performance. All the reviewers agree that this is a very useful contribution for the community. The authors updated their manuscript to take into account reviewers' comments, and pledge to continue extending the benchmark, with 2 new methods recently added.",21-97.txt,0
41,https://openreview.net/forum?id=Q9SKS5k8io,1,8,3,"L58 would be greatly strengthened with the addition of examples. L126 - for future promises, maybe instead focus on how will this benchmark serve the field for years to come.
I'd consider adding a future work section or a 'role of this benchmark going forward' section at the end (or even just as \paragraph section in the discussion) to clarify the role of this benchmark going forward.",21-98.txt,0
41,https://openreview.net/forum?id=Q9SKS5k8io,1,9,3,"What label functions are used for the experiments in section 6? Are there procedural functions that use labeled data? If so, what is the performance of using this labeled data for training instead of LF generation?",21-99.txt,0
41,https://openreview.net/forum?id=Q9SKS5k8io,1,8,4,It wasn't immediately clear to me that the benchmark could actually be used to evaluate LFs themselves until I got to section 5. I think this is a useful contribution and could be highlighted more prominently earlier in the paper as well.,21-100.txt,0
42,https://openreview.net/forum?id=YDMFgD_qJuA,1,6,3,"The reviewers all liked the paper. The authors' response clarified some important points. In view of that, the authors are strongly invited to take the feedback on board for the final version.",21-101.txt,0
42,https://openreview.net/forum?id=YDMFgD_qJuA,1,9,4,"Overall this is a very well written paper. the motivation for the paper is clear and the reviewer believes it is a significant contribution, however, the reviewer has a few concerns on the annotators of the dataset (i.e. how the annotators were selected, The ground truth criteria the annotators used, and how disagreements in annotations were resolved). Furthermore, more information is needed on how the dataset will be accessed in the future, and the plans for maintaining the dataset.
EDIT. Thank you for the response, the review has been updated",21-102.txt,0
42,https://openreview.net/forum?id=YDMFgD_qJuA,1,6,3,"In my opinion, the work has immense potential. In its current form, even though the dataset is complete, the benchmarking seems incomplete. The latter not being the primary focus of the dataset track is why I'm recommending an 'accept'. However, a complete benchmark plus the dataset would make this work very strong.",21-103.txt,0
42,https://openreview.net/forum?id=YDMFgD_qJuA,1,7,5,"A few minor typos and clarifications:
Lines 49-50 exists no large-scale public datasets that include both the raw MRI data along and multiple clinically- relevant observations for each image.
Lines 113-114 Classification/Detection: The MRNet and the Osteoarthritis Initiative datasets provide labels for classification of knee MRI abnormalities - Table 1 shows classification crossed out. Please correct the discrepancy.
Line 205 reconstruction tasks in clinically pertinent regions. When using these labels in the context of the this
Clarification - For the reconstruction track to validate the T2 relaxation, I assume the sub-regions quantification would be based on the ground truth.
Section 6.4 Raw Data Track – Reconstruction - It's interesting that some of the experiments (Unrolled (E1E2)) had higher accuracy and lower variance in the 8x than the 6x. It is surprising that a more aggressive sampling by a factor of 2x produces a higher accuracy. Do the authors have a hypothesis as to why that is happening? It would be a useful addition to the paper.",21-104.txt,0
43,https://openreview.net/forum?id=udVUN__gFO,1,8,4,"This is a well-annotated dataset of agreement and disagreement among Reddit users. The dataset would be very useful for agreement detection, and it is well constructed and well explained. It is also great that the social network can be built which could be of additional value. All reviewers agree that this paper should be accepted.",21-105.txt,0
43,https://openreview.net/forum?id=udVUN__gFO,1,7,4,"Minor typo) line 272: have been been combined -> have been combined
Comments) section 4.3, cross-subreddit evaluation: It shows opposite results compared to the recent works showing that stance detection would benefit from context-sensitive approaches (section 1, line 31). Could you please analyze why learning signals across different subreddits (contexts) improves the performances?
Question) section 4.3, masking parent and child comments: The paper claimed that training with the child message only shows 60.7% accuracy, while the baseline accuracy with full input is 62.4%. Does it imply any artifact or bias on the dataset?
Question) section 4.2, table 5: BERT trained with DEBAGREEMENT tends to fail on Perspectrum, while vice versa. Does it show that DEBAGREEMENT is not an improved version of (dis)agreement detection dataset but a different one?",21-106.txt,0
43,https://openreview.net/forum?id=udVUN__gFO,1,9,4,"What do you mean with ""social evolution""? (line 18)
""to understand"" what? (l56)
Table 2: What are the numbers in parentheses?
Appendix: line 649 [Insert website address].
The section on combining LMs and GRL methods seems odd at its current position. The rest of the section presents actual results, and this is what I expected when I started reading this paragraph. I would move this to a different section in the paper (e.g. future work).
Appendix: ""minimally altered online interactions"" what kind of changes were made? Are these the contractions that were expanded?
the way Reddit is formatted in the text isn't consistent.
I understand that space is limited, but more examples of the data in the main paper would have been very helpful, especially since the only example that was provided wasn't very clear (one could argue for a different label).",21-107.txt,0
44,https://openreview.net/forum?id=y2AbfIXgBK3,1,7,3,"The reviewers acknowledge the novelty of the fallen person detection dataset. Although it has some drawbacks such as limited number of subjects and acted events, this is very difficult to have real data for the problem. Overall, the data and provided analyses show unique aspects and this can be a good resource for the community.",21-108.txt,0
45,https://openreview.net/forum?id=FgYTwJbjbf,1,8,3,"Reviewers all agreed that this medical imaging dataset will be a plus to the community; it include both Chinese and English will help evaluating and training explainable annotations to generate medical reports. The scale of this dataset is big relative to other real-patient dataset. While evaluating quality of the reports remain challenging, authors show their best effort (eg BLUE, Rough) and human evaluations. Authors also responded to reviewers’ any remaining questions.",21-109.txt,0
45,https://openreview.net/forum?id=FgYTwJbjbf,1,7,3,"Lines 161-164: You describe how you had two teams of annotators. It's known from similar fields that human annotators do frequently disagree with each other. Did you collect data on how often the validation team ""corrected"" the original annotations? Lines 182-182: ""tens of millions of reports"" The dataset seems to have 10000 reports, still a lot of effort, but not ""tens of millions"" Line 190: ""text-free"" = ""free-text"" ?! Line 195: There seems to be a significant disparity between the Chinese and English vocabulary sizes. Is this due to the translation or inherent to the languages? In other words, would you expect originally English reports to have a larger vocabulary? And does this matter? Line 319: If the features alone are already helpful (and seemingly outperform even human reports) while bothering to create reports at all?",21-110.txt,0
45,https://openreview.net/forum?id=FgYTwJbjbf,1,8,3,"Despite the fact that I am not an expert in medical report generation, I regard this paper as a very positive contribution, both for the AI and the medical community. It is an impressive piece of work and the benchmark is convincing. It is only too bad that this work does not come with an online evaluation system. Overall, I believe this paper deserves to be accepted for publication.",21-111.txt,0
45,https://openreview.net/forum?id=FgYTwJbjbf,1,9,3,"Overall, the paper is well written, the scope of the work is impressive, and I think this dataset and benchmark should serve as an inspiration for other medical imaging datasets.",21-112.txt,0
46,https://openreview.net/forum?id=9-LSfSU74n-,1,6,4,"This paper presents a very interesting dataset for time-sensitive reasoning in question answering. All of the reviews are positive, and my reading was also positive. The community, especially those who are interested in NLP and QA, would be quite fascinated to hear about this dataset and get their hands on it.",21-113.txt,0
46,https://openreview.net/forum?id=9-LSfSU74n-,1,8,4,"Your submission pdf may have been rendered in camera-ready mode and does not contain line numbers, so it’s hard to locate exact lines when communicating detailed feedback.",21-114.txt,0
47,https://openreview.net/forum?id=701FtuyLlAd,1,6,2,"This paper proposes FS-Mol, a molecular dataset and benchmarking system especially motivated by recent progress and work on few-shot learning approaches in the biological/molecular domain. Overall, reviewers found this to be a strong and valuable contribution to the community.",21-115.txt,0
47,https://openreview.net/forum?id=701FtuyLlAd,1,7,3,"Given that some of the EC categories had only one task in
Dtest
, Table 2 does not add much to the paper. The claim in L330 is also very broad and unconvincing without further elaboration/quantification.",21-116.txt,0
48,https://openreview.net/forum?id=Y6sH0l4PExm,1,5,4,"The problem and data is of extremely relevance. However all reviewers agree on some major drawbacks: limited size of the data, data sharing limitations, and some missing information. After discussion with the authors several critiques have been clarified. Overall the paper achieves the minimum score to be accepted for publication at the NeurIPS data track.",21-117.txt,1
48,https://openreview.net/forum?id=Y6sH0l4PExm,1,6,3,"While the dataset seems to be a valuable medical resource, the experiments section feels lacking to me, primarily due to the use of the self-reported pain levels to predict the pain value.",21-118.txt,1
49,https://openreview.net/forum?id=-v4OuqNs5P,1,6,3,"All the reviewers agree that this is a high-quality dataset, easily accessible, and larger-scale compared to existing datasets. Several reviewers noted that the PointNav task showcased is close to saturation, hence this benchmark will not drive much progress on this task. However, other tasks can be set up, as the authors noted, hence this is still a very useful resource for the Computer Vision and Robotics communities.",21-119.txt,0
49,https://openreview.net/forum?id=-v4OuqNs5P,1,8,4,"L140 This is computed as the ratio between the raw scene mesh area within 0.5m of the navigable regions and the navigable space
According to my understanding, it is the ratio between all the raw scene mesh area and navigable space. all mesh area = navigable space + obstacles. Is it correct? Why is 0.5m necessary here?
Fig 5(a)
The camera pose of real and rendered images do not look the same. Why?
Finally, the visual fidelity of images rendered from HM3D is 20 - 85% higher than prior large-scale datasets,
How do you compute 20 - 85%? Is it based on FID? I cannot compute it from Fig 5(b).",21-120.txt,0
50,https://openreview.net/forum?id=eZu4BZxlRnX,1,5,4,"This work introduced a Unity-based 3D environment for evaluating the ability of meta-reinforcement learning agents to infer latent causal structures from interactions. At the initial reviews, this work received 5, 7, 7 from three expert reviewers. Reviewer 1Tpo requested clarifications on task design and survey about related benchmarks. Reviewer WAyX and WeQ5 felt positive but also raised several clarification questions about the technical details. The authors provided detailed responses to these questions and addressed the reviewers' comments well. In the end, Reviewer WeQ5 agreed to upgrade their rating from 7 to 8 but didn't get a chance to do so before the review system is closed in the final phase. The AC read the paper, the reviews, and the authors' discussions in detail, and agreed with the reviewers that this paper introduced an interesting new benchmark to facilitate research in the challenging meta-RL domain. Taking all the factors into account, we recommend accepting this paper in this track.",21-121.txt,1
51,https://openreview.net/forum?id=aqCD8RINP54,1,7,4,"All reviewers agree that the proposed dataset fills in a gap in the important super-resolution research venue and the paper is well delivered. In addition, the reviewers also confirmed that their concerns are addressed. AC recommends accepting the paper.",21-122.txt,0
51,https://openreview.net/forum?id=aqCD8RINP54,1,7,3,"The authors did not provide any code of their benchmarking methods. If possible, the authors are recommended to provide the code they used to evaluate different methods.",21-123.txt,0
52,https://openreview.net/forum?id=JtjzUXPEaCu,1,8,4,I’m looking forward to this paper and the dataset being published so I can share it with others. Kudos on an excellent and important effort. I foresee this being well-cited.,21-124.txt,0
53,https://openreview.net/forum?id=jpwGODt2Av,1,6,3,"This paper introduces a novel whole-brain vessel graph dataset, which is likely to be very useful for the graph learning community. All reviewers gave acceptance scores, several of them very enthusiasticly so. Clear accept.",21-125.txt,0
53,https://openreview.net/forum?id=jpwGODt2Av,1,9,4,"(1) Is there data bias in the proposed dataset? (2) Only link prediction and node classification are benchmarked, is the graph classification task also useful for the benchmarking of vessel graph dataset? Because I think graph classification is more related to disease diagnoses.",21-126.txt,0
54,https://openreview.net/forum?id=uKv5inrWeld,1,6,2,"Please engage with my review in the rebuttal to help clarify a few points and I will update my score in accordance with other reviewers and your responses. Thank you for your understanding.
Edit: Based on author's responses and further clarifications, I have adjusted my score to 7.",21-127.txt,0
55,https://openreview.net/forum?id=J4Nl2qRMDrR,1,7,3,All reviewers recommend acceptance and point out the good execution and potential impact of the work in the community. The AC has no reason to overturn the reviewers' decision. Congratulations to authors!,21-128.txt,0
56,https://openreview.net/forum?id=oJ0oHQtAld,1,8,3,"The paper extends the existing NeMO toolkit with tools to perform audio alignment and the production of short units of paired, aligned speech and text. The extensions perform pre- and post-processing on the input audio and text. A tool is also provided to explore a speech dataset by providing visualizations of useful information about its contents, e.g., the length of included transcripts; multi-step realignment is proposed as a mechanism for improving the overall segmentation results. A Russian dataset is constructed using the proposed toolkit to demonstrate its utility. The motivation for the work is clear, as the process of building a speech-based dataset is generally burdensome. For users of NeMO, the additional tools provided are likely to improve the overall process. Generally, reviewers agree that the dataset visualization tool (SDE) has value and may be of use to those creating or exploring datasets. Useful heuristics for dataset creation are proposed as best practices, and the paper is well-written overall.

Some of reviewers' initial concerns were addressed, and those clarifications and imporvements should be in the final version of the paper. It's not clear how automated or turnkey this toolkit is, and which steps need to be managed by hand. In terms of the overall alignments, WER is not as easy to interpret as ground truth segmentations would be. It's also not obvious how applicable the developed tools will be to anyone who is not using NeMO, and the theoretical contribution is strongly influenced by pre-existing CTC models.",21-129.txt,0
56,https://openreview.net/forum?id=oJ0oHQtAld,1,6,4,The paper mentions the importance of speaker variance in speech datasets but I wonder if the data explorer tool allows for speaker-based filtering. Accessing the different statistics mentioned in the paper on a per-speaker basis might be useful in bias studies.,21-130.txt,0
56,https://openreview.net/forum?id=oJ0oHQtAld,1,7,3,"If there is a way of hosting the work on an independent repo with more compatibility with other ASR models, uptake could be greatly improved in future. As it stands the tool appears fairly deeply buried within NeMo. This reduces the perceived impact and significance of this work, making it appear to me like two (albeit very well constructed) wrappers for previous methods instead.
Polishing the figures, better linking the tools with specific examples (e.g. I found a nice tutorial notebook for the creation tool here but no mention of it at all in the paper or appendix!) , and trying to generalise outside of specific applications will be steps that would persuade me to revise my score.
As a final note, please engage constructively with my review during the rebuttal phase, and I would be happy to update my score in accordance with other reviews and your responses. This topic is outside of my strict area of expertise so I acknowledge that I may have made some errors and am receptive to feedback. Thank you!
Edit: post rebuttal
I have raised my score to 7 following the satisfactory resolution of many of my comments. Thanks to the authors for co-operating and improving their submission based on feedback.",21-131.txt,0
57,https://openreview.net/forum?id=MQlMIrm3Hv5,1,8,3,"This paper presents a novel benchmark for evaluating robustness against spatio-temporal video corruptions. While the reviewers generally agree that the paper addresses a novel important question, it received split reviews of (3,8,6,7) where the main concern raised by one reviewer is the lack of real corruption data to prove that the proposed synthetic dataset really aligns with the real-world corruption cases.
The reviewers and AC had an intensive and fruitful discussion on this point. Importantly, we all agree that the concern totally makes sense and we all have to be aware of this fundamental issue. At the same time, we also understand that sometimes collecting ideal data is quite difficult and demanding, and we need to find out a reasonable compromise to go forward step by step. From this viewpoint, three reviewers clearly support that the paper opens up a novel research direction and has enough contributions to be published in its current form. The AC considers their conclusion is valid and would like to recommend acceptance.
In addition to that, as a part of NeurIPS, papers in this track are expected to lead this kind of fundamental and scientific discussion in terms of data, so I'd like to encourage authors to consider this issue thoroughly and enhance the discussion properly (in particular, the limitation of the dataset).",21-132.txt,1
57,https://openreview.net/forum?id=MQlMIrm3Hv5,1,3,4,"• In Section 2.2, I recommend including another work by D Tran, et al. in which video classification networks were further improved by decomposing 3D spatiotemporal convolutions into 2D spatial and 1D temporal convs with an activation between (R(2+1)D networks): D Tran, et al. “A Close Look at Spatiotemporal Convolutions for Action Recognition” DOI:10.1109/CVPR.2018.00675
How is the frame rate conversion performed exactly? Is the video data re-interpolated? What interpolation scheme is used? Please clarify this in the text.
How did the authors decide on standard deviation of 0.1 and 0.2 for training with Gaussian noise? Can you show an example of clean images, with std=0.1 noise, and with std=0.2 noise in the supplementary information? Please clarify this in the text.
The authors state that “Gaussian data augmentation has been widely used to improve the robustness of models in image-related tasks,” yet they only cite one work (ref. 13). Can the authors provide other examples? Otherwise, I suggest revising the wording here.",21-133.txt,1
58,https://openreview.net/forum?id=R7vr14ffhF9,1,8,3,"Reviewers agreed that this dataset will add values in evaluating explanatory methods. Despite the simplicity of the dataset, reviewers agree that the dataset will help towards unification of benchmarks. Authors responded reviewers questions, for example, by adding a second real dataset.",21-134.txt,0
58,https://openreview.net/forum?id=R7vr14ffhF9,1,6,5,"Minor comment: The authors cite the distinction between interventional and conditional expectation estimates for SHAP, but it seems that in Section 5.2 they write ""Explainers such as SHAP assume feature independence""---if they used interventional expectations in the experiment for this section, this should explicitly stated, otherwise that claim is somewhat confusing.",21-135.txt,0
59,https://openreview.net/forum?id=8Y50dBbmGU,1,4,4,"This paper presents a dataset of annotated abstracts for faceted query by example (QBE) task. The new task is very interesting, the dataset, albeit not too large, is well annotated and clearly described. This paper will be an important contribution to the field of IR and will serve as a starting point into finer-grain and thus more useful QBE. While some reviewers had some concerns, the author response, as well as the revised paper, addressed most of the concerns.",21-136.txt,1
59,https://openreview.net/forum?id=8Y50dBbmGU,1,8,5,"In the revision process, the authors addressed all points I raised. I appreciate their effort for these. I changed my rating from 6 to 8.",21-137.txt,1
60,https://openreview.net/forum?id=wEc1mgAjU-,1,8,5,"Refer to Weakness.
Figure 1 visualizes distinct density populations for various datasets. Therefore, it would be interesting to discuss the performance of baselines in terms of different density populations.",21-138.txt,0
60,https://openreview.net/forum?id=wEc1mgAjU-,1,6,4,"Please clarify, is the data on the archive saved as the raw data or preprocessed?
Are all the results for univariate forecasting? using only a single variable or all available features are used as inputs? Since it seems that neural network only get univariate features (take covid death as an example)
########Post rebuttal
Thank you for the clarification. Given that preprocessing is done within the GluonTS, I raise my score to 6.",21-139.txt,0
61,https://openreview.net/forum?id=enYjtbjYJrf,1,8,4,"All reviewers agreed that this dataset makes good contribution. It includes many known chaotic dynamical systems, benchmarking 16 forecasting models with many ML methods (not just DL). It show cases usefulness in transfer learning, symbolic regression and others. Authors has responded to reviewers’ comments appropriately (eg adding glossary terms).",21-140.txt,0
61,https://openreview.net/forum?id=enYjtbjYJrf,1,8,3,"The authors propose a great benchmark that would help the ML community as a whole. With more experimentation and clear writing, I think it will be a good benchmark for people to start using. I'd be happy to revise my score if some of my concerns are addressed.
Update: Increased the score based on the rebuttal provided.",21-141.txt,0
62,https://openreview.net/forum?id=eLYinD0TtIt,1,7,3,"Although the dataset shows unique aspects, there are critiques related to lack of proper motivation, missing design decisions (noise, sampling decisions, partitions, labeling, sanity check, etc.) and analyses. However after discussion with the authors, several critiques have been clarified. Overall the paper achieves the minimum score to be accepted for publication at NeurIPS data track.",21-142.txt,0
62,https://openreview.net/forum?id=eLYinD0TtIt,1,6,4,"Please state the differences of set-valued classification with related formulations, eg., multilabel classification, set-based classification, hierarchical, link based classification, etc.
can you please define the right term of Eq (2)?
Can the authors further elaborate on why it is important to approach the problem formulated in Section 2?, this is, motivation for this problem, why it is worth approaching it this way and not using multilabel classification for example?
The information on metrics (4.1) is to detailed even when the metrics are quite common and intuitive (eq 5 is widely used as the authors mention and eq 6 is also widely used in multi label classification)
Authors try several baselines but none of them tries to optimize the proposed evaluation metrics directly. This is one of the motivations of the dataset, yet authors still use cross-entropy as loss function. It would be interesting to know the performance of models that either optimize such metrics directly or other approaches that are appropriate for approaching this problem, e.g., methods for hierarchical classification, multilabel classification etc.",21-143.txt,0
63,https://openreview.net/forum?id=UuUbIYnHKO,1,7,5,"The paper studies methods for contrastive learning on graphs, categorizing and performing extensive benchmarks on existing methods. They also provide an open-source package to facilitate further study of graph contrastive methods. Reviewers felt that this was an important topic to study, and that the detailed investigations and open-source library will be of use to the community. Reviewers wnP1 and rwpd initially had some concerns about details related to experimental setup and comparisons, but these were largely addressed by the comprehensive responses provided by the authors. Congratulations on having your paper accepted to the NeurIPS 2021 Datasets & Benchmarks Track! When preparing the final version of the paper, authors are strongly encouraged to take into account the issues raised by Reviewers wnP1 and rwpd.",21-144.txt,1
63,https://openreview.net/forum?id=UuUbIYnHKO,1,4,4,"Given the authors say the InfoNCE require more negative samples, the authors should at least attempt to describe the computational scaling requirements of models using InfoNCE for comparable performance or compare the computational requirements for each method to achieve what are usually small marginal advantages over other methods, so the reader can understand whether fractions of a percent of accuracy are worth additional compute. Along those lines, the authors should at least run experiments on a larger GPU mem or mini-batch (e.g., cluster GCN) experiments that run OOM.. a failed experimental implementation should not be reported in the table. The intereptation of Observation 6 is not much of an argument at all but a speculation and should be removed to strengthen the paper. Instead, adding details on the datasets used might be helpful, and esp. to ensure that accuracy alone is fair, e.g., that there are not class imbalances.",21-145.txt,1
63,https://openreview.net/forum?id=UuUbIYnHKO,1,7,2,"I think the performance of feature augmentations alone (FM/FD) should be plotted in Figure 3 as well (so that it is easier to see hybrid augmentations perform the best compared with all non-hybrid options). Similarly, the performance of stochastic augmentations alone (FD/ND) can be plotted in Figure 4.",21-146.txt,1
63,https://openreview.net/forum?id=UuUbIYnHKO,1,7,4,"Instead of leaving it in the appendix, more emphasis should be put on negative-sample-free methods, as negative sampling can be too expensive for large-scale real-world datasets.
I am willing to increase my score after clarifications from the authors.",21-147.txt,1
64,https://openreview.net/forum?id=BlcUQYxknbX,1,6,4,"The image quality of the figures is low.
Fig2 and Fig3 have a dot in the center of the page 5 lines below the image.
Table 2 extends beyond the page margins.
Varying number of significant digits in Tab 2.
typos L147 the gain the accuracy",21-148.txt,0
64,https://openreview.net/forum?id=BlcUQYxknbX,1,7,2,"Feedback is provided in no particular order.
Figure 1: I found the first figure slightly confusing. For example, for the Saliency bias portion, it is unclear what the task is. The left position would imply the task is to find the plane, but I am unable to clearly see the image. Furthermore, the class negative bias portion utilizes two colors (orange and white) for masks, but it’s unclear what orange refers to here.
L70: there seems to be a typo using both determine and gauge in succession.
Sec 3.2 and Figure 2: It would be good to clarify that DeepLab v3+ is a supervised baseline. While the subplot header indicates that, I think stating it in the caption/text would make it easier to understand the point being made.
Sec 3.2 and Figure 2: I think the point being made by the authors here is that models with poor OSS test accuracy result from their presence in the training data. However, the plot being shown doesn’t clearly show that. First, it’s unclear which model is being shown in the right portion of Figure 2: relative performance between classes 3,4,5 does not match the OSS or Supervised model from the left pane. Second, it is unclear if comparing the performance of the model with its presence supports the point being made. Rather, one would be interested in seeing the relative performance drop compared to presence. If so, then a plot showing the correlation between those two variables would be more convincing.
The examples provided in Figure 3 for L4 and L5 seem flipped.
Table 2: this table exceeds the right margin of the document. Furthermore, the naming of the rows is very confusing and requires a lot of cross referencing between the text and the figure to understand what is being tested. I would suggest that the authors use more descriptive row headers and reformat the table to improve readability.
L242: Why is CLIP features a good measure of image similarity with respect to this task? While I could imagine this being a useful heuristic, its use here is not supported by empirical results (experiments or prior work) or some justification in writing.
L410: broken reference.
L251: The authors discuss weighting the different elements of the SCS metric using w_j, but it is unclear how w_j is computed.
The notation for equations 2 and 3 are not standard with some of the elements in equation 3 being undefined (eg, M_q).
The word “complexity” is used to refer to the first tier of the TOSS, but it’s not clear what the authors actually mean by that. Furthermore, the first tier splits the test pairs according to saliency and difficulty of each pair (evaluated based on a current method). It’s unclear how complexity can refer to those two dimensions.",21-149.txt,0
65,https://openreview.net/forum?id=0uQIr4XA77f,1,9,4,"The paper received somewhat diverging ratings, from strong accept to weak reject. The AC read the paper, the reviews and the authors' responses. I do understand the reason why some concerns were raised by reviewers about what it means to track every pixel and other such nuances, but I do find myself to side with the authors. The dataset is interesting and opens the door for more comprehensive and challenging visual recognition. The paper is well written and well executed. So, congratulations to authors!!",21-150.txt,0
66,https://openreview.net/forum?id=jyd4Lyjr2iB,1,7,3,"The reviewers all liked the paper. The authors' response clarified some important points. In view of that, the authors are strongly invited to take the feedback on board for the final version.",21-151.txt,0
66,https://openreview.net/forum?id=jyd4Lyjr2iB,1,7,3,"General Comment
We thank the reviewers for their hard work and detailed feedback.
We were glad to see that the reviewers felt that the benchmark “tackles an important problem… which is critical for safety-critical applications” [7w45], has “important social implications, since it aims to evaluate how a medical diagnosis system would perform in a real-world clinical setting” [LzQ9], and presents “tasks [that] are similar to what a real-life predictive model may encounter” [9xr4].
Furthermore, we were pleased that reviewers found that our experimental evaluation “is done appropriately” [7w45], considers a “multitude of uncertainty methods” with “sound evaluation methods and metrics” [9xr4], and successfully “demonstrate[s] that [our] proposed tasks are more informative of real-world model performance than standard metrics” [LzQ9].
Lastly, we were glad that reviewers appreciated our efforts on the code release, noting that the “code release appears to be quite polished and should help simplify user interaction and future experimentation” and is “very well documented” [7w45], and that “all data and code used in the paper is publicly available” [9xr4].
Suggested Experiments
We conducted two additional experiments suggested by Reviewer 7w45:
An investigation of how changes in preprocessing affect downstream predictive performance and quality of uncertainty estimates;
An investigation of how the quality of uncertainty estimates varies with the underlying clinical labels 0-to-4.
Summary of Results
Link to Results | We varied a Gaussian blur hyperparameter, where a higher “blur constant” results in stronger smoothing on examples during preprocessing (example.pdf in folder). We found that on in-domain evaluation across the Severity and Country Shift tasks, the highest blur constant (the default, 30, used throughout previous experiments) performed best. Surprisingly, the trend was reversed for the top performing MC Dropout methods on the Severity Shift distributionally shifted evaluation dataset, with the lowest smoothing improving accuracy at 0 referral by 10% over the default setting. Additionally, for MAP (a deterministic model) and Deep Ensembles on the Country Shift, a middle-range blur constant significantly improves performance over that of the default across referral rates. This experiment highlights the importance for practitioners to test changes in experimental setup across uncertainty quantification methods and metrics.
Link to Results | We find that MC Dropout and function-space variational inference (FSVI) have the highest quality of uncertainty estimates on the distributionally shifted evaluation datasets, across all clinical severity labels, though no model produces good uncertainty estimates on APTOS label 1.
We have updated the manuscript to reflect your suggestions and updated the supplement (Appendix D) with figures and tables presenting the results above as well as further analysis.",21-152.txt,0
66,https://openreview.net/forum?id=jyd4Lyjr2iB,1,7,4,"I see that the APTOS 2019 dataset was graded by clinicians using the same 0-4 scale as the EyePACS dataset. You may want to mention this in the description of the APTOS dataset
Keep the symbol for “referral rate” consistent. You define it as “X” in Section 2.4, then redefine it as “\tau” in Section 2.6. Also note that “\tau” is already used to denote the precision in the prior distribution over the network parameters in Section 5.1.
In Figure 6 (supplementary information) – are these images only from the EyePACS dataset? Or do these include APTOS images as well? It will be helpful to the reader to make that distinction. Also, including example images from both datasets may help the reader further understand the difficulty of the Country Shift prediction task.",21-153.txt,0
67,https://openreview.net/forum?id=SSKZPJCt7B,1,8,4,"In this paper the authors provide a benchmark for adversarial robustness, enabling testing with a wide variety of attacks, nuanced discussion of incorporating adaptive attacks, and already evaluations and open-sourcing of numerous models.

The largest concern raised by multiple reviewers was on the limitations of this benchmark. As with most benchmarks, the discussion of the limitations of this leaderboard is crucial so as to not discourage new research outside the scope of focus of this framework. I'd strongly encourage the authors to continue to clarify and qualify the limited scope of this evaluation, e.g. in how the work is titled, on the website, etc.

That said, all reviewers agreed this was a very valuable resource to the community and as such I believe it should be accepted.",21-154.txt,0
67,https://openreview.net/forum?id=SSKZPJCt7B,1,7,3,"I found the orange text in Figure 1 and the website a bit difficult to read. (The contrast ratio is below the value recommended by WCAG: https://web.dev/color-and-contrast-accessibility/)
The model zoo seems very useful. Is there any harm in making it a requirement to integrate a model into the model zoo as part of submitting a new defense for inclusion in the leaderboard?",21-155.txt,0
67,https://openreview.net/forum?id=SSKZPJCt7B,1,7,4,"The authors have already published an article of the same title in the ICLR 2021 workshop on Security and Safety in Machine Learning Systems, see here https://aisecure-workshop.github.io/aml-iclr2021/papers/47.pdf
There is significant overlap between the two versions, albeit the current version is substantially fruther developed. I did not consider this in my review (also because i am not fully aware of the details of Neurips policy on this kind of situation) so far and i'd like to give the authors the room to justify their new submission to this conference, if necessary at all (e.g. it may already have been cleared).",21-156.txt,0
67,https://openreview.net/forum?id=SSKZPJCt7B,1,7,5,My concerns are listed in weakness. I think the benchmark is an interesting work to the community and I hope the authors keep the promise to regularly update it.,21-157.txt,0
68,https://openreview.net/forum?id=qkcLxoC52kL,1,9,4,"In this paper the authors present three real-world datasets in different domains to test graph ML capabilities in three distinct tasks. The authors present baselines and learnings from these datasets being used in the 2021 KDD Cup. All reviewers found the paper to be strong and valuable, and as such I believe it should be accepted.",21-158.txt,0
68,https://openreview.net/forum?id=qkcLxoC52kL,1,8,4,"The authors may discuss how the data is cleaned and how to ensure the label quality, especially for the MAG240M dataset, where arXiv subject areas may not be always accurate and moderators often change the initial category chosen by the authors.
Since there are different challenges for different datasets, I wonder whether the OGB team will adopt more evaluation metrics, not only limiting to performance. For example, I am in particular interested in resource-constrained scenarios for giant graphs, considering common practices like the neighborhood sampling are computation-intensive.
Some key takeaways summarizing common practices could be provided at the end of Section 2.",21-159.txt,0
69,https://openreview.net/forum?id=1k4rJYEwda-,1,7,3,"  All points covered in the main response.
  Update post-rebuttal: The authors have gone to great lengths to address all concerns raised by the reviewers and I have upgraded my score to 7 to reflect the improvements made to the submission!",21-160.txt,0
69,https://openreview.net/forum?id=1k4rJYEwda-,1,7,4,"Overall I do not see the paper as having a significant contribution. The largest contribution is organizing existing code/benchmarks into a consistent API. I see no discussion of maintenance plan for this framework, and I think it's design of many docker containers and the constantly evolving nature of deep learning frameworks puts it at risk of needing continuous effort to avoid bitrot.
Beyond packaging existing benchmarks, a few basic ML algorithms are included, yet with no more than a paragraph of discussion. Not even an explanation of the kernel being used for the SVM is provided! The results seem to themselves provide no evidence that these new benchmarks provide some information not available from the existing benchmarks, with very similar mean-rank curves in plots. I think a much more thorough and thoughtful discussion of what existing benchmarks do not cover, why that is a problem, and how a expanded set of benchmarks will rectify those issues, would bring this paper to a much greater level of contribution and significance. In it's current form it feels like a bunch of popular ML tools were thrown in ""just because"" and left at that.
After revision the paper is much improved, uses a proper hypothesis testing approach, and is much easier to read for both new and knowledgable researchers of HPO methods. IMO this work now passes the bar for a NeurIPS paper and am comfortable with acceptance. I do still think the references I provided in initial review would be valuable to fully incorporate, particularly ones like repo2docker that discuss reproducibility with respect to characteristics of this system.",21-161.txt,0
70,https://openreview.net/forum?id=XyDozX3_L4l,1,6,3,"The paper presents a set of semantics-preserving spoken-language transformations that can be applied to existing datasets. The goal is to provide a mechanism for evaluating the generalizability of learned models in the face of changes to the language that preserve meaning and are consistent with human speech behaviors and ASR-induced errors, in order to measure the overfit of especially deep learning methods on existing benchmarks. Extensive evaluations, comparisons, and analyses are provided, including baselines for other forms of language perturbation. The datasets provided are large enough and well-evaluated enough to be of use, and supports the basic claim that existing models are prone to overfitting on surface patterns. The proposed transformations are intuitive and the paper is well written.",21-162.txt,0
70,https://openreview.net/forum?id=XyDozX3_L4l,1,6,3,The link you provided in the comments does not work. I do not know whether it is the same material that you included in the GitHub repository or not.,21-163.txt,0
71,https://openreview.net/forum?id=sD93GOzH3i5,1,6,3,"The paper proposes a dataset of 10k programming problems to promote research in code generation. The authors then evaluate SOTA language models on the proposed benchmark. All the reviewers agree that the proposed dataset is a good contribution. Therefore, I recommend acceptance of this paper.",21-164.txt,0
71,https://openreview.net/forum?id=sD93GOzH3i5,1,6,3,"I played a little more with the dataset.
Unless I'm mistaken, it seems that over 40% of the the test case solutions y are in {0...10, -1, yes/no}
So if you can do well on a 12-way multiple choice test, you may be able to do well on the dataset
Here is the command I ran:
>> mean([str(y).strip().lower() in ""-1 0 1 2 3 4 5 6 7 8 9 10 yes no"" for xy in input_output for y in xy[""outputs""]])
0.43534307850557247
The > 40% rate holds across competition/interview/introductory levels.
This seems relevant to the 20% test case accuracy touted in the abstract, but I did not see such statistics discussed in the paper.",21-165.txt,0
71,https://openreview.net/forum?id=sD93GOzH3i5,1,6,3,"Given the above mistakes found in the dataset, how should we trust that other things like de-duplication/etc. were done carefully. For example, a simple test would be to run the solutions from one problem on all others -- one would not expect the solution to a non-trivial problem to solve another non-trivial problem. Have you tried this? Did you find that any solutions to some problems solve others? This paper does not seem to discuss the checking and double-checking that should go into creating such a valuable resource.",21-166.txt,0
71,https://openreview.net/forum?id=sD93GOzH3i5,1,7,5,"Show some examples of successful code generation to see how simple they are (e.g. are they one-liner or more complex). Are the generated code examples in the supplementary correct code? Show the performance metrics for the pure pre-trained (not fine-tuned) GPT-2 and GPT-Neo models to get a baseline and an indication of how much fine-tuning helps. Use sequentially bigger fine tuning training set (1000, 2000, 5000 samples, etc) to see how the strict accuracy reacts to fine tuning training dataset size",21-167.txt,0
71,https://openreview.net/forum?id=sD93GOzH3i5,1,7,4,"Figure4 label is confusing: It shows the size of the respective GPT-Neo models, but states “few-shot” for GPT-3 instead of the size.
L308: “Open AI Codex is a 12B parameter Transformer language model” typically, Vaswani’s encoder-decoder transformer model is referred to as the Transformer, which incorrectly suggests that Codex is an encoder-decoder model, while Codex is the GPT-style model (decoder-only transformer). While language modeling is possible with encoder-decoder models, throughout the task you distinguish two models (encoder decoder Transformer and GPT) so might as well do it here.",21-168.txt,0
72,https://openreview.net/forum?id=DjzPaX8AT0z,1,7,4,"Dear authors,

Thank you for submitting your paper and addressing the issues raised by the reviewers.

The paper proposes a new benchmark for meta-learning based on channel coding. The reviewers commend the practical relevance of the considered task as well as the possibility to directly control the difficulty of the task. Limitations include the lack of newly emerging classes at test time and the fixed shot/way setting of the benchmark.

The scores of the reviews are 7,6,5. The main criticism of reviewer 3, was the relation to real world problems, which was adequately addressed by the authors in their rebuttal. Considering this as well as the high confidence of reviewer 1 giving a score of 7 is sufficient to accept the paper.",21-169.txt,0
72,https://openreview.net/forum?id=DjzPaX8AT0z,1,6,2,"Dear authors,
the provided URL to the repository https://github.com/ruihuili/MetaCC is invalid.
Please provide a working URL or clarify that all data/code/information necessary for reproduction is available in the supplementary material.",21-170.txt,0
73,https://openreview.net/forum?id=gN35BGa1Rt,1,7,2,"This appears to be a large initiative. It would be of interest to the field for the authors to describe in more detail the unique challenges and logistics behind co-ordinating such an effort. For example, they allude to the requirement for the maintenance of a cold chain for some the samples/reagents/",21-171.txt,0
74,https://openreview.net/forum?id=43mYF598ZDB,1,6,2,"The paper proposes CLEAR - an image recognition benchmark for continual learning based on YFCC100M dataset. The reviewers agree that this benchmark departs significantly from existing datasets in this space in terms of task definition and scale. The scale is achieved by relying on recent high-performance vision-language models (CLIP) and crowd-source effort to ensure quality and deal with certain ethical aspects (e.g. remove inappropriate images). To have a more natural CL setting, the benchmark task is built around the temporal evolution of visual concepts (e.g. computer, dress, etc) over a decade. Since the proposed benchmark is a curated subset of YFCC100M, the authors recommend (and show the benefit of) using the remaining part of YFCC100M for semi-supervised and unsupervised learning, highlighting the shortcomings of current fully-supervised methods for CL.

Despite some initial disagreements around terminology and claimed contributions, the reviewers and the authors engaged in a very constructive review process, resulting in a significant improvement of the submission. After detailed discussion among the reviewers and the area chairs, we concluded that this paper makes a significant contribution to the CL field and has high potential to drive research in the field. There are still some concerns about some of the scores being close to saturation and clarity of describing the contributions, and we strongly encourage the authors to revisit these aspects for the camera ready version.",21-172.txt,0
74,https://openreview.net/forum?id=43mYF598ZDB,1,7,3,"As a minor point that the authors can just improve upon easily, instead of writing “see supplementary for details”, there could just be a direct reference to section, figure, table etc. in the appendix.
The authors should ideally host their dataset in a form that has a DOI and supports versioning for future changes and reproducibility. Zenodo could be one popular choice.",21-173.txt,0
75,https://openreview.net/forum?id=NxWUnvwFV4,1,7,3,"Overall I am positive about this work, my main concerns are as follows:
Currently, the work is limited in scope (only a single task and a few attacks that are being evaluated). How do the authors
Plan to expand the scope of their work?
Plan to keep their benchmark current and useful with the state of the art attacks?
I am a bit concern that due to the authors choice of splits may hinder generalization to real world scenarios and as such may impact how useful this work actually is. Can the authors either provide an argument for why my concern is unlikely to ever materialize or matter, or could they add in some more splits to test their data on to ensure that anything observed in this work generalizes?",21-174.txt,1
75,https://openreview.net/forum?id=NxWUnvwFV4,1,4,4,"A robustness benchmark for graph machine learning certainly seems useful to have! GRB is heading in the right direction, but I think it is critical to address the weaknesses above before such a benchmark is relied upon by the research community.",21-175.txt,1
76,https://openreview.net/forum?id=FZBtIpEAb5J,1,6,2,"This paper describes a large scale (>10 mln) dataset of atmospheric radiative transfer data spanning a large time period (40 years) and has various out of distribution test conditions. It also provides a good set of benchmark results. The reviewers agree that this dataset is an important contribution to enable the development of ML methods that help tackle some of the problems with climate change and weather. The paper is well written, and the dataset is well documented. For this reason I recommend acceptance,",21-176.txt,0
76,https://openreview.net/forum?id=FZBtIpEAb5J,1,7,3,"*Post dicussion period
First of all, thanks for the crafted rebuttal in such a short time! Sorry for the part-missing question as I didn't check thoroughly the content.
After viewing the author's rebuttal (and modified version), I am happy to change my review score to ""6: Marginally above acceptance threshold"" as they addressed my major concern of necessity of ML emulators. Although I still think improving the simulation code (to make it GPU-friendly) is a more promising goal, as authors pointed out, this dataset is still valuable before that actually happens.",21-177.txt,0
77,https://openreview.net/forum?id=KGeAHDH4njY,1,8,4,There is strong consensus amongst the reviewer that this paper offers a valuable contribution to this track. Reviewers are in agreement that the contribution is significant and timely and especially appropriate for the first iteration of this new NeurIPS track. I am very pleased to recommend acceptance into this track as an oral presentation.,21-178.txt,0
77,https://openreview.net/forum?id=KGeAHDH4njY,1,10,5,"Section 3: It would be worth citing here the literature on citations of retracted scientific papers, which shows a similar effect. For example, Brainard, Jeffrey, and Jia You. ""What a massive database of retracted papers reveals about science publishing’s ‘death penalty’."" Science 25.1 (2018): 1-5. A comparison & contrast with retraction of scientific papers might further illuminate the possible steps forward. In particular it seems worth nothing that in scientific paper retraction, a reason typically is given and the paper remains archived, two points raised in Section 3, but unfortunately the paper itself often remains available without its retraction context, as described in:
da Silva, Jaime A. Teixeira, and Helmar Bornemann-Cimenti. ""Why do some retracted papers continue to be cited?."" Scientometrics 110.1 (2017): 365-370. One might worry that if the retraction reason is not archived in every place the dataset is archived, the existence of a publicly-accessible reason will not do as much to prevent continued use as might be hoped.
Section 4:
I was not clear on how to interpret the points presented in this section — what should be done to mitigate this kind of harm or who was responsible for the harm. It seems very true that new applications may raise new ethical concerns, and that people who are not the dataset’s author may transform the datasets in ways that make previously benign datasets problematic, but what is the normative significance of this? Is the implication that this is bad because the authors may be associated with the new applications (and therefore in some way responsible for them?) but have no control over their production? Or is the concern that transformative use of the dataset is in some way different in kind than the same bad or misguided actors creating their own irresponsible dataset from scratch? If this is an ethical risk, is it one that dataset authors can reasonably predict or control? My sense is that it is not — there is nothing a responsible dataset author could do to prevent later irresponsible use of a dataset — so I am not sure what stewardship could mitigate any of the dataset harms mentioned here.
Section 6:
I thought the story in this section was much more interesting and nuanced than the summary at the beginning of the first paragraph or the quotation at the end of the last paragraph. What is interesting about the cases you raise is that they are not the same. In the case of LFW, auditing and activist work in combination with increased public awareness and concern about algorithmic bias/fairness seems to have brought about a genuine shift in analysis of the “diversity” of the dataset. By contrast the ImageNet story, as the paper notes on 216, is murkier in that the offensive labels would have been similarly offensive when ImageNet was still released. Thus the story can’t be summarized as “what society deems fair and ethical changes over time.”
Section 7 Harms and misuses related to current practices were especially clear & well connected here.
Line 343-346 does not seem actionable.",21-179.txt,0
78,https://openreview.net/forum?id=H-d5634yVi,1,10,4,"This paper presents a dataset of chest X-Ray images with VisualGenome-style scene graph annotations. All reviewers acknowledge the value and significance of the dataset and gave positive review scores, two of them are 10. Although the dataset mainly consists of silver (automatic) annotation, it also incorporates a portion of gold annotation to discuss the quality of the former, which is highly appreciated. Overall, this is a very good paper that will significantly contribute to promote researches toward high-quality medical image diagnosis in an explainable manner.",21-180.txt,0
78,https://openreview.net/forum?id=H-d5634yVi,1,10,2,"On page 3, Methods section, first paragraph, last sentence: there is a typo. “HIPPA” should be “HIPAA”
On page 5, Silver Dataset section, the authors state: “we manually validated or corrected the bounding boxes for 1,071 CXR images…” Who is we? Clinicians? What level of experience? Please clarify this in the text.",21-181.txt,0
78,https://openreview.net/forum?id=H-d5634yVi,1,6,4,"Evaluating anatomy object detection for CXR images: the annotator would localize the bounding box based on the medical report's global “weak” label. If the annotations don’t agree with the report, how could we treat these cases?",21-182.txt,0
78,https://openreview.net/forum?id=H-d5634yVi,1,8,4,"I think it would be nice to include a summary of the results of Table 4 (from PhysioNet) into the main paper. As a reader it's very important for me to know how good was the bounding box algorithm.
UPDATE: Great job incorporating the feedback! Looking forward to the camera-ready version",21-183.txt,0
79,https://openreview.net/forum?id=FQLzQqGEAH,1,5,4,"The final scores for this paper are 9, 7, 6, 4. There were some concerns about whether this paper was sufficiently significant given that it analyzes only one dataset, but the authors released results on Twins, and now the majority of reviewers recommends acceptance and there is a champion. The reviewer who recommended 4 also has not responded and has not commented on the new results on Twins. I have to say I disagree with the authors about their claim that the negative reviewer has issues with the fit of the paper for this track (I think their main criticism is about using only one dataset). In any case, I feel like there is enough support for this paper.",21-184.txt,1
80,https://openreview.net/forum?id=e82_BlJL43M,1,7,3,"This paper presents a method of benchmarking robotic manipulation tasks across a variety of hardware and environments. The core idea is that multiple different groups implement a set of baselines, and the local performances on each baseline are combined into a global ranking against which work can be compared. The paper introduces four baseline tasks, which are tested in two different research settings.

The approach proposed in the paper addresses a real existing problem in robotics, where different platforms and environments tend to make comparison of research results difficult. The approach is clearly and thoroughly explained. As conceived, the ranking approach is amenable to being extended with additional baselines and is constructed to have additional research groups join. However, some concerns remain. The problem explicitly omits the vision and grasping components of the manipulation pipeline, which may not be appropriate in all cases. More seriously, when a new method/model is proposed, it must be implemented by multiple groups in order to receive a global ranking, and it's not clear how the purely local ranking should be considered; both for this reason and in general, the proposed benchmark is of most use when there is significant buy-in from multiple research groups, which may be difficult to bootstrap. Reviewers also have concerns about the potential for bad actors to affect the system by maliciously manipulating their claimed local results; however, these concerns are consistent with the current state of the art in robotics. Authors have engaged constructively with reviewers, and are encouraged to thoroughly incorporate improvements from these discussions in subsequent versions of the paper.",21-185.txt,0
80,https://openreview.net/forum?id=e82_BlJL43M,1,7,3,"The title suggests a generic robotics benchmark while the paper is specifically about manipulation. This must be adapted to avoid confusion.
My rating is based on the fact that there are several open questions about implementation, utility, and documentation. I am open to raising the rating if my concerns are properly addressed. Overall, the proposed approach is a step in the right direction.",21-186.txt,0
81,https://openreview.net/forum?id=-or413Lh_aF,1,6,2,"This paper proposes a dataset and a benchmark for materials design. It is an important contribution that would allow ML innovations in this important area of research. The reviewers mostly agree that the paper is well written, introducing material design well to the ML audience. The dataset is well documented and formatted, and appropriate baselines are presented. This paper is an important contribution to multi-disciplinary research in an important area of science, so I recommend acceptance.",21-187.txt,0
81,https://openreview.net/forum?id=-or413Lh_aF,1,8,2,"I do must admit that I have very little experience regarding the field of artificial electromagnetic materials (AEMs), therefore my review mostly focuses on the general benchmarking part of the paper.",21-188.txt,0
81,https://openreview.net/forum?id=-or413Lh_aF,1,8,4,"Figure 4
the Color dataset results seem to be cropped slightly
Figure 5:
Not sure which best performance means in this case? Would that mean lowest average MSE across datasets? That would just mean that number of parameters are log ratio'd by their performance on the Color dataset, which seems like a weak comparison.
Supp. Figure 2:
missing a-f labels.
Minor Comments
3.3: ""a parameterized geometry g – which is of order g ∼ 101 – of AEMs as input""
slightly confusing. would suggest either using some notation to indicate that the dimensionality of g is on the order of 10s or remove the em dash entirely
Missing grid search results. The authors state in Section 5 that they performed hyperparameter optimization to find the best performing model using a validation set. It would be helpful to ML practitioners as well as strengthen the paper to include these results.",21-189.txt,0
82,https://openreview.net/forum?id=GEcWUTN1v1v,1,6,4,All reviewers agree this paper proposes a new and challenging reading comprehension dataset in Chinese that goes beyond past work in offering high-quality and diverse reading comprehension questions.,21-190.txt,1
82,https://openreview.net/forum?id=GEcWUTN1v1v,1,5,5,"Given the Reviewers' suggestions, we uploaded an updated version of the paper and the supplementary, with the changes marked in red.
In the main paper, we
highlight that a lot of questions in NCR require the capability of understanding and reasoning with negations; (line 106 ~ 110)
correct the inconsistent numbers; (line 123, 128)
fix the typos; (line 131, 233, 239, 247~250, Table 8)
propose a simple criterion to categorize writing style for training data; (line 176 ~182, Figure 1, 2)
descript the details of human performance and measure the inter-agreement; (line 252~255)
explain the human performance of Table 8; (line 264 ~ 271)
investigate whether an AI model purely trained on modern Chinese can directly transfer to classical texts (line 272 ~278, Table 9)
In supplementary materials, we
submit a separate PDF of the datasheets;
add the truncation details; (Appendix B)
investigate some special patterns. (Appendix C)",21-191.txt,1
82,https://openreview.net/forum?id=GEcWUTN1v1v,1,7,4,"Questions and comments:
In human evaluation, it is unclear how human performance was calculated. Is it an average score among all three student annotators? What is the inter-agreement?
In Table 8, the human performance is somewhat surprised me. As I suppose the performance of modern document to be the highest. However, it seems that modern documents are the most difficult subset. Do you have any further comments on this? The content in line 247-251 seems to be quite shallow to me.
Why are the dev/test sets not classified into modern/classical/poetry categories? It is OK to have an overall score, but we need more precise classifications here. Each category is quite different from others, and thus it is not good to blend them as a whole. After checking the original dataset, the reviewer did not find category information. Without such meta-data, we are not able to analyze how our MRC system performs on each category and adjust the model design for further improvements.
I downloaded the dataset and source code for baselines from https://sites.google.com/view/native-chinese-reader. After a rough scanning, it seems that the baselines are based on Longformer, but not pure BERT/RoBERTa. While in both the main paper and appendix, there are no illustrations on this nor citation to Longformer. This causes a discrepancy of your paper and the actual baselines that were used.
The test set seems to be already made public, which may affect the integrity of testing process for future works. Why did you not keep it as private (as you have organized a competition beforehand)?
Grammatical issues / Typos / Minor comments:
line 126: high level -> high-level
line 222: from a industry -> from an industry
line 228: token -> taken
Table 8: The size of the left and right quote mark differs in '(w/o poetry)'.
Please be consistent in showing your evaluation results. For example, in line 236-240, you use percentage for accuracy, while it is not in Table 7/8/9/10.",21-192.txt,1
82,https://openreview.net/forum?id=GEcWUTN1v1v,1,8,4,"In lines 118-123, the number of questions is not consistent: 20477+193==15419+2443+2613 ?
It may be worth highlighting that a lot of questions in NCR are postulated in a way that requires choosing the one “incorrect” option out of 4 options (the other three are “correct”), which requires the ability to understand and reason with negations.
In Section 4.1, the authors forbid the usage of additional labeled data in the competition (line 218), yet they use additional training data to help finetune the model (line 214). Are these additional data labeled? If so, are these part of the NCR dataset so that competition teams were allowed to use them?
In Section 4.1, did you try fine-tuning your model on only the modern-style part of the NCR dataset? I wonder how well the MRC ability transfers to classical-style documents if it has only seen modern-style data. It may show the importance of having classical-style data in the training set in helping the model understand classical literature.
Tables for experiment results could be improved aesthetically (e.g. using bold fonts, thicker lines, more space).",21-193.txt,1
83,https://openreview.net/forum?id=WV0waZz9dTF,1,7,4,"The data provided in the paper is of relevance for the NeurIPS data track. Based on reviewers opinions, and in particular after discussion with the authors, the paper achieves the minimum score required for publication at NeurIPS data track.",21-194.txt,1
83,https://openreview.net/forum?id=WV0waZz9dTF,1,8,3,"Complete related work with a broader survey of existing land-use datasets.
Consider the enrichment of the dataset with pixel level, open-source, geographical data.
Define in clear terms legitimate uses of this dataset.",21-195.txt,1
83,https://openreview.net/forum?id=WV0waZz9dTF,1,4,4,"I commend the authors for a strong submission to NeurIPS 2021 Track Datasets and Benchmarks; where the empirical data can greatly benefit researchers from various disciplines in studying contemporary social issues.
However, there are some issues above which will need to be resolved or clarified before it is ready for acceptance: the issues raised above are mainly for completeness (in terms of methodology), especially in terms of how the dataset is compiled.
I wish the authors all the best for NeurIPS 2021.",21-196.txt,1
84,https://openreview.net/forum?id=gWIbXsrtOCc,1,6,3,"This work introduced a set of benchmarking environments for inducing causal models in model-based reinforcement learning. Causal learning is an important research direction to improve the generalization and robustness of reinforcement learning algorithms. This work provides one of the first systematic evaluations of causal discovery in two RL environments. Albeit the simplicity of these environments, they can serve as a good testbed to facilitate this line of research. The authors' responses during the discussion period have increased the reviewers' assessments on this paper. All four expert reviewers believe that this work has passed the threshold of acceptance. The AC found no evidence to overturn the reviewers' ratings and would recommend accepting this paper.",21-197.txt,0
84,https://openreview.net/forum?id=gWIbXsrtOCc,1,7,3,"The paragraph ~line 84 seems out of place. It describes the analysis of results in the intro, which just seems too early to have analysis of results before the paper has even introduced the environments, experiments or results. This should maybe be rewritten into a conclusion section or put into experiments.",21-198.txt,0
85,https://openreview.net/forum?id=c20jiJ5K2H,1,8,4,"The authors present MSWC, a large dataset of spoken words across 50 languages extracted from Common Voice dataset. The reviewers agree that this dataset is a significant contribution since it is well constructed and larger in terms of number of words and languages compared to existing resources. I recommend acceptance.",21-199.txt,0
85,https://openreview.net/forum?id=c20jiJ5K2H,1,7,4,"Section 4.4 -> what are the different heuristics and thresholds that are applied? Only one is mentioned here.
Section 5, experimental setting 1: how many samples per keyword are used here?
""we empirically select k=5 as the number of clusters"" -> how was this selected?
lines 167--168: please elaborate and expand more on this as it is a key part of the pipeline
Perhaps you can swap Sections 3 and 4 and/or present the construction of the dataset first and then continue with the data analyses.
The following need to be fixed: line 124: ""we contain"" line 135: ""a give languages"" line 135: ""broadly with"" line 157: ""to grow to include""
Subject--verb agreement errors in caption of Table 6.
line 158: ""audio, and"" -> remove comma line 212: ""lowest"" -> smallest line 253: ""and vice versa"" is not quite right here",21-200.txt,0
86,https://openreview.net/forum?id=vKQGe36av4k,1,6,4,"The paper introduces three self-driving datasets: a multimodal dataset suitable for supervised perception tasks, a Lidar dataset that is the largest existent lidar dataset and suitable for self-supervised learning, and a rich motion forecasting dataset for prediction tasks, given history of object location, heading, velocity, and category. All the reviewers agreed that this is a good paper, providing a very diverse and useful resource for the self-driving community. The authors updated their submission to take into account reviewers' comments.",21-201.txt,0
86,https://openreview.net/forum?id=vKQGe36av4k,1,7,3,"Relationship to other datasets
We have added new comparisons to the manuscript to clarify why our datasets are “more challenging or an interesting addition w.r.t. existing datasets” (R1) and to add “justification for the necessity of the proposed dataset over existing ones” (R2). “there is no direct diversity comparison” (R3)
Sensor Dataset: Figure 1, comparing the number of cuboids per category, is expanded to compare with ONCE and Waymo, in addition to nuScenes. ONCE and Waymo have smaller taxonomies. We expand Figure 2 left, showing the number of cuboids as a function of range, to include Waymo, ONCE, and nuScenes. This figure shows that we have dramatically more objects beyond 70 m in range than existing datasets. We also add a new figure, 2 right, which is a histogram over the number of objects present in each lidar frame. The average number of cuboids for Argoverse 2.0 is 75, compared to 33, 61, and 29 for nuScenes, Waymo, and ONCE, respectively. We include a new figure, 3 left, which is a histogram over the number of object categories simultaneously present in each lidar frame. This figure shows that our dataset has far more frames with 5+ categories of objects than existing datasets. The mode of this distribution is 8 objects for Argoverse 2.0, compared to 4 for nuScenes and ONCE, and 3 for Waymo. And while any expanded taxonomy is naturally heavy tailed, just as the real world is, we still have enough instances to evaluate detection and tracking for 26 different categories compared to 10 for nuScenes. Finally, we add Figure 3 right showing that our new sensor dataset has more moving vehicles than any other dataset.
Motion Forecasting Dataset: We expect Argoverse 2.0 to be more challenging and interesting than other relevant datasets. Since all the datasets have different raw format and source, it is non-trivial to compute the interestingness score for all of them using the exact same methodology. As such, we rely on some derived experiments and stats to compare the challenging/interesting nature with some of the other datasets (1) Table 2 shows that Argoverse 2.0 has the maximum number of object types. Methods now need to account for object type specific behaviors, e.g. BUS usually has a high turning radius, CYCLIST can cut through narrow spaces. It also shows that Argoverse 2.0 has the maximum scenario duration and maximum roadway coverage, which would make the forecasting methods learn from longer sequences and diverse road geometry. (2) We have updated the paper text to include Figure 6, which shows that Argoverse 2.0 achieves a much higher interestingness score compared to Argoverse 1.1.
The same plot also shows that the interestingness score has a high correlation with Miss Rate metric for a particular motion forecasting method, thereby justifying that the interestingness score computed here is indeed a reliable measure of difficulty. (3) Table 2 in supplementary shows that transfer learning from 2.0 to 1.1 is more useful than the reverse despite having a lesser number of scenarios. This reinforces the fact that Argoverse 2.0 is more challenging than its predecessor. (4) Finally, we compare the metric values between the state-of-art methods on the Waymo motion dataset and Argoverse 1.1. For a 3 second horizon on vehicle object types, the ones on Waymo achieve a minFDE of approximately 0.51 m. This is much lower compared to 1.13 m achieved on Argoverse 1.1. Further, the same method stands 1st on Waymo’s leaderboard (under the name Multipath++) and 5th on Argoverse’s (under the name poly).
We have added additional text to the related work section to more concretely position Argoverse 2.0 relative to other forecasting datasets.
Lidar Dataset: There is only one similar dataset, ONCE. Keep in mind that ONCE is concurrent work, released this summer and under submission to this same dataset track. Our lidar dataset consists of 6 million frames vs. 1 million for ONCE. ONCE samples are drawn at 2 hz instead of 10 hz for us, so the time period covered in each dataset is similar. However, our higher time resolution makes our dataset more suitable for tasks related to motion, e.g. the point cloud forecasting task we explore. Our dataset also contains HD maps for each scenario, while ONCE does not.",21-202.txt,0
86,https://openreview.net/forum?id=vKQGe36av4k,1,7,4,"line: 73-74:
Table 4 (+ text): The definition of ""social context"" is not clear from the text.
line 299 (Limitations): ""... and we would do so [correct egregious mistakes] for these datasets"". Why was this not already done? It should be part of preparing and cleaning a dataset.
line 278: ""significant increase in dataset difficulty when compared with AV 1.0"". Could you explain why? It's not clear from the text and numbers.
line: 73, 74: is there groundtruth data for optical or scene flow?
preview2 (sensor dataset): the bounding boxes for the pillars seem to be off and/or inconsistent. Is this a data problem or a visualization problem?
Motion forecasting dataset: what is the main difference to the Waymo and Lyft datasets? Why should people use the proposed motion forecasting dataset? I understand that there is table 1 which illustrates this to some degree but it would help to explicitly state the important/key differences to motivate the usage of the dataset.",21-203.txt,0
86,https://openreview.net/forum?id=vKQGe36av4k,1,6,5,"Here are some suggestions:
(1) Diversity comparison. The authors can give some concrete examples to illustrate how their dataset is more diverse than previous datasets, or design some diversity comparison experiments.
(2) The authors should at least provide some self-supervised learning results on their lidar dataset. It seems that only point cloud and motion forecasting results are presented.",21-204.txt,0
86,https://openreview.net/forum?id=vKQGe36av4k,1,7,4,"I appreciate the hard work of the authors for this nice dataset. However, I will be more confident if the authors can offer more results or discussions regarding the perception tasks, clarify the reason why they build three separate datasets, and illustrate the dataset limitations comprehensively. I will raise my score once the authors address my concerns.
The authors addressed my concerns, and they also improved the paper with more results and discussions. So I would like to raise my score from 5 to 7.",21-205.txt,0
87,https://openreview.net/forum?id=GF9cSKI3A_q,1,8,4,"This is an interesting submission and all reviewers recommend acceptance, hence I also recommend accepting the paper.

My only concern is whether adversarially constructed test sets are truly harder than non-adversarial distribution shifts. For instance, the Swag benchmark was constructed via adversarial filtering and current models achieved only low performance scores, but two months after the release of the Swag paper, the BERT paper achieved the same performance on Swag as a human expert. Similarly, models have made quick progress on the adversarially-filtered ImageNet-A test set (e.g., see Figure 4 in https://arxiv.org/abs/2007.00644). I would be curious to hear the authors' perspective on this point (and it may be a valuable addition for readers of the paper as well).",21-206.txt,0
87,https://openreview.net/forum?id=GF9cSKI3A_q,1,8,3,"In line 43, consider replacing ""human"" with ""humans"".
In section 2, you give a clear motivation for your work, citing that it is hard to guarantee the validity and soundness of existing toolkits and benchmark datasets. However, I believe it would be helpful to relate some of your results from sections 3 and 4 to prior work, namely
ALBERT(XXLarge) is the most robust to adversarial attacks
Typo-based Perturbations such as text-bugger tend to be the most effective
Most SOTA language models evaluated on GLUE are considerably vulnerable to adversarial attacks
In either case, relating these conclusions to prior work can only bolster your overall paper. If these observations are consistent with prior results, then mentioning this would likely give more credibility to your overall benchmark. If these results are new, then emphasizing their novelty will make for an even more interesting paper.
I believe that motivating your inclusion of the Fleiss Kappa statistical measure would add clarity to your paper, similar to how you justified using the Filter Rate.",21-207.txt,0
87,https://openreview.net/forum?id=GF9cSKI3A_q,1,8,4,"Can you clarify my question above regarding ALBERT?
Can you add confidence intervals (e.g., for multiple fine-tuning runs) for each perturbation & task combination, especially when the #samples is small?",21-208.txt,0
88,https://openreview.net/forum?id=74TZg9gsO8W,1,7,3,"The data provided in the paper is of relevance for the NeurIPS data track. Based on reviewers opinions, and in particular after discussion with the authors, the paper achieves the minimum score required for publication at NeurIPS data track.",21-209.txt,0
88,https://openreview.net/forum?id=74TZg9gsO8W,1,6,4,"I believe the previous reviewers’ criticisms were appropriately tackled, and I would be willing to increase my score if the authors show evidence of being receptive to implementing some further issues and suggestions raised in this review.
Please be aware than the link to the past reviews does not give me access to view them, so if I contradict any individual comments made there, it was because I have no way of seeing the full reviews and rebuttals. Perhaps the area chairs can open those up if we need to discuss any points from there further.
Update: Post rebuttal
I have raised my score to 7 and increased confidence to 4 based on the author's responses during the rebuttal period. A few outstanding changes remain, which I would like to see implemented before the camera ready deadline to keep this score. Many thanks to the authors!",21-210.txt,0
89,https://openreview.net/forum?id=cXCZnLjDm4s,1,7,3,"3 out of 4 reviewers recommend accepting the paper and the concerns of the 4th reviewer are clearly discussed in the authors' feedback. Overall, HD maps have been heavily used in production systems for autonomous driving. The authors leverage the industrial resources and open the venue for researchers to study the HD map maintenance problem. The contribution can be very important for the machine learning research.",21-211.txt,0
89,https://openreview.net/forum?id=cXCZnLjDm4s,1,7,4,"See weaknesses.
The authors addressed my concerns, and they also improved the paper with more discussions. So I would like to raise my score from 5 to 7.",21-212.txt,0
90,https://openreview.net/forum?id=O24OhmqpJtP,1,6,3,All reviewers agree on acceptance. I recommend the authors to look at the comments by the reviewers and use these to improve the paper for the camera-ready version.,21-213.txt,0
90,https://openreview.net/forum?id=O24OhmqpJtP,1,7,4,"Please excuse my review for being brief, I was called in last minute. In particular I have also not yet had the opportunity to have a good look at the code/tools provided, or the reviews on the previous version, or even to sit down to mull things over.",21-214.txt,0
91,https://openreview.net/forum?id=OCrD8ycKjG,1,6,5,"This paper's review have quite high variance (4/5/6/8).

The reviewer who scored a 4 did not express high confidence (3) and their main concern with the paper is that it is out of scope as it details a software platform. I agree with the authors here that this paper is well within the scope of the track and am heavily downweighting this reviewer's score.

The reviewer who scored a 5 gave a very lightweight review with the main concern being limited conceptual novelty. I have read the authors response to this point, and believe it clarifies and addresses the point about novelty. The reviewer has clarified that they remain unconvinced that the paper is an interesting contribution, but that there are no major flaws that would block publication.

The two positive reviews (6/8) have provided more substantial constructive feedback for the authors most of which have been addressed.

After conferring with another AC I have decided to recommend acceptance of this paper.",21-215.txt,1
91,https://openreview.net/forum?id=OCrD8ycKjG,1,8,5,"the autogenerated figures on the openml website under Analysis could use work. The dots and font size are typically too small and the aspect ratio is typically too wide.
At the time of this writing, the analysis tab of OpenML-CC18 https://new.openml.org/search?type=study&study_type=run&id=99&sort=runs_included does not load for me in Firefox or Chrome.",21-216.txt,1
91,https://openreview.net/forum?id=OCrD8ycKjG,1,5,4,"Please clarify and define the terminology you are using at the beginning of the paper and use it consistently.
Please clarify your understanding of a task.
Please clarify are there any curation protocols currently that are used for defining benchmarking studies in OpenML.
Please discuss how novel tasks can be included in OpenML in order to define benchmarking studies on top of those tasks.
Please discuss in a sentence or two the choice of design criteria for the two example benchmarking studies you have presented. How were they selected?",21-217.txt,1
92,https://openreview.net/forum?id=p2dMLEwL8tF,1,7,3,"The reviewers feel that the dataset is useful and complementary to existing benchmarks, but also raised some issues including but not limited to: (1) three reviewers pointed that the hyperparameter tuning was not performed, therefore, some claims may not be convincing and (2) there is no official validation set provided. As a result, the contributions of this work are greatly weakened. The authors' response clarified some important points. In view of that, the authors are strongly invited to take the feedback on board for the final version.",21-218.txt,1
92,https://openreview.net/forum?id=p2dMLEwL8tF,1,5,4,"  Just a quick note to double-check that the authors have not yet submitted a response? On occasion, the Readers box may be left blank which renders the response invisible to the reviewers.
 ",21-219.txt,1
92,https://openreview.net/forum?id=p2dMLEwL8tF,1,7,3,"Overall I'm super positive about the work! If the authors can address all my points in the clarity category that would be sufficient to move to a 6 in my opinion, although higher would require a bit more discussion with my current understanding.
I have a couple of questions for the authors:
What types of mutations are sampled in each of the landscapes? For example, are all the single mutants in GB1 modifications charged AA -> Non-polar AAs? What types of mutations are the ones that are present in the data?
A single metric is provided for evaluation, but is this sufficient for a benchmark? This may leads to various failure modes being left unobserved (for example, two models get the same score but they fail to generalize in different ways). Are there any plans to flesh out existing tasks with more metrics/visualizations/probes to better evaluate what failure modes a model may be falling into?
Is all the data available? I know the authors mentioned that they downsampled the GB1 data(which may in itself be a problem), but is it possible to download all the raw data so that other groups may try different methods of accounting for the class imbalance? It wasn't entirely clear to me if this was possible from what I could currently see.
Update
Thank you authors for your response. I'm updating the original review instead of following up due to not being able to get to this until after the discussion period ended. The authors response has been overall satisfactory and I have updated my score to reflect this. More details to come at a later time.",21-220.txt,1
92,https://openreview.net/forum?id=p2dMLEwL8tF,1,7,4,"This reviewer would like to thank the authors for their diligent efforts in standardizing protein fitness prediction for the machine learning community. This reviewer believes such work may have great broader impacts resulting from it, such as accelerated biological and representation learning research for climate change.",21-221.txt,1
93,https://openreview.net/forum?id=9KArJb4r5ZQ,1,5,3,"The topic and dataset are of extremely relevance. However reviewers mention several drawbacks of the current version of the dataset and paper, including limited number of positive samples, lack of metadata and noise info, and the limitation of considering self-report annotations, among others. The authors did a great job discussing with the reviewers all critiques. Still some critiques hold but reviewers overall are more positive regarding the contribution. Overall the paper achieves the minimum required score to be accepted for publication at NeurIPS data track.",21-222.txt,1
93,https://openreview.net/forum?id=9KArJb4r5ZQ,1,8,3,"Update after discussions with authors: I have updated my rating (from to 6 to 8) based on the answers provided by the authors on my review. My opinion is that the dataset is constructed in a sound way and has an adequate quality to be relevant for the machine learning community. The restricted access is justified by the nature of the data, and supplementary material is available to support the use of the dataset by other researchers.",21-223.txt,1
93,https://openreview.net/forum?id=9KArJb4r5ZQ,1,7,3,"More baselines can be added to the benchmark, and further analyses can be made. For example, in what kind of scenarios the machine learning model will give the wrong prediction?",21-224.txt,1
93,https://openreview.net/forum?id=9KArJb4r5ZQ,1,4,4,"I think this paper has great potential with major improvements to the work, and I would highly encourage the authors to continue working on this dataset.
Most importantly, clear documentation would significantly increase uptake and participation (and access to code). Several claims related to the potential/promise of the method based on the scores reported could use adjusting. Furthermore, a change in the overall title, scope and claims of the paper could increase the focus on the (majority) parts of the dataset which have not got a COVID label for alternate use cases as listed in the paper.
Further de-biasing and updating of metadata to include source audio format will be helpful to users. What is the bias in terms of audio formats used in training, validation and testing? Each audio compression format targets frequency bands in different ways, affecting spectral-based features. When using VGGish, the feature space will be filled with 0s for the lower sample rate signals. By resampling to wave but not stating the origin of the file, information about encoding is lost (e.g. which is m4a, which is webm, which is ogg?) It would be beneficial to include the format and sample rate in the metadata.
For all other suggestions for improvement please refer to the main body of text of the review.
Post rebuttal
I'd like to thank the authors for their detailed responses. While the paper has undergone improvements as a result of the rebuttal, further concerns have arisen, and outstanding issues remain. My justification for these is given in https://openreview.net/forum?id=9KArJb4r5ZQ&noteId=lDyc_2QSZsV, and thus my score remains a 4.",21-225.txt,1
94,https://openreview.net/forum?id=IfzTefIU_3j,1,8,4,"The paper focuses on instance segmentations in videos of scenes with severe occlusions. This is a very challenging task, and improving models' capabilities in such scenarios will lead to significant advances in general scene understanding. The authors proposed a dataset and an associated challenge for this task. All the reviewers appreciated the usefulness of the dataset and the depth of the provided analysis. The authors updated the manuscript to take into account reviewers' recommendations.",21-226.txt,0
94,https://openreview.net/forum?id=IfzTefIU_3j,1,7,3,"Questions to authors
I had two discussion questions regarding the paper that were not weaknesses or issues, but rather simply questions to the authors:
In L129, the authors state that they remove videos in which most objects are standing still without moving. I am curious if the dataset still contains many non moving objects. One visualization shows the existence of subject objects (bike in third row of the visualization). I am curious about this since it could result in a bias against correctly segmenting objects when they are not moving.
In L143, it is noted that videos range from 3 to 6 fps. Does that mean that some videos were only captured/released at 15 fps? Do you observe any impact on segmentation performance based on the annotation granularity.
Minor issues/Presentation suggestions
Those points are minor issues or presentation suggestions and did not impact the rating of the paper. The authors do not need to respond to them, but I would encourage them to consider them. Comments in (rough) order of appearance in paper.
L13: typo, based not basing
L19-22: the word our seems unnecessary here.
Figure 1: while showing animations is great, I noticed that they do not show up at all in printed documents. I would suggest using something that is more accessible in the paper and reserving animations for online viewing.
L147: severe no server.
L154-156: future tense used when I think the present tense should be used.
Table 2: AP_{HO} goes over the line.
Table 2 and L229: I think it would be good to include the results on the VIS dataset as well instead of simply mentioning the difference in text.
L255: typo, based not basing.
Figure 5: I think describing the failure mode for each row in the caption or in the figure would help make this figure easier to understand. It looks good now, but I think adding a succinct summary of L270-278 here would make it much better and more self-contained.",21-227.txt,0
95,https://openreview.net/forum?id=5HR3vCylqD,1,6,2,"Dear authors,

Thank you for submitting your paper and addressing the issues raised by the reviewers.

The paper proposes a set of 15 problems for monitoring Sustainable Development Goals (SDGs) with machine learning. They provide a python module with dataloaders similar to WILDS and a website with leaderboards for each dataset.

The main criticism was

Missing comparison with existing work Incompleteness of the website Some tasks were not described clearly

All three points were addressed by the authors in the rebuttal. Furthermore, this work allows the ML community easier access to data related to the highly important sustainable development goals.

Taking all this into account as well as the scores of 6,5,7,7 the paper is accepted.",21-228.txt,1
95,https://openreview.net/forum?id=5HR3vCylqD,1,5,3,Overall I think it is a crucial area and I think the SustainBench could be valuable benchmark suite. Please address the questions in the weaknesses section.,21-229.txt,1
95,https://openreview.net/forum?id=5HR3vCylqD,1,7,3,Personally moving the table A7 (summary of SOTA methods on the benchmark tasks) to the main text would make the main paper more self-contained; though it is also understandable that describing such a large-scale dataset within the page limit is challenging.,21-230.txt,1
95,https://openreview.net/forum?id=5HR3vCylqD,1,7,3,"Thank you for incorporating feedback, I feel like this was very helpful and the result is much clearer to me as a reader. I'm changing my score to a 7.",21-231.txt,1
96,https://openreview.net/forum?id=kBNhgqXatI,1,7,4,"This work provided a systematic investigation of the efficacy of representation learning for imitation learning. The experiment settings are well thought of and the benchmark is designed in a modular fashion with great extensibility for future work. The results have indicated that several recent advanced representation learning methods have led to marginal gains over the simple baseline of image augmentation when used with an imitation learning algorithm (behavioral cloning). While this is a bit of a negative result, such evaluations could potentially benefit the research community for validating the real merit of representation learning for policy learning. At the end of the discussion period, all four expert reviewers expressed positive impressions of this work. The AC agreed with the reviewers that this work could be further strengthened with 1) expanded discussions on the experimental results, 2) evaluations with other imitation learning algorithms, and 3) testing on more realistic decision-making domains (e.g., autonomous driving and robot manipulation). Nonetheless, these suggestions should not be considered as a ground for rejection. The AC believes that this kind of systematic empirical evaluation, albeit with negative results, should be encouraged in our community and thus recommends the acceptance of this paper.",21-232.txt,0
96,https://openreview.net/forum?id=kBNhgqXatI,1,6,4,"The implementation and design is great. My primary concern (and I would really appreciate some thought into this) relates to my first question. The authors propose some ""hypotheses"" for why things turned out the way they did (i.e. no consistent improvement using representation learning over image augmentation, for BC). However, I found the analysis and diagrams to be explanations of the result itself, but not an explanation of why such a result is obtained. Any further insights/clarity would be good.",21-233.txt,0
96,https://openreview.net/forum?id=kBNhgqXatI,1,6,3,"Other questions/suggestions:
L52-54, 'We argue ... Fig. 1' -- the 'axes of variation' is not very clear; either change the statement or change the figure.
L149-150, 'We have implemented ... a RecurrentEncoder' -- as far as I understand, even momentum encoders could be single image based encoders. If that's the case, then this statement isn't entirely correct?
L219-220, 'Using our modular ... learning algorithms': any particular reason why you used just these 5? Also related to this, maybe report the missing experiments mentioned in L228-230 ('Note that we ... doing so') in the Supp.
L242, 'Note that mot' -- 'Note that most'?
Table 4 and 5: the table captions aren't self-explanatory. As a reader, it's not very pleasing to go back to the text to understand a table.",21-234.txt,0
96,https://openreview.net/forum?id=kBNhgqXatI,1,6,3,"I'm not sure if including the implementation sections were needed. Figures 2 and 4 were a little hard to read and in Figure 4 showing the original scenes would help give more context. In the conclusion the variation in datasets is discussed and, having figure 3 show samples from the DMC task would be helpful. It would be useful to see the results of the preliminary experiments in the Appendix that showed momentum, projection heads, and compression did not show any advantage.",21-235.txt,0
97,https://openreview.net/forum?id=woX9uagUQiE,1,6,3,"Dear authors,

Thank you for submitting your paper and addressing the issues raised by the reviewers.

The paper introduces a novel dataset on diet planning and a tool that generated the data. The dataset was generated synthetically but edited and improved by human domain experts and Machine Learning.

The reviewers commend the importance of the data, use of human experts to improve the data quality, general clarity of the paper and easy to use python package.

The main weakness is the potentially limited use of the data and its limitation to Korean food. In the appendix A7 several additional use-cases are discussed.

The reviewer scores are 7,6,6 and 7 which is sufficient for accepting the paper.",21-236.txt,0
97,https://openreview.net/forum?id=woX9uagUQiE,1,7,3,"It would be great if the authors could run a clinical study to further evaluated the suggested diets. It is beneficial to see how participants would take the diest and hear their feedback. In addition, showing quantifiable health measures from the clinical study would make this work significantly stronger.",21-237.txt,0
98,https://openreview.net/forum?id=qM45LHaWM6E,1,6,3,"The paper presents a welcome addition of distribution shifts beyond the image classification domain. The reviewers are generally favorable of the paper, hence I recommend acceptance.

For a more comprehensive overview of robustness research in the image domain, I recommend the authors refer the reader to the following robustness evaluations: https://arxiv.org/abs/2007.01434 , https://arxiv.org/abs/2007.00644 , and https://arxiv.org/abs/2007.08558",21-238.txt,1
98,https://openreview.net/forum?id=qM45LHaWM6E,1,5,3,"I have some questions about the Section.4 Machine Translation.
Do you have plans to support more languages? But I fully understand that collecting the professional translation may be expensive (as depicted in the Appendix), and it is hard to support other languages.
Since the pre-trained model is the most popular paradigm in the current NLP field, do you think Shifts Dataset can be used to examine the robustness of pre-trained language models?
BLEU cannot evaluate the semantic-level features. Why not introduce more evaluation metrics that correlate well with human judgements, e.g., COMET, BLEURT, BERTScore?
The PDF should not be generated with preprint option. There are typos in this paper. For example, check them:
Abstract & Introduction: In this work we -> In this work, we
Section.3.Baselines: appendix -> Appendix
Section.5.Dataset: appendix -> Appendix
Section.5.Baselines: appendix -> Appendix",21-239.txt,1
99,https://openreview.net/forum?id=WcY35wjmCBA,1,7,3,"While this is an interesting direction, the tasks in this work don't present any unique difficulties not present in other deformable benchmarks, and the learned policies are not transferable to real-world robotics domains. In addition, the evaluation is not sufficient as the paper lacks many task evaluations, baselines, and experimental details.",21-240.txt,0
100,https://openreview.net/forum?id=Q0zOIaec8HF,1,6,2,"This paper presents a collection of tabular+text datasets and also introduces new methods for handling the combination of tabular+text. This doesn’t provide a perfect focus on the actual new dataset collection, which was the primary issue that reviewers criticized. The authors added much new information about the actual datasets in an extensive rebuttal phase with all reviewers, in the end leaving the reviewers satisfied. I therefore recommend acceptance now. I encourage the authors to add any further documentation and statistics about the datasets they think could be useful.",21-241.txt,0
100,https://openreview.net/forum?id=Q0zOIaec8HF,1,6,2,"Thanks for the insightful and detailed comments by all reviewers. We have revised our paper according to the feedback and have uploaded an improved version. Major additions include more in-depth: justification of the many choices we had to make and descriptions of each dataset. We have also added a new Table S1 to Appendix, which provides a finer-grained summary of AutoML performance over our benchmark (broken down by prediction problem type) than the aggregate metrics over all datasets provided in Table 3. A point-by-point response to reviewer questions is provided below, please let us know if any other questions come up!",21-242.txt,0
100,https://openreview.net/forum?id=Q0zOIaec8HF,1,6,3,"The paper reads a bit like an advertisement, e.g.
The data files are currently hosted in AWS Simple Cloud Storage 150 (S3), which is a reliable and highly-available medium.
modeling is simply done via AutoGluon-Tabular, an easy-to-use and highly accurate open-source tool for automated supervised learning on tabular data
or
For real-time applications with latency constraints, AutoGluon provides options to accelerate ensemble inference via pruning or distillation [17] which seems completely unrelated to what the paper is proposing.",21-243.txt,0
100,https://openreview.net/forum?id=Q0zOIaec8HF,1,6,3,"This question does not impact the score, but addressing it may be considered helpful for further reading:
The second point in the discussion seems to be highly related to the third point, that in general, stacking models later helps rather than fusing low-level features. Do you have any rationale or intuition behind the lower performance or any suggestions regarding what can be improved for the Fuse-Early type of approaches?",21-244.txt,0
100,https://openreview.net/forum?id=Q0zOIaec8HF,1,6,3,It appears all citations after page 1 are somewhat broken as clicking them sends me back to page 1. The issue was reproduced in two different pdf readers.,21-245.txt,0
100,https://openreview.net/forum?id=Q0zOIaec8HF,1,6,2,"The descriptions of the datasets seem inconsistent. All descriptions cover the prediction task, but some include more information of the dataset source and what they are used for. Are the other datasets from Kaggle not used in competitions?",21-246.txt,0
101,https://openreview.net/forum?id=rNs2FvJGDK,1,7,4,"The work presents a new end-to-end document understanding benchmark, which all reviewers think is interesting. Additionally, the authors have set up a leaderboard website with some useful features. Despite some concerns from reviewers, the authors make a clear and convincing rebuttal.",21-247.txt,0
102,https://openreview.net/forum?id=Gln7zxMffae,1,7,3,"This paper proposes to split existing datasets to subsets such that we can test whether models for knowledge prediction behave in a way that respects various reasoning properties. All reviewers agree this dataset is useful and the results are interesting and illuminating. Several concerns have been raised that must be addressed in the final version: (a) using versions of the datasets where there is no leakage from train to test, though authors explain this might affect just a small portion of the data (b) improve clarity (c) improve related work and refer to models that try to specifically look at properties such as symmetry and explain how the evaluation here is more comprehensive than prior work.",21-248.txt,1
102,https://openreview.net/forum?id=Gln7zxMffae,1,4,4,"Hi, I'm writing to point out a very closely related work of BoxE (Abboud et al., 2020) which is completely missed in this work. Inference patterns are comprehensively studied in the BoxE paper, including patterns such as symmetry, anti-symmetry, inversion, and relational hierarchies. These patterns are even studied jointly and BoxE is formally shown to capture a rich rule language. Furthermore, an empirical evaluation is given for rule injection on a dedicated dataset.",21-249.txt,1
102,https://openreview.net/forum?id=Gln7zxMffae,1,6,3,"In Table 7 and section 4.3, a comparison between two models on the introduced subsets of FB15k is made. Given that this benchmark includes both FB15k and WN18, it is a bit strange that an equivalent comparison for WN18 is not included.",21-250.txt,1
103,https://openreview.net/forum?id=OFiGmksrSz1,1,7,3,"Although reviewers agree on the high relevance of the problem, there are common critiques regarding missing justifications/details, limited data size, and issues related to the lack of a training set. After rebuttal several of the critiques of the reviewers have been carefully discussed and addressed by the authors. Overall the paper achieves the minimum score to be accepted for publication at the NeurIPS data track.",21-251.txt,1
103,https://openreview.net/forum?id=OFiGmksrSz1,1,6,3,"To properly evaluate anomaly segmentation, some domain shift is necessary. In this sense, we are thankful that the reviews pointed us to improve our definition of anomalies, as we now do in lines 140-146. If our data had no domain shift, it would be impossible to differentiate between a method that performs good semantic anomaly segmentation, i.e. segmenting only what does not belong into the class definitions, and a novelty segmentation method that segments out anything that appears visually different to the training data, such as a car in a novel color. If a method is very vulnerable to domain shift, it is by our definition therefore not a good anomaly segmentation method and consequently receives low scores in our benchmark.
We however also acknowledge that class definitions can be fuzzy (lines 194-196), which is why we
use the ‘void/ignore’ label to exclude semantically ambiguous parts from the evaluation (e.g. rickshaws), and
designed RoadObstacle21 to focus on the more clear and practically relevant drivable areas and obstacles.
While the definition of anomaly in our benchmark closely matches with the CAOS and Fishyscapes benchmarks, we understand that the term ‘anomaly’ is also used in other works e.g. for production anomalies or visual outliers and it may have been unclear to readers and reviewers without the revised definition what the purpose of our benchmark is.",21-252.txt,1
103,https://openreview.net/forum?id=OFiGmksrSz1,1,5,4,"For clarity, it would be useful to add a legend to the image in Appendix F5 that explains the color coded class labels.
In equation
(1)
,
A(k)
denotes all GT pixels of the dataset that are not within the GT region
k
. I believe the denominator of the equation should be
|k∪K^(k)−A(k)|
instead of
|k∪K^(k)|−|A(k)|
?
General comment: The paper considers an interesting and important problem for which more high quality research datasets are needed. The paper is well written and many recent methods were benchmarked on the proposed dataset. My main concerns are the low number of dataset images as well as the absence of a dedicated training split. In addition, there are several questions in my review above for which it would be great to have an answer to from the authors. In its current form, the weaknesses of the work outweigh its strengths which concludes my paper rating - marginally below acceptance threshold.",21-253.txt,1
103,https://openreview.net/forum?id=OFiGmksrSz1,1,6,4,"In addition to the results on the anomaly segmentation benchmark, it might be a good idea to also report the results of the methods on CityScapes. That way, a reader would get a more complete picture.
I think there is a typo in equation (1). I think it should be
sIoU(k):=|k∩K^(k)||(k∪K^(k))∖A(k)|
.
The ""Checklist"" section at the end of the paper begins with an instructions block. That block can be removed.",21-254.txt,1
104,https://openreview.net/forum?id=zNQBIBKJRkd,1,8,4,"The reviewers found a lot of praise for this paper, and I strongly recommend acceptance. The findings are general, and I believe the material could make for an oral presentation that is interesting for a broad audience.",21-255.txt,0
104,https://openreview.net/forum?id=zNQBIBKJRkd,1,10,5,"Small points: Line 73: Footnote 2 shoudl be after the period, not before Line 158: the comma after e.g; it shoudl be e.g., Line 295: The parenthesis before Figure 4a should not have a leading space",21-256.txt,0
104,https://openreview.net/forum?id=zNQBIBKJRkd,1,8,4,"I noticed several minor problems:
you write that
p=0
in #234, I believe it is a typo and there should be some non-zero value,
""e.g."" in #46 lacks a comma despite the fact you add it in other parts of the paper,
footnote in #73 should be placed after the period,
""datset"" instead of ""dataset"" in #169.",21-257.txt,0
104,https://openreview.net/forum?id=zNQBIBKJRkd,1,8,4,"Overall, the paper is interesting, very well written and delivers a concise punchline. However, my main concern is that without a ""related work section"" it seems too removed from prior work in the benchmarking community.
in addition to all your discoveries, I believe that the paper is a great opportunity to showcase your new ""dataset of datasets"" sourced from PWC. The crawled data can be emphasized throughout the paper not just as a pre-processing step, but rather as one of your main contributions to the community.",21-258.txt,0
105,https://openreview.net/forum?id=EfgNF5-ZAjM,1,5,3,"The paper proposes a video question answering benchmark that relies on scene hypergraphs connecting atomic entities and relations, to enable reasoning tasks given a video context. Questions and answers are procedurally generated, and cover interaction, sequence, prediction, and feasibility situations. Different models are evaluated on this benchmark showing their shortcomings around visual understanding of the scene and abstract reasoning, and a neuro-symbolic tool is proposed for diagnosing model failure.

Several concerns related to novelty, clarity of writing, resources release, and data bias were raised by reviewers and the authors actively engaged in providing explanations and adding clarifications to the manuscript. Following detailed discussions between reviewers and area chair, the reviewers increased their scores (or expressed a wish to increase them) to acknowledge the manuscript updates made by the authors. Given the final positive evaluation made by all reviewers, we conclude that this benchmark is useful, the data collection process is sound, so this is a valuable resource for the community and we recommend it for publication. However, we strongly encourage the authors to add all the clarifications promised in rebuttal and carefully proofread the manuscript for the camera-ready version.",21-259.txt,1
105,https://openreview.net/forum?id=EfgNF5-ZAjM,1,6,4,"Thanks to the reviewers for their comments and questions. We appreciated that reviewers recognized our motivation, contributions, design choices, diagnostic method, documentation and provided constructive comments.
We have revised our paper according to all questions, suggestions, or concerns to include the following changes (highlighted in blue text):
We have added new perception comparison experiments to show quantitative results on the well-turned perception (vision, language, hypergraph) models in paper Section 5.1 (Video Parser) and supplementary Section 4 (Quantitative Results of Perceptions). (R1)
We have added new end-to-end NN model experiments via VisualBERT/ClipBERT to observe the effectiveness of different perception settings in paper Section 5.2 (Visual Perception) and supplementary Section 4 (NN Learners with Perfect Perception). (R1, R3)
We have added new answer distribution visualization to further analyze the potential data bias and balanced answer distributions in paper Section 3.2 (Debiasing and Balancing Strategies) and paper Figure 2. We have highlighted our strategies to reduce data bias via balancing and shortcut breaking and the visualized answer distributions in Figure 2. (R2)
We have discussed contribution, problem, or method differences between our dataset and conventional Video QA or NLP Dialog QA. (R2)
We have elaborated on the module details (program executor, functional program, and answer generation) in paper Section 3.2 and Section 5.1. (R4)
We have revised the paper carefully, including providing more language difficulty in Section 1, experiment settings in supplementary Table 5, annotation details in Section 3, implementation details in supplementary Table 2 or paper Section 5.1, other explanations, and grammar checking. (R1, R2, R3, R4)
Please don’t hesitate to let us know if you have any further comments on the manuscript or the changes.",21-260.txt,1
105,https://openreview.net/forum?id=EfgNF5-ZAjM,1,6,4,"I may raise my score if the following points can be improved in a rebuttal:
-more quantitative results on the perception (vision, language, hypergraph) models.
-results on combing NN learners with perfect perception.",21-261.txt,1
106,https://openreview.net/forum?id=6vZVBkCDrHT,1,7,3,"Most reviewers agree on acceptance. The comments by the only reviewer that tends to rejection were successfully addressed by the authors. I, therefore, recommend acceptance.",21-262.txt,0
106,https://openreview.net/forum?id=6vZVBkCDrHT,1,7,3,"I’m not sure how and if to treat the cooperation with Global Women in Data Science, the challenge, and the project’s stars from an academic point of view. As a personal note, I am impressed.
The use of the data set for translation is tempting but it is not clear how to do it. After all, each problem has many solutions in the same language so which pairs should be matched? A possible way is to use cycles For example Python1 -> ToJava(Python1) -> ToPython(ToJava(Python1)) == Python1 Is that what you had in mind?",21-263.txt,0
107,https://openreview.net/forum?id=79shW3z5Eaq,1,6,1,"Reviewers all agree that this paper offers a valuable contribution to this track. While the confidence of the reviewers ratings is lower than I would have desired, the two highest scoring reviews (7/7) are the highest confidence (3). The authors engaged with the constructive reviews, responding to many of the comments and addressing most of the weaknesses. The authors have also revised their manuscript in light of the reviews and discussion.

Given the positive reviews and in-depth author response, I am recommending acceptance.",21-264.txt,0
108,https://openreview.net/forum?id=m5rEiGxOGiL,0,6,4,"While all reviewers agree on acceptance, this is still a borderline paper, tending to reject, due to the lack of enthusiasm from the reviewers. I recommend the authors take into account the reviewers' comments for improving the paper and submitting to another venue.

Extended meta-review:
On the positive side, most of the shortcomings of the original paper were quite successfully addressed in the rebuttal, and the topic of the dataset is very interesting and relevant.
On the other hand, the review ratings were only slightly improved (no reviewer truly championed the paper), and some weaknesses did still remain in the view of the reviewers, such as the limited novelty over existing datasets and tasks, and even explicitly: 'it remains my opinion that the choice and exploration of benchmarks and metrics remains insufficient'. Another reason for concern was that both the paper and the code were changed drastically during the rebuttal period (even the conclusions were rewritten extensively), and further changes were requested after the paper was updated in the rebuttal, such as clarifications on claims on the novelty of the tasks. That raises concerns that the paper should likely go through yet another round of updates and reviews. In all, the balance points truly to a borderline case.",21-265.txt,-1
108,https://openreview.net/forum?id=m5rEiGxOGiL,0,7,5,"Are there any other relatively fast algorithms for ground-state 3D structure prediction besides RDKit for comparison?
It seems that based on table 4, validity of the predicted 3D input is not important (results for predicted 3D distance vs. coordinates) for property prediction, which sort of makes sense as SchNet only takes in pairwise distance, but is there any correlation between errors in predicted 3D structures used as input vs. errors in the predicted property?",21-266.txt,-1
108,https://openreview.net/forum?id=m5rEiGxOGiL,0,6,3,"  All points covered in the main response.
  Update post-rebuttal: I have upgraded my score to 7 following an extensive response from the authors addressing many of my concerns.",21-267.txt,-1
108,https://openreview.net/forum?id=m5rEiGxOGiL,0,6,5,"The result of their benchmark, that conformers from the model with lower errors in distances harm property prediction, deserves more discussion. It raises the possibility that the proposed metrics are not sufficient to assess a structure's suitability to be used for property prediction.",21-268.txt,-1