id,url,decision,ratings,confidence,text,Filename
0,https://openreview.net/forum?id=JhZOkalsiI,1,9,4,"Firstly, I would like to congratulate the authors upon successfully participate and complete the reproducibility challenge. Furthermore, I would like to highlight the commendable job put together by the authors in submitting a comprehensive report evaluating the reproducibility of the key aspects of the original paper. I believe that this submission is extremely strong and checks almost all boxes of the Reproducibility challenge. 
Below is the detailed feedback with specific judging criteria:
1. Problem statement - The authors did an excellent job of explaining the background and the problem description. This report adds to the understanding of the problem presented in the original paper.
2. Code - From my understanding, the authors prepared the codebase from scratch and made their codes publicly available on Github. I believe this is extremely crucial for replicating or advancing the current work.
3. Communication with original authors - The original authors were contacted for clarification over specific details of the implementation (as I could see on the openreview forum). The report mentions certain email correspondence, which is not available (not an issue). However, it would help if the report could mention which details were clarified by the original authors via personal communication (eg. highlighting the information in Table 1). Also, a minor comment about the plots in Fig. 1: I see certain breaks that are absent in the original paper. It would be great if the report could mention a reason for possible discrepancies or simply comment about the same.
4. Hyperparameter search - The authors tried several values of the mixup parameter alpha and also reported the variance in results. I feel this is incredibly useful and should be more often included in deep learning research as the variance in results on multiple runs carries significant information about the robustness of the method.
5. Ablation study - There was not much scope for an ablation study in this work. But the authors do a commendable job in verifying the baselines (no mixup) training. 
6. Discussion on results - The authors briefly discuss the results obtained and report the portions of the paper that they could reproduce. It would be great if they could list them in a table to improve readability and for quick reference. They also report that the variance in results was huge and that the original results were within one standard deviation of the reproduced results. However, I was wondering if that is still the case with FashionMNIST data wherein the ECE trend seems to be different from the one reported in the original paper. In addition, it would be great if points in the scatter plots (Fig. 3 and Fig. 4) were slightly bigger to improve readability. 
7. Recommendations on reproducibility - The authors presented a summary of the implementation in Table 1 and I feel this is extremely useful for quick reference in the future. It would be great if they could update this table indicating which information was provided by the authors in the original manuscript. This could act as a guideline for future researchers to improve the reproducibility of their work.
8. Overall clarity and organization - I found the report to be well organized and extremely easy to read. The plots, however, could be slightly updated to improve readability (see above for suggestion).

Conclusively, I feel that this report is extremely comprehensive and clearly indicates the portions of the original paper that could be reproduced by the authors. It would be great if the authors could add certain pointers to improve reproducibility, based on their personal experience. Also, the plots in Fig. 5 are missing x- and y-labels. I would request the authors to add these in order to make it more comparable to the original plots. ",19-0.txt
0,https://openreview.net/forum?id=JhZOkalsiI,1,7,4,"**Overall comment**
Overall, I think this is a good submission, showing that the original paper is reproducible by other researchers.  The results were shown for only some of the datasets and tasks, and did not reproduce the original experiments perfectly, but they were close enough to show the same trends, observations, and conclusions of the original paper.

**Problem statement**
1) It is clear that the paper states and understands the problem statement of the original paper, which is to study the calibration effects of Mixup for neural networks.
2) There are sufficient background on calibration, mixup, and the metrics that are used in this and the original paper such as expected calibration error and overconfidence error.

**Code**
A github repository was provided by this paper, and it seemed that the code was developed from scratch, since an official repo does not exist.

**Communication with original authors**
The authors of this paper have posted questions on OpenReview.  The authors state that they also communicated through emails about details on the experimental setup, such as the number of interval bins.

**Hyperparameter Search (For Track 1 and 2)**
Although this paper is for Track 3, the paper shows results for \alpha \in [0, 1.0] while the original paper show results for \alpha \in [0, 0.5].  The results shown did not change the conclusions of the original paper, but it was nice to see these additional results. 

**Ablation Study (For Track 1 and 2)**
1) Although this paper is for Track 3, it has experiments ""without Mixup"" in addition to ""with Mixup"".
2) This paper do not show experiments with label smoothing and entropy-regularization but this is acceptable since the paper is submitted to the 3rd track.

**Discussion on results**
This paper shows that the similar results with the same observations were reproducible, at least for the datasets and tasks chosen by this paper.

**Recommendations for reproducibility**
1) The conclusion is very clear, that they were able to reproduce the major results in the original paper, confirming that adding Mixup will improve confidence calibration.
2) The paper recommends to add the standard deviation information which was missing from the original paper.

**Other bonus points**
The paper conducted experiments for testing in out-of-distribution with a different dataset (Fashion-MNIST to MNIST).  The experimental results show similar results from the original paper which chose the STL-10 and ImageNet datasets.

**Overall organization and clarity**
The comments below are minor errors and suggestions and did not impact my final score.

1) In page 3, \hat{y}_i and y_i are explained as true label and predicted label, but they should be the opposite since y_i is already defined as the labels of the dataset in page 1.
2) In the end of the 2nd paragraph in page 6, in the caption of Fig.4, and in the end of the 2nd paragraph of Section 4.4, FashionMNIST or fashionMNIST should be Fashion-MNIST.
3) Citation [5] in References do not have splits between the first 3 authors.
4) In Figure 1, the numbers and names of the x-,y-axis are too small and I cannot read them.
5) The sub-figures (d)-(f) also have small fonts, and would be nice to have them larger.
6) I would like to suggest using a more narrow y-axis range for sub-figures (a) in Figure 2 to Figure 4.  For example, using 0.5 to 0.8 in Fig2(a) instead of 0.0 to 0.8.
7) Out-of-dsitribution --> Out-of-distribution in Section 4.4, 1st paragraph.",19-1.txt
0,https://openreview.net/forum?id=JhZOkalsiI,1,7,4,"The report explains the ideas of the original paper 'On Mixup Training: Improved Calibration and Predictive Uncertainty' briefly but clearly.

The report clearly explains the implementation details including initialization and hyper-parameters of their reproducing, which is important for deep network performance.

Could the authors publish the reproducing code in the future? I will raise my score if the code is available.",19-2.txt
1,https://openreview.net/forum?id=GAY8Xw9Qzs,1,7,4,"The authors do extensive experiments on the model in [1], including realted ablation study, hyperparameter tuning, altering the objective and the receptive filed of the model.

Quality: the authors state the original's idea, setting and evaluating methods. The reproduce experiments were conducted based on the code on GitHub. The results are make sense.

Clarity: the report is well written and easy-to-follow.

Originality: the authors investigate the proposed method from multiple aspects, including ablation study, hyperparameter tunning, receptive filed etc. They fully explored the unclear statement in the original papers.

Significance: RL-related experiments have large variance, however due to the computational limitation, the authors suggests to leave the variance reduction as the future work.

Overall, I think this is a solid reproduction report.

[1] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre Côté, and R. Devon Hjelm. Unsupervised State Representation Learning in Atari, NeruIPS 2019.",19-3.txt
1,https://openreview.net/forum?id=GAY8Xw9Qzs,1,8,4,"## Summary

* The report is based on the paper ""Unsupervised Representation Learning in Atari"". The code (for the paper) is available.
* Focus: 
    * Reproduce some result.
    * Tweak the learning objective and training setup.
* The authors of the report reached out to the authors of the paper to make sure their understanding is correct.


## Significance

* Reproduced results for 22 Atari games.
* Focused the abalation studies on ideas which seemed to be underexplored in the paper, thus adding to the details in the paper.
* The report mentions that creating an ensemble helps to improve the performance. While this is expected, it is good to have it empirically validated. 
* Most of the abalations seem to support the design choices from the paper.
* The authors rightly pointed out one limitation of their analysis - the experiments are done with fewer environments (in some cases, only 1 environment) and on fewer seeds.",19-4.txt
1,https://openreview.net/forum?id=GAY8Xw9Qzs,1,6,3,"This report presents an ablation study of the paper ""Unsupervised State Representation Learning in Atari"". The authors start by reviewing extensively the original paper. Then they use the original codebase to reproduce the results from the original paper as well as conducting a series of ablation experiments.

In particular, the authors ablate i) different parameters of the encoder, ii) the relative weights of the different parts of the loss functions, and iii) the receptive field of the encoder.

For the parameters of the encoder, the authors modified the inputs to be RGB or be in grayscale, and they also tried using a different framerate. The results here were not very conclusive, indicating that these choices didn't have a great impact.

Changing the relative weights of the global-local and local-local losses changed the results significantly, with the authors concluding that equal weight on both losses was the best trade-off, as done in the original paper.

As for the receptive field of the encoder, the authors also found that the original choice of 1/16 was the best across differents tests.

Overall the report is clear and well written. The ablation experiments empirically validate some of the choices made in the original paper that were not thoroughly justified. The main drawback of the report is the limited setup of the experiments, with results only for few games/parameter choices. This is due to reduced computational resources as noted by the authors. Some other interesting ablation experiments such as modifying the capacity of the networks used or the contrastive loss would also be interesting to include in newer revisions.",19-5.txt
1,https://openreview.net/forum?id=GAY8Xw9Qzs,1,7,4,"The authors study the reproducibility and different variants of the paper ""Unsupervised Representation Learning in Atari"".

Using the available code, the original results were reproduced without difficulty in this report (random agent).

Different ablation studies are performed: 
-  learning rates for the disentanglement
- modify the loss function
- modify the receptive field for the encoder

This study is quite well-written and study a few key elements of the original paper. It provides interesting insights regarding the key hyperparameters of the original paper. The authors of this study also suggest as future work to train different encoders coming from different ablations to obtain better F1 scores.",19-6.txt
2,https://openreview.net/forum?id=y53aaSM5o,1,8,4,"This paper provided a comprehensive study on BOP methods for training binary neural networks. From the perspective of reproducibility, the paper showed slightly better results for the training classification model.  In addition, the authors also applied BOP methods for training the Autoencoder model.  Experimental results were interesting.  Here, the reviewer still has some concerns about the current manuscript.
1. In Figure 2, the authors should report the test results without BN layers for a better comparison. 
2. For BNN, can larger batch-size improve the stability of the training process? 
3. Do the different upsampling techniques influence the performance, such as deconvolution, bilinear upsampling, and nearest upsampling?
4. From the reported results in Figure 4, binarization operations would degrade the performance of Autoencoder. However, what is the core part in binary Autoencoder that degraded the performance most, the binarized encoder or binary decoder or both? ",19-7.txt
2,https://openreview.net/forum?id=y53aaSM5o,1,9,4,"This paper aims to reproduce and analyze the results from the paper ""Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization"", which introduces an optimizer called Bop for binarized neural network but without the need of latent weights.   This is a good reproducibility report with a systematic approach. It first provides an analysis on the basic assumption the Bop paper made: latent weights are not necessarily needed. It then provides a ablation study on the effect of the hyperparameters introduced by Bop. Furthermore, the paper goes beyond the Bop paper and proposes to use layer normalization to replace batch normalization in BNN training, which leads to improved performance. Finally, the paper also analyzes the effect of BNN and Bop beyond just image classification tasks, revealing gaps and opportunities for BNN training beyond the original Bop paper. 

On the other hand, the latent weights are not needed for BNN training seems to be a very strong statement, given that the Bop paper only evaluated the effect of latent weights on image classification tasks with relatively simple model architectures. It would be better to also compare the accuracy from binary weights and latent weights on a slightly wider range of models, such as those for NLP tasks, including language modeling, neural machine translation, etc. 

The paper also does not mention which implementation of Bop it uses for its study, presumably because it uses the open-sourced version of Bop. But it would better to mention it explicitly. 

Minor:

In the second paragraph of Section 1, ""an novel"" -> ""a novel"", ""inorder to flip"" -> ""in order to flip"".

In the second last paragraph of Section 2, ""the results are provided for MNIST and CIFAR10 datasets are provided in Table 4)…"" -> ""the results for MNIST and CIFAR10 datasets are provided in Table 4)…""

In the first paragraph of Section 5, ""same except the each layer"" -> except that each layer

""Layer Normalization"", ""layernormalization"", ""LayerNormalization"", ""LayerNorm"", ""layernorm"" are used randomly across the entire paper, and same for ""batch normalization"". It would be good to make it consistent. ",19-8.txt
3,https://openreview.net/forum?id=HyxQr65z6S,1,7,4,"In general, I feel that the submission has done an excellent investigation of Franceschi et. al.'s algorithm. 

1. The submission has covered all experiments in the original paper. Particularly, the amount of experiment in the original paper is relatively large considering normal machine learning papers. 

2. When details are missing from the original paper, the authors of the submission use good reasonings to decide their own setting. It is particularly true for the IHEPC datasets. These details are very important for recovering results in the paper. 

3. The submission itself has covered most implementation details. After reading this submission, I don't see any further details that are needed for the implementation. 

A few comments: 

1. The submission mentions ""An offset is expected since the experiments were only run once for each dataset, as in the article. There is stochasticity in the sampling of the sub-sequences during the training of the encoder"".

I fully agree with the comment. Is it possible to run these experiments multiple times and see how much variance there are? In some sense, we are testing the reproducibility of these numbers reported for the *algorithm*. When this algorithm is used for real applications, it naturally has the uncertainty from stochastic training. It will be great if the submission can quantify such uncertainty. 

2. Are these error bars from in Figure 1 (sparse label experiment) from multiple random labelings? Neither the original paper or this submission mentions this.  ",19-9.txt
3,https://openreview.net/forum?id=HyxQr65z6S,1,8,3,"The reproducibility reporters did a series of experiments to investigate the replicability of the NeurIPS paper. According to the report, most of the results are replicable, which shows that the original paper is a strong paper. The reporters give a good report to support the original paper. After reading the report, I have one question about the code.

I find that the original authors have released their code on Github. Have you tried their code and compared the results? Can the codes released by the original authors and the reproducibility reporters yield approximately the same results?",19-10.txt
3,https://openreview.net/forum?id=HyxQr65z6S,1,6,4,"Authors participated in the Replication Track and implemented the basic method (Algorithm 1) introduced in the original article from scratch and have performed analysis of  approximately 1000 univariate and multivariate datasets. To identify missing information and unexpressed assumption that prevents reproduction of results, no communication with the original authors took place nor was code provided by the original authors used. The re-implemented code was made available on GitHub. 

The paper briefly and concisely outlines the main idea of the unsupervised approach introduced by the original paper. This presentation could be a just bit more specific and describe e.g. the parameter K, which is essential for the encoder performance.

The authors implemented Algorithm 1 based on the pseudo-code from the original paper and performed hyperparameter optimizations where necessary. If the information in the article was not sufficiently detailed, default values were used, and decisions were made based on the authors own assumptions. Authors discussed several parts of the original paper that lacked detailed information on the implementation. Although several assumptions had to be made, authors were able to successfully replicate most classification, regression and transferability results from the original article. 
Note that the original authors posted a message in response to the replication paper through OpenReview that clarified that missing information was embedded in the code. 

The paper is well written, clear and the presentation and comparison of the authors' results with those of the original authors is presented appropriately. 

Pros:
- Authors were able to replicate the results, which largely follow the results obtained by the original authors.
- The examination of missing information and assumptions authors made is quite critical. Additionally to the information provided, would be interesting to know how many resources the authors invested to make their own decisions about missing information and assumptions in comparison with the resources needed to implement the method and code the analysis.
- Authors analysed three additional datasets that the original authors excluded

Cons:
- The comparison of results is purely descriptive. Authors report that the average difference in performance is less than 5% in about 70% of the UCR and UEA datasets. The reviewer agrees and understands that there is randomness involved, which causes variability. However, what is the rationale for authors to say that the results are still within the same range when 30% of the results are outside the 5% margin? Statistically speaking an average difference of 5% can still be significant and in favour of one of the two implementations. A more in-depth characterization of the comparison results would be appreciated.",19-11.txt
3,https://openreview.net/forum?id=HyxQr65z6S,1,7,3,"This report is well written and organized. More importantly, it states and studies the 4 questions that need to be addressed based on a thorough understanding of the original paper. 1) It verifies that the results in the original paper can be reproduced. 2) It verifies that the methods proposed in the original paper can be transferred as stated in the original paper. 3) It reports some details of the reproducibility experiments that are missing in the original papers. Though some of them are not exactly what the original author did, they reproduce similar results as reported in the original paper. This further verifies that the proposed method is quite robust to some subtle details. 4) It concludes that there are no hidden tricks or assumptions that mattered for the result by additional comprehensive experiments.

The report is reimplementing the algorithm from scratch, which satisfies the replication track. The report contains detailed discussion on the reproducibility of the original paper. Moreover, the reporters are actively communicating with the original authors through OpenReview. Besides, in one of the experiments (IHEPC), the reporters made several assumptions and did a slightly different experiment than the original ones and report some interesting observations. Hopefully, the reporters and the original authors can exchange their ideas on the observations through further discussions and make a revision in the final report.
Hello,

Let me comment as an author of the original paper.

Firstly, thanks a lot for your interest in our article, and congratulations for your great work on testing its reproducibility!

While acknowledging the reproducibility of our method, you mentioned some unclear implementation details in our paper. These details are indeed only explained in the code provided with the paper, and, without further explanation in the paper, they might lead to some misunderstandings. Let me take this opportunity to clarify them.


1. Sequence padding for time series of unequal lengths.
When time series in a dataset are of unequal lengths, we do not use sequence padding. Instead, we independently choose x_ref, x_pos and x_neg for each element of the batch, thus removing the need of sequence padding. However, your use of padding seems to yield similar results.


2. SVM training.
We did not optimize the penalty term when either the training set size is below 50, or the training set size over the number of classes in the dataset is below five, as we are performing five-fold cross-validation.
Regarding their training time, we limited the training set to 10000 random samples (solely for SVM training) in order to limit the computational time, if the training set size is larger than 10000. We also parallelized cross-validation with multiple threads to reduce its computational cost. Moreover, cross-validation usualy chooses a large weighting factor for the error term, thus making it possible to choose a typical factor value and avoid performing cross-validation.
These optimizations reduce the computational cost of training SVMs on large datasets. Training a logistic classifier instead of an SVM can also further reduce this computational time at the cost of a small performance drop.


3. Architecture of the ResNet baseline.
We used the code provided by Fawaz et al. for their state-of-the-art review (Deep learning for time seriesclassification: a review, DMKD 2019) to train the ResNet baseline: https://github.com/hfawaz/dl-4-tsc.


4. IHEPC experiment.
We indeed performed our experiment on the main feature of the dataset (global active power).
We replace missing values with the last value that was observed in the series, which should give similar results to your linear interpolation as the proportion of missing value is small (1%).
Regarding the much lower MSE scores that you obtain when training a regressor on the learned representations compared to the ones that we report in the paper, I think you actually trained a regressor to predict the future representations, and not the future raw values. In other words, we train the regressors over our representations and the raw data with the same objective, that aims at predicting the next day / quarter mean consumption, in order to compare their performance and efficiency on the same task.
Other than these details, you recovered the training method that we used in our code.


Thanks again for your work that helps us to clarify unclear points in our explanations. We are open to any other comments and would be glad to answer any question.",19-12.txt
4,https://openreview.net/forum?id=HJxNSp9MTr,1,7,4,"This paper aims at reproducing Hamiltonian Neural Networks (HNN). HNN is based on Hamiltonian mechanics and it models the energy given the state (position, momentum), such that the derivatives can be derived from the backward pass of the neural network.  The original paper provides code and this paper successfully reproduces nearly all experiments under similar settings (only on a pixel pendulum system this paper uses convoluted optimization procedure to reproduce the results). Additionally, this paper considers a new experiment: real-world mass-string system with friction such that energy cannot be conserved, and found that while the baseline did a poor job in following the states, it did a better job at modeling the energy (since HNN conserves energy).

Pros:
1. reproduces most results under similar settings. This is assuring.
2. the added experiment shows the limitation of the approach in the real world.

Cons:
1. for pixel pendulum system the optimization procedure is weird. Given that the code of the original paper has been released, is it possible to check the hyperparameters used?

Overall this paper is assuring. And the added experiment exposes the weakness of the HNN under certain scenarios. I'd recommend the acceptance of this paper as it would contribute as a useful review to the original paper.",19-13.txt
4,https://openreview.net/forum?id=HJxNSp9MTr,1,8,4,"Problem statement:
The authors of the reproduction set up the problem well and appear to understand the problem being addressed by the HNN authors. The clarity of the reproduction report could be improved by including more details about the design of the neural network (the loss and the structure of the architecture, and how these are related to Hamiltonian mechanics), which is not mentioned. 

Code:
The authors reproduces the original code from scratch, as required by this track. They implement their reproduction in Tensorflow instead of Pytorch (as used by the original authors), which offers the additional benefit of showing how to implement the method in an additional framework.

Tuning the method and additional data:
The authors had difficulty reproducing the results of the original paper on inferring Hamiltonian state from pixel inputs. They are able to train the architecture and loss of the original paper using a handcrafted schedule for the different components of the loss, but not using the simpler method described in the original paper. The authors of the reproducibility study do not mention contacting the original authors. Given these difficulties, it would have been useful to speak with the original authors. 

The authors do not conduct an ablation study. However, they experimented with variations on the architecture to reproduce the results of the pixel experiments. They also provide an experiment on an additional real-world dataset. I commend the authors for this experiment, as it demonstrates the strength of the original results and the limitations on applying the HNN method on systems that are not strictly conservative.

Discussion on results:
The results presented in the reproduction are generally quite clear. The authors helpfully present and analyze their reproduction results experiment by experiment. This makes it clear which parts of the original are easier and which harder to reproduce.

There is room for improvement. The quantitative results in the final experiment (section 4) are not shown, but only described in text. These results show the sensitivity of the model to system parameters on a real-world system (a mass-spring system). This set of results is probably the most interesting contribution of the reproducibility report, and the report would be improved if these results were plotted and possibly shown in a table.

I believe this reproducibility report makes a wise choice in choosing to highlight the kinds of systems that the HNN does not model well: namely, nonconservative systems in systems and open systems in particular. However, the authors of the reproducibility seem to overstate the claims made by the authors of the HNN paper. claiming for example that: ""The results in the paper suggest that HNN is a solution to all the problems that are governed by physical laws."" In contrast, the HNN paper is designed to model conservative systems. 

The plots reproducing the results from the original experiment are very useful. Many of the displayed plot are in the same format as those in the original paper and code release. With this in mind, it would be very helpful to display the reproduced results side by side with the originals to allow readers to inspect the reproduced results along with the originals.
 
Organization, clarity, and presentation:
The reproducibility report and code are well organized and generally clear. 

However, I believe some of the phrasing in the introduction and abstract are unnecessarily broad (""In today's world, ..."", broad generalizations about trends in neural network research.) Similar unhelpful breadth occurs throughout the paper, e.g. ""The importance of understanding Hamiltonian Mechanics is felt when working with systems with many degrees of freedom like celestial systems, fluid systems etc.""

While the content of the report is generally clear, the report suffers from misspellings (e.g. ""Syplectic"" vs. ""Symplectic"", ""auickly"" vs. ""quickly"", etc.), incorrect capitalization and bad text spacing (""TO understand the concept of HNN we need to understand Hamiltonain Equ ations and Mechanics.""), and some awkward and ungrammatical prose (""Similar as in ideal mass-spring system, the first plot is the trajectory followed by each of the model.""). The report would be greatly improved by more thorough proof-reading.

The plots included are not vector graphics, and so are difficult to read at high resolution.",19-14.txt
4,https://openreview.net/forum?id=HJxNSp9MTr,1,9,5,"This is an excellent reproducibility study. The experiments of the original paper were produced using PyTorch, whereas here the authors implemented the method using Tensorflow. Thus I believe the code is built from the scratch. The report does not mention whether the authors were in contact with the authors of the original paper. Anyway, as it is implemented in Tensorflow and there is an additional experiment (mass-spring system), I would recommend publishing the code e.g. in Github.

As mentioned, the authors have implemented the method for yet another toy model, a mass-spring system. The results reveal an interesting phenomenon: when there is damping added to the system (i.e. there is energy loss along the trajectory), the baseline method (i.e. the non-Hamiltonian Neural Network) follows the energy of the exact solution more accurately.

Also, the authors have implemented the methods for a three-body problem in addition to the two-body problem of the original article (three body problem was found in the appendix of the original article). The results are much worse than in case of the two-body problem. This is an issue the authors of the original article should had emphasised more. It is well known that n-body problem is much more difficult to handle for n>2, so there might be even something more fundamental in play there.

I agree with the conclusions drawn by the authors, it does feel that the impression given by the original article is overly positive. It was a pleasure to read this report and I wish there were more of these for machine learning conference articles.",19-15.txt
4,https://openreview.net/forum?id=HJxNSp9MTr,1,7,3,"Overall, the report was able to reproduce all (or most of them) the systems presented in the original paper. It dives deep in one case where reproducing is challenging, so the author(s) had tried different adjustments/modifications (parameters and losses) to reproduce the original paper results. Furthermore, the report also goes a step further to experiment HNN on real world data and compare it with the Baseline (NN) and provides some interesting insights bout this comparison. 

Below are detailed comments on various axes:

1. Problem statement: the report clearly states the problem and summarizes the original HNN.

2. Code: the original code from the original paper was provided in pytorch, this report reimplements HNN in tensorflow.

3. Communication with original authors
The report did not mentions about any communications with the original authors for testing reproducibility.

4. Hyperparameter Search
On pixel pendulum experiment, the report tries various modifications to reproduce the results (not mention if they had any communication with the original authors) and successfully reproduces the results.

5. Ablation Study
The most interesting section is the experiment with real world data (section 4) and compare HNN with baseline on various range of 
β
. The reporting behavior is also interesting which indicates HNN is good at low values of 
β
 while baseline is good at high values of 
β
.

6. Discussion on results
The discussion this report suggests is that HNN maybe good for ideal situation while baseline is more applicable for real world system.

7. Recommendations for reproducibility
The report may be useful for reproducibility (section 3.6) in pixel pendulum experiment.

8. Overall organization and clarity
Overall the report has a good presentation flow, except for some minor typos (see detailed comments below).

* Detailed comments and questions:
- grammar (in page 1): ""are applied is in Physics""
- page 2, paragraph 1: ""followo"" -> ""follow""
- page 2, paragraph 2: Equ ation -> Equation
- page 2, paragraph 4: auickly -> quickly  ",19-16.txt
5,https://openreview.net/forum?id=BJxUSaczTH,1,7,4,"This paper is a replication track contribution to the Reproducibility
Challenge including a reimplementation of the major parts of the
method introduced in the NeurIPS 2019 the paper titled ""Tensor Monte
Carlo: Particle Methods for the GPU Era"" by Laurence Aitchison.  The
main contribution of the original study is a parallelizable (through
tensor operations) sampling method for multi-sample,
importance-weighted variational autoencoders (IWAE). The replication
paper re-implements the code using a different machine-learning
library, provides some clarifications to the original description of
the model that are useful for implementers, and presents replication
of the original findings with the new code also including establishing
the variability of the reported results. The authors also present
additional experiments for hyperparameter search and an ablation
study. The authors find the main claims of the original paper hold:
the proposed method works better than IWAV (as expected because of
increased number of samples), and the additional time required for
computation is rather small.  The authors publish their
re-implementation on GitHub.


This is a straightforward replication/reproduction study. The authors
use the same data set (with their own implementation) to test the main
claims of the original study. The contribution of the additional
experiments with the hyperparameter search and the ablation is rather
modest. The authors report interaction with the authors of the target
paper for clarifying some points, and also share the ease/difficulty
of the effort. Overall, the way the study conducted is sound, and
paper is well written. The study would be much more informative, if
the authors reported experiments with additional data (as they also
note in the paper). However, as it is I think this is a good
replication paper. I have a few minor notes below.

- I believe an abstract would be useful, even if it is not required.
- The authors should not use parentheses around the name-year
  citations that are used as names. E.g., ""works of (Kingma and
  Welling [2013]),""  -> ""works of Kingma and Welling [2013],"".
- footnote marks (footnote 1) should follow the punctuation.
- Section 3, first paragraph: The sentence "" This to justify our
  narrowing down of explanations of the work done in the TMC paper.""
  lack a verb.
- Section 3.1 the second bullet point: I cannot parse the sentence
  ""The results in the TMC paper were presented with and without these
  techniques, why our reproduced results are still comparable.""
- Page 6, paragraph above the figure: ""what does the reconstruction
  look like"" -> ""what/how the reconstruction looks like"" ?
- Table 1: what are the values following ±, range, standard deviation?
- Page 7, third line below the figure: I also have difficulty parsing
  ""This since the parameters of the prior distribution are fixed – the
  model assumes a standard normal distribution, why there is no new
  information when evaluating the latent variables under the same
  distribution parameters."" looks like the same as above usage of
  ""why"", should it be  ""that is why"", ""therefore"" or maybe ""so that""
  instead of ""why""?
- The order of the figures and their discussion does not match.
- Continuous ""heat map"" bar for the categorical values is somewhat
  confusing in Figure 6.",19-17.txt
5,https://openreview.net/forum?id=BJxUSaczTH,1,7,3,"The authors of the reproducibility report for the work ""Tensor Monte Carlo: Particle Methods for the GPU era"" begin their report restating the major concepts of VAE, IWAE, and TMC methods.
The report restricts reproduced results to the case of non-factorized models and experiments to the MNIST dataset.
Results are presented for the IWAE and TMC models together with a hyper-parameter evaluation for different sample sizes K and learning-rates. Although the reported results differ between the original paper and the report, the general claims of the original work could be reproduced.


Pros:
- The report complements the original work with a more detailed description of training / testing schemes,
pseudo-code and a graphical overview of the TMC architecture which help to understand and build-upon the new method.
- The well structured and documented provided code basis is a starting point for further investigations.
- Besides some typos, the report is well written and structured.


Cons:
- As indicated by the authors, an evaluation of the difference in results between the original paper and this report is missing (e.g. effect of missing weight normalization).
- Some minor results of the original work were not reproduced (e.g. toy example).
- Even some evaluation of runtime was performed, more rigorous performance tests of the different models and runtime dependency on hyper-parameters would have been desired.  
- Licenses are missing for the github repositories of the original work and the reproducibility report.",19-18.txt
5,https://openreview.net/forum?id=BJxUSaczTH,1,7,3,"This is the reproducibility challenge writeup for “Tensor Monte Carlo” (https://papers.nips.cc/paper/8936-tensor-monte-carlo-particle-methods-for-the-gpu-era). The authors primarily focused on reproducing the MNIST NLL result from the original paper with a new implementation in Tensorflow (vs the original PyTorch implementation). They do not seem to have managed to get exactly the numbers of the paper (there seems to have been an implementation issue with weight normalization – as referenced in both the codebase and report – along with some other error but un-investigated), but managed to get the general trend of better NLLs with more importance samples (due to the tensor tricks in the paper) at negligible computational cost. 

Summary: I think that the authors of this challenge did a pretty good job overall in both implementation and writeup. They managed to replicate one of the experiments getting the qualitative results, but not the quantitative numbers. To improve my score, they would have to track down the implementation differences between their implementation and the authors'.

From a writing perspective, it would have been very nice to explain why Eq. (3) is efficient to compute and doesn’t scale linearly in the number of samples. Currently, I have to go back to the original paper to understand this reason – which makes the paper non-trivial to understand. 

I agree with the comment in Section 4.1 – although the Tensor Monte Carlo (TMC) bound is probably straightforward to implement and monotonically closer to the true p(y) than the IWAE bound/ELBO, detailed experimental details (ie training procedures, etc.) are always extremely helpful. This fact isn’t really a failing of the TMC paper however, as it’s unfortunately common throughout the field.

With respect to the first reproducibility comment in Section 4.2, I both agree and disagree. Having some experience in the VAE area myself, I believe that this discrepancy is due to differences in how VAE models are implemented in Tensorflow (and other static computation graph autograd software) vs PyTorch (which exectutes eagerly – enabling reparameterization on the fly). I have somewhat low confidence in this statement however. As such, it is both a writing issue (which ought to be made more clear by authors of VAE papers) but also a technical understanding issue.

“Furthermore, our implementation of IWAE …” Please be more specific in this sentence and mention the number of samples used for K in IWAE and the effective number of samples used in TMC.

Given the “exponentially many number of samples for TMC,” are the K values mentioned in Figure 2 actually the number of samples? Alternatively, given that this is the small model – isn’t this just IWAE then?

I would have liked more discussion on the clustering results and experiments – my understanding is that a four dimensional latent space was used – so are these the first two latent dimensions?

The authors mention fair communication with the author of the TMC paper, and seem to have looked through the PyTorch codebase. 

They also seem to have done a fair amount of due diligence wrt the hyperparameter sweep – obviously, learning rate seems to matter a lot.

Scoring rubric: (Note, I’ve never reviewed a reproducibility challenge before so I don’t have a good calibration scale)

Problem statement: 3.5 / 5 (this is somewhat low because I think the TMC paper is about more than just a better VAE, whilst the reproducibility challenge is all about re-implementing that experiment)
Code: 4/5 (code is commented, seems to be somewhat minimal and understandable. I didnt try running it)
Communication with original authors:  4.5/5 (Based on repeated references, it currently seems fair enough to me)
Hyperparameter Search:  4/5 (the usual suspects were investigated)
Ablation Study: 3/5 (although I really would have liked to see something thorough like knocking out portions of the original papers’ code to see what gets that implementation back down to your numbers. Particularly, if the actual explanation is not averaging over multiple runs, show the numbers for a single run demonstrating that this is the case.)
Discussion on results:   4/5 (I think the authors did a pretty good job overall) 
Recommendations for reproducibility: 4/5 (I really like the abridged sentences at the end of each section. Good job!)
Overall organization and clarity: 4.5/5 (The paper is organized well, a couple of minor typos and organizational issues below)

Typos and Minor Issues:
- you misspelled reproducibility in the README of your github repo :)
- section 6, line 4, there doesn’t seem to be a space between the “the” and the citation
- the citation for the paper you’re reproducing is incorrect – please include the conference affiliation and reference there instead of the arxiv version.
- consider using \citet instead of \citep if possible when mentioning the paper in text – e.g last paragraph of Section 2, In \citet{burda…}, the authors state two fundamental benefits of the approach… should remove the parentheses.
- It would be very nice to include a short abstract in the pdf of your report as a summary. Something like: In this reproducibility challenge, we reimplemented Tensor Monte Carlo (citation) in Tensorflow. We then managed to reproduce the qualitative results of the TMC paper by comparing to our own IWAE (citation) implementation for VAEs on MNIST. Some discrepancies seem to remain in our implementation that include the lack of weight normalization and ....",19-19.txt
6,https://openreview.net/forum?id=SklFHaqG6S,1,7,4,"Selected for ReScience-C Journal Publication. 

Good overall reviews, good reproducibility effort with clear and well-written report and open-sourced clean codebase which adds to the understanding of the original paper.",19-20.txt
6,https://openreview.net/forum?id=SklFHaqG6S,1,9,4,"In this report, the authors reproduce parts of the computational experiments of ""One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers"" by Morcos, Yu, Paganini, and Tian (MYPT). Roughly speaking, MYPT experimentally demonstrates that the ""winning tickets"" are transferrable across datasets and optimizers within the natural image domain. Of the 6 datasets the MYPT experiment with, the authors reproduce the results with the smaller 4 datasets. This choice is justified given the computational limitations, as the authors discuss thoroughly.

The undergraduate students clearly learned a great deal from pursuing this project. The writing is excellent, and the paper provides value to the field in the following ways
- It reproduces and confirms the results of MYPT for the smaller experiments.
- Clean code is open-sourced and released. (MYPT did not release code.)
- The clarification of experimental details (data augmentation and random masks) is useful.
- Discussion of the cost of reproducibility is also useful. Perhaps these concerns are obvious to experienced practitioners. However, for students or researchers without computational experience, such information is interesting and helpful.

For these reasons, I view this manuscript as an excellent output of the reproducibility challenge, where the students learn a great deal while providing value to the larger ML community. I recommend acceptance.



In this report, the authors reproduce parts of the computational experiments of ""One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers"" by Morcos, Yu, Paganini, and Tian (MYPT). Roughly speaking, MYPT experimentally demonstrates that the ""winning tickets"" are transferrable across datasets and optimizers within the natural image domain. Of the 6 datasets the MYPT experiment with, the authors reproduce the results with the smaller 4 datasets. This choice is justified given the computational limitations, as the authors discuss thoroughly.

The undergraduate students clearly learned a great deal from pursuing this project. The writing is excellent, and the paper provides value to the field in the following ways
It reproduces and confirms the results of MYPT for the smaller experiments.
The code is open-sourced and released. (MYPT did not release code.)
The clarification of experimental details (data augmentation and random masks) is useful.
Discussion of the cost of reproducibility is also useful. Perhaps these concerns are obvious to experienced practitioners. However, for students or researchers without computational experience, such information is interesting and helpful.

For these reasons, I view this manuscript as an excellent output of the reproducibility challenge, where the students learn a great deal while providing value to the larger ML community. I recommend acceptance.



Minor comments:
- It is unclear why sections 2 and 3 are separate. For example, the description of the pruning mechanism is split across the two sections, which makes things less clear.

- I would be interested in seeing the VGG19 fashionMNIST results when they are done.

- I find that section 6, which claims the authors will continue working on reproducing the paper, entirely unnecessary. Frankly, it is hard to believe that the authors will do so, and it is not what I would recommend. The authors should move on to other projects or studies. In publications, ""future work"" usually refers to future research ideas.

- The formatting of the references has some inconsistencies and incorrect capitalization. For example, CVPR is abbreviated while ICLR is not. MNIST should be capitalized.",19-21.txt
6,https://openreview.net/forum?id=SklFHaqG6S,1,8,4,"This work is in the replication track.

Problem statement:
This replication contains clear hypotheses from the original work to be evaluated. They include experiments replicating those from the original paper, and are clear about which experiments support which hypotheses.

Code:
The authors link to a code repository. While it does have documentation and instructions on how to run it, I haven't tried so I can't measure its quality.

Communication with original authors:
The authors of this work claim they contacted the authors of the original work to inquire about data augmentation and random masks. There isn't much discussion here, but this seems reasonable, as they claim reimplementing the pruning algorithms was fairly simple.

Hyperparameter search:
The authors are fairly clear about their experimental setup, and include some information about hyperparameter settings. They list the initialization type, the learning rate and annealing schedule, and the number of epochs. They have a fairly clear description of the models used. It seems like they did no additional hyperparameter search (only taking hyperparameters from the original work), and they don't mention the number of initializations they evaluated so I'm assuming it was just one (there are no error bars).

Ablation study:
I see no ablations.

Discussion on results:
They do have a discussion section in which they claim they reproduced the original paper's conclusions. I would argue these experiments show there is more nuance than just that the claims of the original paper are reproducible. For example, the claim that winning tickets from more complex datasets (with higher number of classes) generalize better is supported by experiments in Figure 3. However it's only supported in 3/4 plots in Figure 3, one of which is showing that CIFAR-100 tickets generalize better than CIFAR-10 to CIFAR-100, but this could be because the tickets are generated and evaluated on the same dataset (there is no transfer).

Recommendations for reproducibility:
The authors release some winning tickets, which is useful. They also include a section on the cost of the reproduction, which is great. They listed the hardware they ran on, and how much cloud compute cost, and how long training the models took. This is a high-quality contribution to the report.

Organization:
The paper is clearly organized and well written. The hypotheses to test are clearly stated, and it's clear which experiments support them.",19-22.txt
6,https://openreview.net/forum?id=SklFHaqG6S,1,7,4,"Morcos et al. Original paper: 
Empirically shows that winning tickets are transferable across datasets (and that larger dataset transfer better).
Datasets - MNIST, SVHN, CIFAR-10/100, ImageNet, Places 365
The authors use late resetting across all datasets, global pruning + iterative magnitude based pruning on two models VGG19 and ResNet-50 
The random mask (in differentiation with previous work) varies both structure and value.

Replication report:
The authors replicate the results of the Morcos et al. original paper from scratch + open source the checkpoints. In addition, a complete discussion of the computing resources and cost of reproducibility is included. Given that Morcos et al. did not release code (at least at the time of this submission) this is a very valuable open source contribution. 

Further clarification is needed from the authors about why the random baselines appear to degrade more sharply then the corresponding baseline in Morcos et al paper? This is not discussed in the submission. In addition, the results for transferring winning tickets within the same data distribution appear to be considerably lower than that reported by Morcos et al. for cifar-10? 

Comments on report: 
1) The authors evidence clear understanding of the problem statement of the original paper. The authors replicate the full set of experiments on a subset of the datasets in the original paper (Cifar-10/100, SVHN, FashionMNIST). The authors demonstrate rigor in the adherence to many of the original hyperparameters (epochs, initialization, learning rate schedule, optimizers)
2) There remain some limitations to the replications - the authors did not replicate the experiments for all datasets which leaves a gap in our understanding reproducibility of results for larger datasets. The authors did not replicate the 6 random seeds the original authors implemented for each variant. This is acknowledged by the authors and given the computational cost and spirit of the reproducibility challenge is very understandable.
3) “We set the value of the parameters to be pruned to be zero before each forward pass” - does this diverge from the  implementation of gradient updates during iterative magnitude pruning used by Morcos et al?
4) In fig 1., it appears that the test-set accuracy at convergence for same data transfer is far lower than that reported by Morcos et al. in figure 2? For example, cifar-10b in replication appears to hover around 50% vs. in Morcos et al around 70%? 
5) Across all charts fig 1,2, 3, 4  the performance of random appears to differ from that reported in the associated charts in Morcos et al (fig 5)? In all these experiments, the degradation in random performance appears to be far sharper. Is there a difference in the code implementation that would explain this difference?

Overall organization and clarity (+ small suggested improvements):
1) The writing is very strong, well structured + easy for the reader to follow. 
2) Small nit pick: (section 4) the imagenet dataset in the Morcos et al. is not 10 million images, but is the reduced ImageNet dataset typically used ~1.8 million. 
3) Small nit pick: the color legends for random change plot to plot. Would be easier for the reader if these were a  single color.",19-23.txt
6,https://openreview.net/forum?id=SklFHaqG6S,1,5,4,"I recommend weak rejection; I have following major concerns.

1. The implementation seems to implement training of ""pruned neural nets"" by alternating between an SGD step and zeroing out step. I am not perfectly sure if that is exactly equivalent to what is typically done; I would rather implement ""masked linear"" and ""masked conv2d"" class, with custom ""forward"" operations which take into account a mask buffer. Following the method the authors used, I believe that there could be a significant discrepancy, in terms of the convergence speed (and even optimum itself).

2. Authors' description of random tickets and random masks (Sec 2.4.) are somewhat confusing. As the authors correctly indicate, ""random tickets"" are ""winning masks + random weights,"" while ""random masks"" used in the original work is using ""random masks + random weights."" But the authors choose to use the ""random mask"" baseline instead of ""random tickets,"" which is a weaker baseline, without no clear reason. Perhaps this lead to the discrepancies between the plots in the original paper and the reproducibility report.

- Minor remarks

1. I do not see any batch size mentioned in the report.

2. Overall, authors correctly indicated the problem setup and contribution of the target paper. One thing that I am not sure about, is their statement in Sec 5.4. that ""different architectures have different sensitivity to pruning fractions""; this is NOT a very new observation, or the claimed contribution of the target paper (or even nontrivial).",19-24.txt
7,https://openreview.net/forum?id=SkxCSTqG6H,1,6,3,"This paper provided a comprehensive study on the paper ""Generative Modeling by Estimating Gradients of the Data Distribution"". Due to the absence of some details of parameters, the SOTA results were not been reproduced.  In addition, the authors conducted more experiments by replacing the architecture of the score network as U-net.   

1 . We recommend that the authors could conduct more results on other larger dataset, such as imagenet, which was composed of much more images.  
2.  In addition, the reproduced paper has already released its code ""https://github.com/ermongroup/ncsn"".  The authors can direct compared the reproduced results and hyperparameter settings with the original paper.  ",19-25.txt
7,https://openreview.net/forum?id=SkxCSTqG6H,1,6,2,"Problem statement:

I do believe that the authors have explored the ideas from the original work in detail. 

Code:

Authors of this reproducibility report have tried to reproduce the results from scratch and changed the implementation framework from PyTorch to TensorFlow 2.0. The GitHub link was provided, and the commit history suggests that the implementation was indeed from scratch. However, I would like to point out that there was a lack of documentation to help navigate the codebase.

Communication with original authors:

The report mentions some communication with the authors through correspondence, but, it seems that no interaction took place on the OpenReview forum. With regards to the discussion that is reported:
1) It is unclear why the authors did not clarify certain implementation details. In particular, several architectural assumptions have been made for MLP  and ResNet. One can not help but ask if there was correspondence, why weren't such queries resolved?
2) Authors mention that they were directed to the original RefineNet paper for details; however, the basis for the final assumptions remains ambiguous. 
I understand that every little detail can not be verified in the mail and would also result in unwanted delays. However, multiple departures from the original work would lead to compounding errors. Moreover, it is essential that authors precisely specify their assumptions as to their own, if any. 
 

Hyperparameter Search:

The authors take away some extra points in this direction. They experiment with a broader range of two main hyperparameters. Further, they also conjecture that the original paper seems to have understudied the sensitivity of these two factors.  Moreover, the authors also explore different architectures and annealing schedules, the latter of which provides novel insights. 

 
Discussion on results:

The authors of this report have been able to reproduce the qualitative results and provide relevant details on several occasions. For any quantitative discrepancy, the authors have made a commendable effort to expand on it. 

Recommendations for reproducibility:


The authors in this report identify two specific instances where the hyperparameters used by the other authors differ from what is reported in the paper as per this report; (i) the 
σ
 values for the Gaussian Mixture Model experiment, and (ii) the dilation rates for RefineBlocks. 

Furthermore, their discovery of the correct range of \sigma values raises some critical questions regarding the choice of other hyperparameters, 
ϵ
, and 
T
, which they have further explored in Section 6. I believe that these discoveries require some additional comments from the NeurIPS authors.   

Overall organization and clarity:

Overall the report is well written. I offer some optional suggestions to improve the organization:
1) The authors should be clear about where their implementation diverged from the original. More specifically, the authors should be very precise about the assumption they are making even if they do not expand on the reasoning behind every small detail.
2) The introductory section can use more refinement. Even if such summaries are not the primary objective of a reproducibility report, it felicitates for a more comfortable overall read.


Final Summary:

The results of the reproducibility report broadly verify the original hypothesis. Apart from the replication of the qualitative results, the authors of this report provide some novel insights into the working of the paper and also identify some instances where the numbers reported in the original work differed from what was used in their implementation. 
Several claims are qualitatively verified; however, the quantitative results have not been satisfactorily replicated. More specifically, there is a discrepancy in FID and Inspection scores on CIFAR10. The authors offer two explanations for this, (i) difference in small architectural details, and (ii) high-variance model selection metric. 

In particular, I would recommend authors to discuss further the possibility of re-reporting the FID score for CIFAR10 dataset. I will be very interested in re-analyzing the numbers when more images(2048 or higher, as reported by the authors) are used to select the best models from the checkpoints. If not, possible for all the checkpoints, can we do this for a smaller subset of checkpoints? This exercise shall help us broadly identify the lesser of the two evils(implementation difference and metric variance). Furthermore, I would like to hear what the authors have to say regarding the lack of communication with the original authors on the points mentioned above.

Overall, I believe that with some further explanations on their experiments and above mentioned changes, this work can be welcomed into successful reproducibility reports collection. ",19-26.txt
7,https://openreview.net/forum?id=SkxCSTqG6H,1,7,4,"The report falls under Track 3, of attempting to reproduce the results of the paper: https://arxiv.org/pdf/1907.05600.pdf

Here are my thoughts:

Report's general summary of paper/Problem Statement: The (reproducibility challenge) authors provide a good description of the paper in question. The paper's main idea of using a score network to estimate the gradient of data log-likelihood or score. An objective is designed to carry out this estimation as explained in the main paper. However, the base estimator does not perform well when the number of samples is small (as happens in points of low likelihood), and as the data lives in a much lower latent dimension, where the gradients might not exist. As a result, the data is perturbed with noise so that samples are available everywhere and estimation is possible. For inference, Langevin dynamics with a schedule akin to simulated annealing is used. The report adds a few insightful comments that while noise perturbation is beneficial during training, it can be harmfui during inference and one therefore needs to reduce noise level with a schedule.

Code: Code is provided in tensorflow 2.0, which is not only different from the author provided pytorch code, but is also written in a different framework. 

Communication with Authors: Apparently, the paper's authors were contacted to clarify network architecture choices, upon which the authors pointed to their code implementation, with which a few architecture clarifications could be gleaned in relation to RefineNet and architectural components. 

Hyperparameter search - sensitivity to inference parameters: The report goes over a set of hyperparameter studies for sensitivity to parameters epsilon and T (low, high) and offers insight into sampling performance in these scenarios (low epsilon - does not move from noisy initial point, likewise for low T; high epsilon - too large step size leads to clipping to end points, large T settles on what might seem like a mode, losing details).

Reproduction of results: The report does a great job of reproducing and explaining the results in the paper. 

1) Toy problem with GMMs: Results with Sliced Score Matching reproduced, showing that the paper's motivation is correct. In addition, during reproduction with Langevin dynamics, the report notes that the paper's settings for 
ϵ,σL
 don't work because of numerical overflow. They go on to modify these with a different setting (larger epsilon, fixed 
sigmaL
) to recover a learning curve close to the paper's.

2) Main experiments with images: The paper is largely able to obtain good looking samples for image categories considered (MNIST, CIFAR-10, Celeb-A), with a sensible discussion of experiment settings. However, the report also notes that their evaluation scores (Inception, FID) were considerably worse than those reported in the paper. They attribute this to differences in number of samples used (1000 used, whereas FID needs 2048), differences in architecture, data. 

3) Results from inpainting experiments are very good and show that the network does not memorize images, when tested with data with symmetry removed. 

4) Nearest neighbor experiments also show that the network does not memorize. 

5) Additional experiments on annealing schedule show that it might be possible to combine a linear annealing schedule along with paper's proposed geometric recommendation. 

6) Network architecture alteration: Unet recommended as better than Resnet owing to skip connections propagating more information (also seen in results). 

Recommendations for reproducibility: The report comes up with a few ideas: the note on sensitivity to inference parameters (epsilon, T); linear annealing schedule and unviability of simple (non-Unet based) architectures. 


Summary:
Generally, the report reads quite well. The model is described clearly, adding insight to the paper. Likewise, it is easy to follow the experiments and analysis. The report is also able to reproduce the general results of the paper. My suggestions to the report authors is to 

1) Elaborate on the refinenet architecture for the benefit of the reader with some ablations on its components as applies to this model. Are we using identical architectures to that in the paper (no, is my reading)?

2) A bit more discussion on the model itself. While it is not very difficult to follow the thread if we have read the paper (which in itself is written very well), and perusing the report adds to the reader's understanding, a bit more elaboration on the theoretical aspects would be helpful. 

3) The report is able to reproduce many of the paper's claims. However, the comparison metrics are off. I think a bit more of an in depth study on why this happens would be interesting. Could it also be due to architectural differences (if any) with the paper, in addition to the speculations that the report makes for the FID.",19-27.txt
7,https://openreview.net/forum?id=SkxCSTqG6H,1,9,4,"This report aims at reproducing the results of the paper ""Generative Modeling by Estimating Gradients of the Data Distribution"" by Song and Ermon (2019) from scratch. The original work proposes a novel generative model by sampling the data distribution using stochastic gradient Langevin dynamics (SGLD) and score matching. The performance of the model is demonstrated for image generation and image inpainting.

PROBLEM STATEMENT
The model of proposed in the original work is very well summarized and all explanations are clearly described. The problems of the model (manifold hypothesis and low-density), as well as the proposed solutions, are also briefly discussed.

CODE
The code is very readable. Some implementation decisions were documented in the code.

COMMUNICATION WITH ORIGINAL AUTHORS
As far as I can see there was no communication with the authors of the original paper, neither through OpenReview nor through GitHub.

HYPERPARAMETER SEARCH
The authors of the report additionally analyze the robustness of the choice of the SGLD hyperparameters. Their experiments show that the quality of samples highly depend on the noise level and the number of steps. This is not surprising, but important to mention.

ABLATION STUDY
The authors apply a linear (instead of a geometric) annealing schedule, which leads to more detailed, but also more noisy results. A different neural network architecture was used for the experiments. The results show that the choice of network structure seems to be very important.

DISCUSSION ON RESULTS
The results of the experiments are discussed in an appropriate way. The authors are very transparent about which experiments they were able to reproduce and which not. The presentation of the results was done very thoughtfully and nearly all results from the original paper were reproduced. The authors also report standard deviation of the Inception score. A table comparing the achieved scores with the scores reported in the original work would have been nice.

RECOMMENDATIONS FOR REPRODUCIBILITY
No recommendations.

OVERALL ORGANIZATION AND CLARITY
The organization of the report is flawless. 

CONCLUSION
The report is of very high quality and the experiments seem to be performed very extensively.

MINOR CORRECTIONS
- ""sub-par"" should be ""subpar""",19-28.txt
8,https://openreview.net/forum?id=rkezvT9f6r,1,6,3,"The authors well-studied the MBPO paper and reimplemt MBPO with PyTorch. The experimental results on two continous control tasks, Hopper and Walker2d, demonstrated that the MBPO is reproducible. The conclusion of the MBPO paper has been validated. Details of the re-implemented algorithm and hyper-pameter settings have been provided. I would like to encourage the authors to open source their reproduced code to the comunity. 


minor:

Section 4.4: ... based-free algorith...m -> ...model-free algorithm...",19-29.txt
8,https://openreview.net/forum?id=rkezvT9f6r,1,8,2,"The report is very well written, (minor glitch the abstract on the openreview.net which is supposed to be copied by the introduction has some typos, needs to be recopied and also fixed latex to text typos). The authors of the report seem to have a very good understanding of the reinforcement learning algorithms they implemented. They took the extra step to replicate the code of the paper from scratch, by consulting the pseudocode in pytorch, while the original implementation was in tensorflow. I find it interesting that they chose to implement libraries that didn't exist in pytorch, so that they can do the comparison. Their experiments and the evidence they produced suggests that they were able to replicate the results even though they used a different platform.

I would question the clarity of figure 2. It is a bit confusing. The red curve is the average return and the shadow the standard deviation. While the red is smooth the blue is not. It is not clear to me why",19-30.txt
8,https://openreview.net/forum?id=rkezvT9f6r,1,7,3,"The authors have conducted a careful replication of the original paper. They appear to have understood the algorithm well and successfully re-implemented it. Impressively, they did so in a different framework -- PyTorch instead of Tensorflow -- and were able to get very similar results. One weakness of their replication is that the accuracy of their replicated predictive model was much weaker. However, this is excusable since it is not the focus of the original paper. The writing is clear and the results are reported well.",19-31.txt
9,https://openreview.net/forum?id=SJlRPTqM6B,1,8,3,"The paper presents a solid ablation study of some of the methods proposed in ""Latent Weights Do Not Exist,"" including analysis of the sensitivity of the Binary Optimizer (BOP) to its gamma and tau hyperparameters and exploration of its applicability to different architectures. It also broadly confirms the paper's main result that latent weights are not necessary to train binarized neural networks effectively (one or two continuous optimizer states per parameter are all you need).

While the paper is certainly readable, it could benefit from some additional proofreading. For instance, ""deep learning methods are finding _itself_ useful"" should be ""themselves"" and ""never flipping of weights"" should be ""never flipping weights"". But again, this wasn't an impediment to understanding.

My main criticism is that the authors treat the latent weights and BOP approaches as mutually exclusive when in fact they're closely related mathematically. In some cases they may even be equivalent (the original paper mentions that when the threshold tau is set to zero, BOP is a reparameterization of SGD with latent weights). So I think it's inaccurate to say that the original authors claimed ""that binary weight cannot be considered as an approximation of latent weights""; the argument was more that they _shouldn't_ be considered an approximation of latent weights (and that latent weights are more accurately understood as something like momentum/inertia). Maybe another way of putting it is that the claim in the original paper is along the lines of ""that the latent weights found in traditional BNN optimizers are an approximation to the binary weights is a mathematical coincidence"". In any case, though, that's mostly the fault of the original paper for making a somewhat vague overall claim, and not of this reproduction paper (which makes purely empirical claims that seem to be well-justified).

I think I'm submitting this review a bit too late to get the actual rating+confidence drop-downs, but my intended rating is:
Rating: 7: Good paper, accept
Confidence: 3: The reviewer is fairly confident that the evaluation is correct",19-32.txt
9,https://openreview.net/forum?id=SJlRPTqM6B,1,9,4,"The report provides a comprehensive review of ""Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization"" paper by Helwegen et. al. They formulate several interesting hypotheses related to Helwegen et al. paper and study them in detail. The code accompanying the report seems to be well documented as well.

The report validates several conclusions from Helwegen et. al. For example, applying latent weights to BNN does not outperform the binary weights, and that binary weight cannot be considered as an approximation of latent weights. The report also considers the effect of different hyperparameters and shows that Layernorm can be considered as a potential replacement for BatchNorm in BNNs.

The report is well written. It raises insightful questions and considers meaningful experiments to answer them. The analysis is thorough, and the only suggestion that I can provide is: The authors (of the report) could have run the experiment with multiple seeds to strengthen their conclusions even more. Please note that this is not a criticism of the report (which is doing an excellent job).",19-33.txt
9,https://openreview.net/forum?id=SJlRPTqM6B,1,7,3,"Quality: high. The report confirms the main results of the original paper, and extends the evaluation further into a new task (denoising autoencoders) and evaluates layer norm as an alternative to batch norm, beyond the original paper.

Clarity: high. The report is presented well, and includes good coverage of the related literature.

Originality: high. The extended evaluations provide value above and beyond mere reproduction.

Significance: high. In addition to demonstrating the minor improvements achieved by layer norm, the extra results on the auto-encoding task demonstrate preliminary evidence that the binarized nature of either the network or training regime makes it difficult to obtain high-quality reconstructions, and this is beyond what was considered in the original paper.",19-34.txt
10,https://openreview.net/forum?id=B1gZ8acM6S,1,9,5,"The authors reproduce the paper Zero-shot Knowledge Transfer via Adversarial Belief Matching including a full re-implementation and reproduction of methods, experiments, and evaluation. The original work has been thoroughly reproduced. In addition, the authors also provide a modification of the main method in an attempt to yield better zero-shot knowledge transfer results. I recommend strong acceptance.

In particular, the authors propose to modify the zero-shot training setting by leveraging the updated generator to provide with different batches to update the student based on the teacher's output on these samples, as opposed to using the same sample which was used to update the generator. Their approach yields a more diverse pseudo-training dataset and therefore provides an improved training setting for the student network. The authors report that this modification brings improvements close to 3 percentage points for some cases. 

Moreover, the authors first designed implementations themselves, and also report that they had to integrate some details in the implementation from the official repository of the authors which were not indicated in the main paper itself. 

Additionally, the authors perform a few methods of data augmentation on CIFAR-10 in addition to normalization and report small improvements with these additions. The results are thoroughly explained for all experiments.",19-35.txt
10,https://openreview.net/forum?id=B1gZ8acM6S,1,7,4,"The paper fully re-implements and reproduces all the methods and experiments in the original paper.

Pros:
1. The paper is clearly written and easy to follow. Besides the reproduced results, it also provides a good introduction of the techniques used in the original paper.
2. In section 3, the paper gives a handy summary of reproducibility issues they found, which may be helpful to other researchers.
3. All the experiments were reproduced with results similar to the original paper. Most of the parameter and training details are well presented in the paper. Discussions on the results are given.
4. The authors also suggest an approach for further exploitation, although this may not be the focus of this challenge.

Cons:
1. As this is a paper submitted to the baseline track, it is expected to show results in hyper parameter sweep and ablation studies. However, these are not fully investigated in the current paper.
2. From the paper, it seems that the authors may not have many direct communications with the original authors. They mainly obtained information from the original paper and the original codebase.
3. The ""Student scratch"" column of Table-1 contains three identical values, 91.38 + 0.33. Any typo or error here?
4. The authors should consider dividing section 5 into subsections for a good paper structure.",19-36.txt
10,https://openreview.net/forum?id=B1gZ8acM6S,1,6,4,"In this report, the authors attempted to reproduce the work in paper “Zero-Shot Knowledge Transfer via Adversarial Belief Matching” in NeurIPS 2019. The authors implemented the proposed method and baselines used in the original papers from scratch, but with consulting the codebase of the paper for hyperparameters. The authors were successful in reproducing all the original findings but with consistent better model performance (Table 1). Reasonable discussion on this improved performance is provided in the report. The author also went beyond the paper, modifying the original training process to improve the diversity in the training examples for the student network in every iteration. Their results show that this modification is effective, with improved performance in all settings (Table1). 

The writing of the report has room to improve. There are typos and inconsistent notations. For example, n_g and n_G both appear in 2.4; “The initial learning is equal to …” in 4.2, should it be learning rate? Also, there are places that need to be clarified. What is the GitHub link in 3.1 item 1? In 4.1, it is mentioned “majority of these images used as training set”, what exactly the percentage is? How exactly the data were split? The authors did not provide the recommendations for reproducibility. I guess just based on the content of the original paper and its supplementary material, it would be difficult to reproduce the results, because the authors had to consult the codebase of the original paper. 

In Figure 1, when M increases, the performance of zero-shot has little change to no at all. It could be interesting to see with continuing to increase M (up to 5,000) if zero-shot can ever achieve the performance that KD+AT-full- data has. ",19-37.txt
11,https://openreview.net/forum?id=pe2pMH75H,0,4,4,"The authors performed a replication study on Xia and Chakrabarti's paper. They re-ran the original experiments and confirmed the results were replicated. However, there was no thorough investigation of hyperparameters, other data sets, ablations, etc. The authors did not really provide any meaningful insights from their experiments. Overall, I'm afraid the study does not really contribute anything meaningful to investigation of the underlying paper.",19-38.txt
11,https://openreview.net/forum?id=pe2pMH75H,0,4,3,"1. The specific problem tackled by the paper:

The authors aim at the reproduction of the paper  '“Training Image Estimators without Image Ground-Truth' which shows an image estimator can be trained through unsupervised data with two different measurements.

2. Claims of the paper:

The authors follow the code of the original paper and report the same numbers can be obtained.

3. Decision (accept or reject) with one or two key reasons for this choice and reasons for the decision.

Reject.

Detailed comments

Cons:

This manuscript is more like a very informal report. The authors just claim that they have re-run the code, and the results can be reproduced. The lack of background information on the problem, proposed algorithm, and experimental details makes it very hard to read without going through all the details of the original paper.

Although it is good that the original paper is reproducible. I recommend rejection of this manuscript due to its self-completeness and readability.",19-39.txt
11,https://openreview.net/forum?id=pe2pMH75H,0,2,4,"The paper reproduces the two experiments in the original paper by using the official published code, however, there are many evident defects during reproducing and I think the paper should be rejected for the following reasons:

1. The reproducing process described is too simple, which is nearly meaningless. For example, The authors claim that they try to figure out why the algorithm is much slower than depicted in the original paper, but they say nothing about what they try, how about the results, and reasonable explainations.

2. It is important for a image estimator algorithm to yield satisfying images, thus it is of vital importance to check the visualization of the generating image. Unfortunately, the authors do not give any attempt to visualize the results of the algorithm.

3. The paper also lacks in many essential descriptions about the configuration, the hyper-parameters, the effectiveness of the algorithm.",19-40.txt
12,https://openreview.net/forum?id=Ss-TvdTmS0,0,7,2,"I really enjoyed reading this report as it provides a clear description of the attempt, and it is made even better by the authors reproducing the results from scratch (I understand there was no source code available from the original paper).  Discussion on the results, reasons for where the discrepancies occur are clearly described. Only missing ingredient is discussion with original paper's authors as it would be extremely helpful for the readers to know if the report authors attempted to contact original authors for any discrepancies or clarifications. Small suggestion: Please consider changing figures to tables where applicable.",19-41.txt
12,https://openreview.net/forum?id=Ss-TvdTmS0,0,4,2,"Thank you for your efforts to reproduce this paper! I enjoyed reading your report. Below is my review:

---

The authors reproduced the paper ""Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods"" by Liang et al. Figure 4 is a nice table that summarizes the reproduced result. Most accuracy values closely align with the original work's values. Section 3.1.2 is an informative addition to the paper.

There is not much information in Section 3.2 (Language Modelling), especially compared to Section 3.1 (Document Classification), due to the limited computation. Although the authors were unable to match the values in the original paper, it would be good to see some conjectures on why they were unable to reproduce the paper.

It would have been a good addition if the authors could contact the authors of the original work to find the assumed values listed in Section 3.1 and analyze the deviations in Figure 4 further. The paper also has quite a few grammatical errors which make the report difficult to read at times.

Here are the major fixes that I recommend:

M1. In Section 3.1.2 Paragraph 1 (""The datasets...""), the explanation of Figure 5 seems lacking. Which ones were run on 1080Ti, and which ones were run in Colab?
M2. In Section 3.2 Paragraph 1 (""Similar to...""), expand on the last sentence by forming conjectures on why the values did not match the original work's values.

Here are other minor fixes that I suggest:

m1. In Section 2 Paragraph 6 (""The authors...""), cite the paper instead of adding a hyperlink.
m2. In Section 3 Paragraph 1 (""To meet...""), instead of having PyTorch website as a footnote, cite the PyTorch paper.
m3. In Section 3 Paragraph 1 (""To meet...""), please write which Colab device you used. I believe Colab has T4 for GPU.
m4. In Section 3.1 unnumbered list, change ""Fig 3"" to ""Figure 3"" for consistency with the rest of the report.
m5. In Section 3.1 unnumbered list, footnote 3 should go to the end of the sentence. Removing the footnote would also work.
m6. In Section 3.2 Paragraph 1 (""Similar to...""), please add the valid and test PPL values of the original work.",19-42.txt
12,https://openreview.net/forum?id=Ss-TvdTmS0,0,6,3,"I will use the term ""paper"" to refer to the NeurIPS accepted paper and ""report"" to refer to this report, the one that is the subject of this review.

In this report the authors reproduce the results from ""Kernel-Based Approaches for Sequence Modeling: Connections to Neural Methods."" The report is ""Replicability"" track however the authors also implemented the baselines. The authors replicate the paper experiments on document classification since ""due to the public unavailability of the LFP dataset."" There are publicly available LFP datasets (http://crcns.org/data-sets), so it is potentially possible to verify the results on a different. Regarding the ability to replicate the results of the paper, the paper authors provide no information on the source of the LFP data set, a fact which is quite concerning. The report authors do not mention an attempt to contact the paper authors in regards to this.

The report results more or less align with the paper. I think that the inclusion of new implementations of the baselines, along with the proposed methods, is quite valuable since there are many subtle design choices when using DL libraries such as PyTorch which can an affect performance. For example, we see that the LSTM simulations (this is a baseline) in report always beats the paper performance and the report performances are higher in general. The paper does not provide much information on their optimization scheme, so one would have to dig through their github code in order to fully understand the differences. These issues are especially important since the performance differences between the proposed method and the baselines is quite tight, usually less than a fraction of a percent. 

Insofar as recurrent kernel methods being competitive with neural network models, I think that the report results support the paper's claim. In regards to performance in particular the results in Table 2 of the paper, the paper has ""RKM"" methods performing best on 4/8 experiments while the report has RKM outperforming on 6/8. The tasks where RKM outperforms in the report is not particularly consistent with the paper making it unclear as to whether RKM outperforms NNet techniques generally. The report authors share my opinion in the report conclusion. 

Overall I think the report is passable and I agree with the conclusions reached by the authors. I think the presentation of the results (""Figure 4"" in the report) with old results vs. new and the baselines reproduced is very nice and should be standard for reproducibility reports using DL libraries. On the other hand there are many grammatical errors which need to be fixed. Additionally, for some methods the performance deviations  from the paper are quite large which is concerning, but without a large amount of effort it is difficult to say where the issue lies. ",19-43.txt
12,https://openreview.net/forum?id=Ss-TvdTmS0,0,5,4,"This report replicates the algorithm and experiments in the original paper. Though no discussion with the original authors was mentioned, the authors achieve comparable results as in the original paper. And the codes were released on Github.

The codes were reproduced from scratch and written in Pytorch while the original codes were written using Tensorflow.

Valuable insights were included like the use of layer normalization. But there are still some concerns that I need to mention:


 1. The problem statement contains several typos that severely affects the understanding of the algorithm itself and makes me hard to follow. For example the definition of 
ht
 seems totally different from what is stated in the original paper.

2.Though the results obtained are somehow a little deviated from the results in the original paper, no hyperparameter search or trials has been done to investigate it.

3.Only experiments on documentation classification task were reproduced and those on language modelling failed to get the results for comparison.

I think to get accepted this report needs to be polished.",19-44.txt
13,https://openreview.net/forum?id=u8AIrahIGJ,0,6,4,"+ The introduction made it clear that the report had two claims to verify, and the experiments indeed supported those two claims. 

- The linear layer in torch does support multiple input dimensions and it parallelises multiple dimensions. Therefore, the protocol 1 is not accurate and could be misleading for people who are not so familiar with torch.
- The author argued that the reason for not being able to match the performance reported in the original paper was due to the limited number of training epochs. I'd recommend the author to check the performance plot (training loss / validation loss, acc on training set or validation set) to make sure that the performance is still increasing instead of having plateaued even before the end of training reported here.
- As described by the author, there is a difference between conventional shortcut connections proposed in preact-resnet and the ones used in the paper. It would be nice to see if the difference here could lead to a significant performance gap. (I don't expect to see that, but it might be interesting to see if the conventional resnet architecture is plausible.) ",19-45.txt
13,https://openreview.net/forum?id=u8AIrahIGJ,0,4,5,"# Problem statement
The problem is clearly defined and original work is described in details.

# Code
The code from original paper is re-used for the pre-processing of the datasets. While this is debatable, what I am mostly concerned about is that the codebase is known by the replicator and therefore it seems likely that the code was not replicated purely based on the paper. The reference to original framework used (TensorFlow) and to original project structure also suggest that the replicator was familiar with the original code.

# Communication with original authors
There is no mention of communication with original authors and no comments on OpenReview.

# Hyperparameter Search
Hyperparameter were chosen based on original paper.

# Ablation Study
The replicator argues that ablation studies are not applicable because of the relative simplicity of the model. I tend to disagree, even though basic ablations like layer removal may be impossible, there is certainly other parts that can be removed or simplified. Nevertheless, this report is in the replication track so lack of ablation study is not critical.

# Discussion on results
Results reported in table 5 are analyzed to interpret the model instead of in comparison to original results. They are significantly lower than original results. The table is also difficult to compare with original results and lack standard error deviation information that is present in the original box plots.

Results in table 7 are significantly lower than original ones as outlined by the replicator. Learning curves should have been presented however to support the belief that this was simply caused by training less epochs as proposed by the replicator. If the accuracy saturated for many epochs before the 80th one, we could safely claim that there is another issue. Furthermore, the results on GrayScale are pure luck which seems abnormal after 80 epochs. Therefore, I would expect there is another issue.

# Recommendations for reproducibility
The replicator recommends to re-use existing code and share code to help reproducibility. The replicator then invite to a good usage of OpenReview to provide more information. I find the last recommendation interesting, which is more an open question that a recommendation. Should shared code be structure following good software engineering practices or should we favor template style main scripts that are self-contained and easy to share.

# Overall organization and clarity
The report is well organized. The writing is good overall beside few typos and  some informal formulations. There is no plot and this would have helped in particular to compare results of table 5 with original one of Figure 2.

# Misc
The abstract should not contain bullet points or different paragraphs.

'Eventually, the code for this work is hosted on GitHub'? The code is available. :)

Section 2
extant -> existant?
affline -> affine

Section 3.1

I doubt the cost of the gaming laptop is a useful information.

channel ls -> channels

Section 3.2

Table 5 corresponds to Figure 1 -> Figure 2",19-46.txt
13,https://openreview.net/forum?id=u8AIrahIGJ,0,4,4,"The authors summarize the motivation and contribution of the original paper well. They replicate the original model and run it on MNIST and CIFAR datasets. The results show similar observation as claimed in the original paper.

A few concerns about the paper:
1. The original paper experiments with 4 different datasets. This paper report the replicated results of two of them. It may seems that the work is incomplete.
2. In the MNIST results, many replicated results are much different from the original numbers.  For example, in ""radial dependent"" with PAR_H, the replicated result is 0.6158 while the original number is ~0.4. In ""original independent"", all PAR's are almost 1.0 while replicated results are ~0.8. I am not sure where the different come and whether it could justify that the authors have make correct implementation.
3. In the CIFAR 10 dataset, the authors claim that they don't wait the model to converge due to the lack of computation resources. It's a reasonable concern. However, it may not be fair to compare the results when the model is not fully trained. For example, the performance of one model may rise faster than others in the beginning but reach the ceiling really soon.",19-47.txt
14,https://openreview.net/forum?id=NCjGB2XnEG,0,6,4,"This paper analyzes ""Ordered Memory"" by Shen et al. 2019. They are able to reproduce the results as well as the baselines for this paper showing that it does indeed achieve better performance than the baselines. Most of their experiments are on a smaller subset of ListOps, but they also reproduce the main result on the full dataset for the Ordered Memory paper. Their experiments on the subset of the data show a smaller gap between the Shen et al. baseline and the related work which is interesting, and I think the authors should expand on some reasons for why this was the case.

Overall, this is a nice paper that is able to reproduce results, despite tuning the baselines (TreeLSTM - though I think more experiments through a grid search here could improve results. I also thought it was interesting how sensitive performance is to these parameters - it could be an affect of the small dataset and perhaps statistical significance tests should be performed as well). They also add their own baseline, treeGRU which is also unable to match the performance of Ordered Memory . Lastly, they only analyze one of the datasets included in the original paper (and another alternative dataset they propose which also shows Ordered Memory having better performance than the baselines), and perhaps if time allowed more datasets should be experimented with (Logical Inference and Stanford Sentiment) to see if results can be matched.

Other notes:
Code:
Authors use existing code for models and baselines, but do not link their own doce

Communication with original authors
Not mentioned

Hyperparameter Search
As mentioned above, they tried some hyperparameters for the most competitive baselines. However I wonder if these are all that different without statistical testing.

Ablation Study
None for Ordered Memory
 
Discussion on results
Mentioned above, but were able to reproduce performance and show improvement over reproduced baselines as well (though these were done on a subset of the data).

Recommendations for reproducibility
Didn't find any.

Overall organization and clarity
Sometimes confusing when talking about the partial dataset. Experiments with this dataset were probably done for computational efficiency reasons, but it would be nice to just use the full dataset and have any discussion on the partial one to be relegated to its own section.",19-48.txt
14,https://openreview.net/forum?id=NCjGB2XnEG,0,7,3,"1. Problem statement - whether the report clearly states and understands the problem statement of the original paper.

The authors clearly state what they tried to produce.  They try to either create baselines that can perform better or create simpler models that can perform equally as well. They found that the Ordered Memory model performs on par with the state-of-the-art models used in tree-type modelling, and performs better than simplified baselines that require fewer parameters.

2. Code: whether reproduced from scratch or re-used author repository.

All models (Ordered Memory, Latent TreeLSTM,  SPINN) were created starting from Github repository models with default parameters.

3. Communication with original authors

There is no mention of communicating with the original authors for testing reproducibility.

4. Hyperparameter Search

The paper did report much on hyperparameter search of Ordered Memory. However, it conducted hyperparameter search for latent tree LSTM model.  Latent Tree LSTM is claimed to be robust to hyperparameter changes. They experimented with different batch sizes on the accuracy of the model. Surprisingly the accuracy is sensitive to batch sizes.

5. Ablation Study

The paper investigated the data efficiency claims of Ordered Memory. With the subset of 20k examples from the originally used dataset, they found an accuracy of 61.15% after 50 epochs. The Ordered Memory model remained as the most accurate model applied to the ListOps dataset even with the much smaller dataset and baseline parameter tuning.


6. Discussion on results
The paper reproduced some baselines from Ordered Memory, validated some results and added new baselines.  Promising results of the Ordered Memory model were replicated. None of the baseline methods reached the Ordered Memory performance. The reproduction results support some of the conclusions of the paper.

7. Recommendations for reproducibility

The paper does not offer much recommendations for the original paper with one except: the original paper relied heavily on other model’s stated performance, and some of the models were not clearly defined and lacking execution details. In other words, the original paper should not trust blindly the reported results from other papers.

8. Overall organization and clarity

The paper is clearly written.",19-49.txt
14,https://openreview.net/forum?id=NCjGB2XnEG,0,5,4,"Pros:
+ replicated performance of the proposed model, OM.
+ tested several baselines from the paper
+ tested an additional baseline, treeGRU.
+ hyperparameter search for TreeLSTM.
+ tested some ablations
+ Background sections (sections 1-4) are clear and thorough.

Cons
- you appear to experiment with the baselines only on your reduced dataset, so it's hard to call this ""reproduction""
- Some spelling mistakes, grammatical errors, and unclear writing.
- In the conclusion you say that details were missing and model definitions were not clear enough; would have been useful to expand on that, and provide concrete recommendations for improving reproducibility.
- organization of experiment section could be improved. I found it hard to keep track of which models were being applied to which datasets. would have been useful to break up the experiment section into a separate subsection for each dataset/dataset-subset, and then talk about the individual models within those subsections.
- external repos were linked to, but final code was not linked to.
- For the last row of Table 1...if i am correct (judging from section 5.2), this row is obtained by training on the reduced dataset. But then it is misleading to include it in Table 1, since the caption says the results in Table 1 are from the full dataset (90k).
- would have been good to test more parameters (e.g. of the TreeLSTM), and interactions between parameters (e.g. a grid search or random search).",19-50.txt
14,https://openreview.net/forum?id=NCjGB2XnEG,0,6,4,"Thank you for your efforts. 

General comments: 
• Problem statement
The authors showed an understanding of the goal of the original paper as shown in the introduction. 

• Code:
The authors used the existing code from the original paper and the baselines the cited using Google’s Colab but haven’t provided a link to that notebook.

• Communication with original authors
Not mentioned in the report nor shown in openReview. 

• Hyperparameter Search
They tried different hyperparameters for the baselines, reported the accuracy and computed statistical significance for the results. 

• Ablation Study
They tested one of the baselines namely, Havrylov et al 2019 by removing PPO (Proximal Policy Optimization). 
 
• Discussion on results
The author managed to reproduce the results of the paper by running the available code. 

• Recommendations for reproducibility
Not provided

• Overall organization and clarity
It could be improved. 

Details: 
Strengths:
1- The report managed to replicate the results in the paper.
2- The results are provided in suitable tables in figures. 
3- proposed a new dataset and used it to compare both the model and the baselines.

Weakness:
1- Some of the results are confusing:
1.a: Section 5.1.2: The authors say that on ListOps dataset when they used more operators (i.e. made the task harder) and used a subset of 20k samples, the accuracy was 92%, which had a 30% increase than the standard task! 

1.b: Section 5.2.1: Their model failed to converge using Adam optimizer, but converged with Adadelta and SGD. This sounds strange, and no code is provided to look into that. One would expect Adam (the more recent optimizer) to converge given the default parameters. However, here it is not the case. 

2- Some of the experiments seemed unmotivated:
2.a: Section 5.2.1 Comparing the algorithms' performance by varying the optimizer. It is not really common to compare optimizers, to my knowledge. Commonly, researchers would use the most recent one as, supposedly, it is better than the previous ones. 

2.b. The same case with the batch size. Trying different batch sizes to see their performance and without testing the statistical significance of the results is not very informative for this work. 

2.c: Section 5.2.1: Testing TreeLSTM model without PPO. This seemed like a part of the proposed model. It is highly expected that the model to perform worse if a major part of the model is removed. Additionally, this setting was not used by the original authors. 

--Thank you for your efforts",19-51.txt
15,https://openreview.net/forum?id=GOkYw-kax,0,5,5,"This paper re-implements the kmeans++ baseline as employed in the original work of Ginart et. al. In addition, it also proposes to compare against other variants of k-means such as bisecting k-means, weighted k-means and GMM model. The finding of this implementation suggests their implementation performs as well as original baseline implementation in terms various evaluation metrics.

Pros:
The paper is well written containing enough explanation on related work and various implemented baselines. 
Overall, their implementation of kmeans++ also ran faster than that reported by original baseline code.

Cons:
1. The results on Botnet data do not quite match original implementation. The runtime is indeed larger when compared to original code (on other dataset it is other way around) and on other metrics it is drifted a bit. Is there a good explanation for this ? It could be possible that some form of pre-processing might be employed either in your code or in the original code.
2. Not clear what is the weighing employed for weighted k-means.
3. As pointed by the authors, they faced numerical issues on GMM and bisecting k-means for large datasets.

Considering the above weakness, I note that only results on k-means and w. k-means are valid as for now. Given that the kmeans++ implementation is simple, I feel that the overall contribution is very marginal.",19-52.txt
15,https://openreview.net/forum?id=GOkYw-kax,0,5,4,"Summary: 
It seems the authors focused on reimplement the *baseline* methods of the original paper only, instead of the proposed two methods. If I understand correctly, authors reuse the code for the proposed two methods from the original paper.

Problem statement: authors did a good job in problem statement. 

Code: 
1. authors re-implement the baseline from the original paper (Lloyd’s algorithm initialized with k-means++). Authors also implemented two additional algorithms as benchmark (bisecting k-means, weighted k-means and Gaussian mixture models with expectation maximization). 
2. In discussion authors mentioned ""The code provided by Ginart et al. was first explored ..."".  So if I understand correctly, authors ust reuse the code from the original paper, while only reimplement the baseline?

Communication with original authors:
I didn't find any communication results with the original author. As authors mentioned ""The original implementation of the baseline k-means algorithm with k-means++ initialization was very barebone and transparent, which allowed us to reproduce it with confidence"" the communication might be not that necessary, but I would encourage authors to discuss with original authors on the speedup part. 

Hyperparameter Search:

1. In discussion, authors mentioned ""the specific parameters used to generate those performance values were not reported"". 
2.  Also in discussion: ""Ginart et al. used 1000 deletion operations whereas we only used 10 deletion operations because of time constraints"" 

Is (2) because the speedup is not as drastic as the original paper? I would recommend authors to communicate with the original authors on both (1) and (2) for better reproducibility.

Ablation Study:
1. In addition to the benchmarks in the original paper, authors further provided b. k-means, w. k-means, and GMM for more comprehensive comparison. Authors also explored the possible variations of the algorithm and its effect on the metrics used to evaluate the model.
2. Authors compared num iterations as hyper parameters.

Discussion on results: the discussion and conclusion sections are quite clear.

Recommendations for reproducibility:  Authors mentioned the original paper is quite transparent and easy to reproduce.

Overall organization and clarity: Good clarity. Would be even better if the similar plot can be presented like the original paper.",19-53.txt
15,https://openreview.net/forum?id=GOkYw-kax,0,4,4,"This paper reproduces the paper ""Making AI Forget You: Data Deletion in Machine Learning"". The authors reimplement the methods in the paper, as well as a few other baselines, and rerun the experiments from the original paper, reaching generally similar conclusions.

The main contribution of this paper is reimplementing the methods and rerunning some of the experiments. I'm afraid the authors might have focused their attention on the wrong baselines. The main gist of the original paper was to explore methods to make deletion more efficient by applying principles such as quantization and modularity to the K-means algorithm. As a result, an interesting baseline would be to explore a potentially simpler way to make the k-means algorithm more efficient. In contrast, if I understand correctly (though I might be wrong as this wasn't stated explicitly), the authors explored other clustering methods, using the same baseline approach (retraining), on which the same techniques could also potentially be applied. This limits the conclusions that can be made from this paper.
I was also disappointed to see that the authors did not explore any hyperparameter except maximum iterations (see more discussion below), and did not perform any ablation study.


More comments:

1. The conclusions in 4.1.1 seems somewhat inaccurate. The reduction in loss is within 1 standard deviation in all cases, so this hyperparameter does not seem to have a significant effect.
2. Table 2 seems somewhat surprising. First, the proposed models are slower than the k-means baseline on Botnet. Second, the values in the table are sometimes (but not always) drastically different from the ones reported in the main paper (see Table 4), for instance Q-K-means runs in 0.026 on CellType in the original paper, while 0.51 in this paper. The authors hint that this could be due to the larger number or runs in the original paper (especially in the amortized setup), but further discussion and analysis would be helpful.

Writing:
- Section 1.2 could use 1-2 sentences summarizing the main results. Also, the title ""Task"" is a bit misleading.
- Section 4.2 should include text describing the experiments, results, and main takeaways, rather than only showing the tables (currently they are combined with the discussion section). 
- The sentence ""The following experiments were run in triplicates"" (4.2.2) is unclear (and missing a period).

Minor:

- No need to cite a paper multiple times in the same paragraph (e.g., [7] in the background section, [21] in section 2.1.3. 
- Please make sure to add whitespaces before citations (e.g., [21] in 2.1.3).
- Typos and such:
-- Background: ""They also proposed *a* another variant""
-- 2.1.4: ""*A* expectation step"" (should be ""An"")
-- 2.4: ""The optimization loss of the k-means objective *correspond*"" (should be *corresponds*)",19-54.txt
16,https://openreview.net/forum?id=3EGF5it-1K,0,2,4,"The report attempts to ""reproduce ... RETINA task on a similar, publicly available dataset, and offer an alternate interpretation of for these experiments."" 

""Authors"" refer to the authors of the report.

In the current form, the report has several shortcomings.

* Figure 1 compares the results of the author's implementation with the results from the Raghu et al. paper. This comparison is not meaningful as the two implementations are trained over different sizes of the dataset. No conclusions can be drawn from this. The authors could have considered reproducing the results from the ""Very Small Data Regime"" that Raghu et al. also considers.

* Section on Replicability analysis is almost nonexistent (3 lines).  The report makes abstract comments like ""We forward a several possibilities ...."" but do not explore them. The conclusions are equally vague ""transfer initialization realization quality plays a role of unknown magnitude for transfer learning tasks."" or ""usefulness of transfer initialization outside of the
presence of massive in-task-domain data sets"". The authors should consider investigating one or two aspects in detail.

* In the figures, consider plotting the mean and the standard error instead of plotting all the individual runs.

* Writing has several grammatical errors.

The report considers ResNet and VGG models. Both the models and their pre-trained weights are easily accessible. The authors should have considered doing meaningful ablation experiments that were not considered by Raghu et al. ",19-55.txt
16,https://openreview.net/forum?id=3EGF5it-1K,0,6,5,"The goal of the original authors which was to understand the validity of transfer learning methods of standard datasets to more specific context medical images is clearly reflected in this report.  The two neural networks selected in the article: ResNet and InceptionV3 are selected with justification, and a detailed analysis is performed including the CCA similarity of activation vector of the final layer which was really insightful. The analysis is clear and the mismatch between the original paper and their results is mentioned and justified to a certain extent. The conclusions made are empirically correlated with the results obtained. 
However, deeper architectures could also have experimented, to understand the impact of depth of the neural network on the transfer learning. Also, RETINA dataset and CheXpert which the report uses are too narrow, too easy to learn datasets for the task. Some more complex medical imaging datasets could have been used to derive a more informed conclusion.  ",19-56.txt
16,https://openreview.net/forum?id=3EGF5it-1K,0,5,5,"This report investigates the reproducibility of the conclusion from ""Transfusion: Understanding Transfer Learning for Medical Imaging"" (Raghu et al., 2019). The authors attempt to reproduce the finding that medical imaging tasks do not benefit from transfer learning from ImageNet as opposed to training from random initialization. Because the diabetic retinopathy dataset used by Raghu et al. is not publicly available, the authors instead perform experiments on a smaller (35k vs. 320k) image dataset, where, in contrast to Raghu et al., they find substantial benefits to transfer learning vs. training from random initialization. They also replicate Raghu et al.'s finding that representations of models trained from random initialization are more similar to each other (according to CCA) than to representations of models trained via transfer learning.

The report is clear, and the procedure as described appears to match that used by Raghu et al., at least to the extent that I can verify this. The findings do not match those of Raghu et al., which reports much smaller gains from pretraining with datasets as small as 5k, but the authors state that they do not believe their results contradict those of Raghu et al., which seems like the appropriate conclusion given the many possible origins of the discrepancy.

Based on the details provided in the text, it seems that the experiments have been conducted properly. However, the reviewer guidelines supplied to me state that the codebase should be submitted, but I cannot find any link to the code in the report or on the OpenReview portal. For this reason, I am giving this submission a rating below the acceptance threshold.

I also have some concerns regarding the experimental setup, although these are related to the lack of clarity in the description in Raghu et al. and probably necessitate communication with the authors of that work. Based on the information available to me, I am unsure that the training time and value of weight decay in this submission match what was used by Raghu et al. It is known that, even when fine-tuning and training from scratch reach similar accuracy, networks trained from scratch take longer to converge. This is shown in Raghu et al., as well as [1,2]. It may be that 50 epochs is not long enough to reach asymptotic accuracy when training from scratch; it's not clear to me how many epochs Raghu et al. train for on the RETINA dataset. Weight decay matters because it implicitly changes the learning rate schedule; in networks without weight decay, the effective learning rate decays over training as the weights grow in norm, and weight decay prevents this from happening [3]. Although Raghu et al. do not explicitly mention weight decay, they use the TensorFlow Slim framework, which adds weight decay by default.

Finally, like the authors, I am unsure how to interpret the failure to reproduce the results of Raghu et al. I believe the difficulty of drawing a conclusion makes this reproducibility report somewhat less useful as a guide for future research, although I would still support acceptance if code were available and the experiments are correctly performed.

Minor:

- The citations for the RETINA and CheXpert datasets seem to be switched in the introduction.
- It might be useful to provide the 10k and 50k numbers from Raghu et al. in section 7, for comparison purposes.
- Typos: ""We forward a several possibilities,"" ""we do no feel""

[1] He, K., Girshick, R., & Dollár, P. (2019). Rethinking imagenet pre-training. In Proceedings of the IEEE International Conference on Computer Vision (pp. 4918-4927).
[2] Kornblith, S., Shlens, J., & Le, Q. V. (2019). Do better imagenet models transfer better?. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2661-2671).
[3] van Laarhoven, T. (2017). L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350.",19-57.txt
17,https://openreview.net/forum?id=FfJZ3nfDHb,0,7,3,"Reviewer 4 notes the code has issues, AC agrees with the inconsistencies in notation in the paper, which could have been more polished. Overall good quality report, and a good contribution to the understanding of the original paper. However, noting the variation of the reviews, AC doesn't recommend to the journal.",19-58.txt
17,https://openreview.net/forum?id=FfJZ3nfDHb,0,9,4,"This reproducibility report actually considerably improves the original code. As I understood from Section 3.2, the authors have implemented their code from the scratch. It is shown (in Figure 4) that the resulting code is much more efficient than the original code. It is not mentioned whether there has been communication with the original authors. However, I think it would be a good idea to release this new code and mention about it to the original authors.

In addition to the improved efficiency and numerical stability, the new implementation has some additional features, e.g., it allows changing the parameter k (Section 3.2).

One obvious question coming to my mind when reading the original paper: how about bigger parameter dimension d? The dimensions (number of features) used in the original paper are very small (3,5,7). The report shows, that already for small increase (d=9,11,13) the results get much worse, even worse than without using the proposed boosting scheme. I think here the authors could have used even bigger values of d to underline the observation that the method is good for small values of d. I think this is not enough stressed in the original paper.

I could not find a study of the effect the hyperparameter |A| in the report, otherwise I could have considered giving even the highest grade.

I wish there were more of this kind of reproducibility reports for articles in machine learning conferences.",19-59.txt
17,https://openreview.net/forum?id=FfJZ3nfDHb,0,8,3,"In this paper, the authors reproduced part of results shown in the paper ""Fast and Accurate Least-Mean-Squares Solvers"". In particular, they run the codes implemented by the authors of the original paper, re-implemented the method by themselves again, and examined the robustness of that approach by applying it to new simulated datasets. The authors found that there are many restrictions for that approach and it is powerful only when there is high demand for numerical accuracy.  The paper is clearly written and the setting of the experiments are described in full details. I appreciate authors' effort in re-implementing the algorithms in the original paper in a more efficient way to make the code even faster. It is a great contribution to identify the cases (e.g. high accuracy is needed) where the Least-Mean-Squares Solvers proposed in the original paper is preferred to benchmarks.",19-60.txt
17,https://openreview.net/forum?id=FfJZ3nfDHb,0,9,3,"This work aimed at reproducing the results from paper ""Fast and Accurate Least-Mean-Squares Solvers"". This seems to be a good paper to check as the results are mostly numerical and it's extremely important to check if the motivation was realistic. Furthermore, the code for the published paper is openly available along with the used datasets, so there is a lot of space for the experiments.

The authors of this submission reimplemented the approach as well as took the original code and compared the results on several problems. Although they didn't manage to run the experiment on measuring the effect of regularization and not every single experiment was reproduced, the authors did an amazing job and presented their numerical findings in a clear manner.

I find the comment "" Notably, we’ve found slight increase of performance, due to cleaner code"" to be a bit strange---cleaner code is of course good for readability, but it's clear to me why it should lead to an improvement in the performance. If the authors know why exactly their code is faster, this would be a good detail to include in the paper. 

In section 5, the authors wrote that the implementation can be further improved. It'd be great if the authors at least provided their ideas of how one can do this. If there certain shortcoming of their code and that from the published paper, it's important to clarify what they are.

I have to say that the writing is not very careful and there are many typos, for instance, in the end of line 23 it should a comma rather than a period and in line 63 ""caratheodory's"" should be capitalized. I ask the authors to proofread their work and fix as many typos as they can find.",19-61.txt
17,https://openreview.net/forum?id=FfJZ3nfDHb,0,4,4,"This reproducibility report is an ablation study of the paper ""Fast and Accurate Least-Mean-Squares Solvers"" submitted by Maalouf et al. The original work proposes a novel method for solving least mean squares problems by utilizing Caratheodory’s Theorem, which states that every point contained in the convex hull of n points in R^d can be represented as a convex combination of a subset of at most d+1 points.

PROBLEM STATEMENT
The original paper considers regularized least squares problems. However, the authors of this report only define least squares problems without regularization. Furthermore, I see some problems with the notation used for this definition: first, the target values from the dataset are denoted by y, which is already used for the outputs of the linear model. Second, the summation used in to express the outputs of the linear model makes no sense. The values do not even depend on the summation index m. Also, the model parameters are denoted by W (upper case) and w (lower case). Moreover, the variable n is nowhere defined. The mathematical writing in this paper is also flawed at other places: See section overall organization and clarity.

CODE
The code is very clean and nicely documented. I was able to run the original implementation without major problems. The reimplementation worked after I removed line 133 from coreset.py. Nonetheless, their implementation failed from time to time due to assertion error. It would have been nice to have some error handling at that point. It would also have been nice to deliver code to reproduce the plots.

COMMUNICATION WITH ORIGINAL AUTHORS
As far as I can see there was no communication with the authors of the original paper, neither through OpenReview nor through GitHub.

HYPERPARAMETER SEARCH
The authors performed new experiments by tweaking the hyperparameter k, which was set fixed in the original work. These experiments show that the choice of k in the original work was good.

ABLATION STUDY
The authors of this report rerun the experiments on the three datasets that were also used in the original work. They also run the experiments on synthetic data. However, the process of data generation is not very well described. The authors reimplemented the method and claimed that their implementation is more efficient.

DISCUSSION OF THE RESULTS
The authors compare their implementation with the original implementation on different hardware setups, which is problematic (see Fig. 4).

RECOMMENDATIONS FOR REPRODUCIBILITY
No recommendations.

OVERALL ORGANIZATION AND CLARITY
The structure of the report is appropriate, but I have got big concerns regarding the notation of the paper. Sometime X is an m-by-n matrix, sometimes it is an d-by-n matrix. This can be confusing for the reader. Furthermore the denotation of optimization problems, solver to solve those problems and machine learning algorithms is mixed up. The authors should be more clear with respect to this.

CONCLUSION
My main concern is the different hardware used for comparing the performance. Also, the quality of the writing is rather low.

FURTHER REMARKS AND MINOR CORRECTIONS
- There is a missing white space in the first line of the abstract: Least-mean squares(LMS) 
- Missing period in line 171 (Reference should also be in brackets.)
- A (line 140) is used to denote the input data matrix. I think at that point a blackboard bold A should be used.
- Abstract: ""becomes impossible due to some reasons"" is very vague",19-62.txt
18,https://openreview.net/forum?id=H27oOePp_K,0,5,4,"I assume this reports goes into the ""Replication Track"". The author of this reports do indeed replicate the exact results from the original paper (of which there where many), including evaluation metrics, colour and form of the charts.
However, the guideline talks about doing this replication ""from scratch"". The author of this report seem to have based themselves heavily on the github: in this case however this consist of a few bash scripts. The description on the replication author's github is more detailed than the original, and importantly also contains the code used to obtain the plots.
All in all I would expect their github to become the reference repository of this paper.

Besides that, the paper does not shed any more insights in the original paper beyond confirming that the experiments hold.  ",19-63.txt
18,https://openreview.net/forum?id=H27oOePp_K,0,6,5,"1. The specific problem tackled by the paper:

The authors aim at the reproduction of the paper  'Are Sixteen Heads Really Better than One?' which shows surprising results that most of the heads in the Transformer (BERT) can be removed, and the performance only drops a little.

2. Claims of the paper:

The authors follow to use pre-trained BERT and Transformers in NMT. They reproduce several experiments on BERT finetuning tasks, including MNLI, Cola, and SST-2, as well as two translation tasks, WMT2014 En-Fr task, and IWSLT De-En task. All the experiments show supportive results, which are quite consistent with the claim of the original paper.

3. Decision (accept or reject) with one or two key reasons for this choice and reasons for the decision.

Weak accept.

Detailed comments

Pros:

1. The authors provide very detailed statistics of removing different head at different layers for various tasks. It is great to see that the authors conduct significance tests to the empirical results.   

2. The authors give a good background introduction to the multi-head attention mechanism, make the manuscript to freshmen of the topic.

Cons:

1. WMT and IWSLT are names of datasets, and BERT is generally considered as a particular model. Therefore, they are not the same concept. It is not common to say, ""The models used throughout the original paper and our reproduction are BERT, WMT, and IWSLT"". The WMT model you refer to is called the Transformer-big model, and you can call the model you used in IWSLT the Transformer-small model. This should be corrected.

2. There are missing experimental settings about the 'finetuning' which is super crucial in evaluating BERT. Do you use hyperparameter search like BERT? How do you set your search space? How many random seeds do you use, and what is the standard deviation of the results?",19-64.txt
18,https://openreview.net/forum?id=H27oOePp_K,0,6,4,"The manuscript reproduces the main ablation studies in the paper ""Are Sixteen Heads Really Better than One?"". In addition, the authors also extended the study to new datasets and to models not fully investigated in the original paper. In particular, they performed one head ablation study on additional datasets CoLa and SST-2, and presented additional per-head ablation results for BERT and for the encoder-decoder and decoder-decoder heads in WMT. They tested WMT model in machine translation task using newstest2014 instead of newstest2013 (original paper). They also studied the effect of pruning heads on IWSLT instead of WMT (original paper). The additional datasets and models produced overall consistent results to those in the original paper. IWSLT is found to be more sensitive to pruning compared with the original paper's results.

The manuscript is clearly structured and well written. The relevant concepts from the original paper are well explained. The main ablation studies from the original paper have been either rigorously reproduced or supplemented by similar studies on different datasets or related models. The ablation study results are well interpreted in comparison with those in the original paper.

My main concern about the manuscript is the scope of the ablation studies. The ablations are restricted to those proposed in the original paper. No additional components or hyper-parameters are investigated. It would also be interesting to investigate the effect of pruning heads in different transformer-based models, or under different NLP tasks.

Itemized comments:
- In Fig 2(a), the BLEU scores on MTNT are much smaller than those in the original paper. BLEU scores of ~10 may indicate poor model performance. It would be helpful to discuss the potential reasons.
- In Section 7, data preprocessing is identified as a potential reason for the different results using WMT and IWSLT. It would be helpful to provide some details on the preprocessing steps employed by the authors.",19-65.txt
18,https://openreview.net/forum?id=H27oOePp_K,0,5,4,"This paper aims at reproducing the results of the paper ""Are Sixteen Heads Really Better than One?"" by Michel et al. (2019). The authors understand the problem stated in the paper (studying the importance of having multiple heads in multi-head attention) and produce rigorous reproduction of the results in the original paper, by using the existing code with small modifications. The results of this paper seem to support the conclusions of the original paper, and the conclusions provide some recommendations for reproducibility. 

My main concern with this paper is that the authors submitted this paper to the ablation track, but did not provide any ablation, apart from re-running some of the experiments on additional datasets. In particular, no hyperparameter search was reported by the authors, or any other modification to the experimental setup except the new datasets. I think this paper would be a better fit to the baselines track. 


Other comments: 

1. It would be helpful for the introduction to provide more details about the results of the paper. Currently it's just two general sentences.
2. Copying sentence verbatim from the original paper (section 2) is discouraged.
3. The experiment training on MNLI and testing on CoLA and SST2 is minimally discussed. I am not sure what is the motivation behind it, given that the tasks are quite different, and even the output space is different (MNLI is an NLI dataset with entail, contradict and neutral labels, and for instance, SST2 has two labels: positive and negative).
4. In contrast to the authors' interpretation (section 4), the results on Figure 3(a) do look quite different from the original results. In particular, pruning 40% of the heads does lead to significant decrease in performance, unlike the results reported in the original paper.",19-66.txt
19,https://openreview.net/forum?id=lFfIA47iRR,0,3,3,"Problem Statement: The authors provided problem statements and introduction to the original paper.  

 
Hyperparameter Search & Ablation Study:  The authors did not conduct detailed hyperparameter search with model hyperparameters 
 

Discussion on results & Recommendations for reproducibility: The authors provided discussions on the reproduced results but seems not quite enough.


Overall organization and clarity: The paper is not well-written and contains many typos even in the title. Figures are not clear enough (specially figure 1)

 ",19-67.txt
19,https://openreview.net/forum?id=lFfIA47iRR,0,4,4,"While the authors seem to have a working implementation of the results, there is not enough reflection on the results themselves, or thoughts about how the AMR authors could improve replicability/clarity, or comparison to the original quantitative results, for the document to provide much aid to understanding of how well/how easily the original paper reproduces. The first part of the paper, that explains the problem formulation/method of AMR is fairly well-written, but the part describing their experiments/results isn't well-described or motivated -- it should mention that these datasets were used in AMR, should make clear which experiments from the original paper they are trying to reproduce, should compare explicitly how AMR performs relative to reported results, should also mention challenges in reproducing / things that weren't initially clear / etc.

One clear aspect for improvement: The paper should compare the relevant table from the original paper to theirs, so a reader can judge whether the results match; as written, it is hard to get a sense for how robust the original AMR paper's results are. Although the authors state that they achieved comparable performance, Table 1 from the arXiv version of the AMR paper seems to have significantly different results from table one of the reproducibility report for e.g. KMNIST (65% replicate vs 81% original w/ Bernoulli). But I had to compare the tables manually (and perhaps my comparison is not the relevant one?). ",19-68.txt
19,https://openreview.net/forum?id=lFfIA47iRR,0,7,3,"This paper aims to reimplement the experiments of the data augmentation technique mixup. The work is simple and straightforward. There should write code to achieve a comparable performance with the original paper. Instead of mixup in pixel space, this paper involves different ways of combing latent represnetations. 

Strengths:
+ Reproduce the performance of the original paper.
+ Experiments are conducted on three datasets.
+ Source code is publically available.

Weakness:
- Missing citation. For example, relevant works of three datasets are missing.
- The details of normalization are not presented.
- Although authors describe the descent strategy of learning rate, how many iterations to decrease the learning rate is unknown.
- In figure1, Equation 6 is not specified. ",19-69.txt
19,https://openreview.net/forum?id=lFfIA47iRR,0,3,4,"Pros:
The authors have re-implemented the method from the original paper. The paper is clearly written.

Cons:
1. As this is a paper submitted to the Replication Track, it is expected to perform exact replication of the presented main results of the original paper. However, the datasets used in this paper are all different from those in the original paper. The presented results may not be a good evidence of the replicability of the original paper.

2. It seems that the authors don't have many direct communications with the original authors.

3. Docs are not provided in the codebase.

Overall, the paper needs more efforts to be accepted.",19-70.txt
20,https://openreview.net/forum?id=Nrdu2F6jhF,0,4,4,"The authors perform an ablation study on the FreeAnchor paper which proposes a more flexible approach for matching anchors to objects in an image. The authors reproduce the performance of two ResNet variants, perform ablation experiments on the Saturated Linear function and the Mean-Max function and conclude with a hyperparameter tuning.

The overall presentation of the paper should be improved. Some sections seem to be simply copy-pasted from the original paper (including parts the abstract itself) and sometimes forget to include important details explaining the notation (i.e. what is P{a_j \in A_{-} } ?). 

The baseline results are conclusive and show that the results are reproducible. However, no explanation is given as to why the training time takes longer? Did the original paper use a different setup?
The ablation experiments in general do not seem well-motivated. Changing the linear function in the Saturated Linear function to a sigmoid function obviously will not change the ranking of the anchors, so it not clear what was meant by this experiment. 
The hyperparameter tuning has a good number of experiments, but little intuition is gained from these experiments. The conclusion mentions ""we gained a deeper understanding of the network’s learning process"", this deeper understanding is precisely what should be included in the paper.  Indeed, the goal of the ablation track is not simply to give a fail/pass to the paper, but to report insights gained from the reproduction of the results.",19-71.txt
20,https://openreview.net/forum?id=Nrdu2F6jhF,0,3,4,"This paper presents an evaluation of FreeAnchor, a NeurIPS'19 paper on object detection learning anchor assignment during training instead of a hardcoded method based on IoU between anchor boxes and ground truth bounding boxes.

The authors verified that the original method performed as claimed in the original method using the authors' code - which is interesting.

The paper also presents 2 experiments:
- 1 experiment that replaces the ""saturated linear function"" used in the original method by a sigmoid in one term of the loss function. The authors observe a small increase of accuracy at the cost of a slower convergence. Given that the saturated linear function is a piecewise linear approximation of a sigmoid, this makes sense but is not very surprising.

- 1 experiment that replaces the mean-max function by a simple max in one of the terms of the loss function. The original motivation for the mean-max was to have a relaxed version of the max to guide convergence. The authors observe that this replacement results in faster convergence, to a poorer model. Again, this result makes sense but is not very convincing.

The experimental results presented in this paper thus do not tell us much about the original method, beside the fact it works as claimed in the original paper. In addition, I found the paper difficult to read, with many small language mistakes. The notations are also sometimes problematic (see ""matrix C in {0,1}"" for example, or the fact that A is not introduced). Parts of some sentences are copied from the original paper (eg ""breaks the IoU restriction"").",19-72.txt
20,https://openreview.net/forum?id=Nrdu2F6jhF,0,7,4,"The authors reported successfully re-produced the results. The report is clear and overall high quality.

Pros:
The report organization and clarity are overall pretty good. The report starts with a summary of the core techniques of the original paper. Then it presents the results of re-producing the main results presented in the original paper. Baseline results are re-produced and match the original paper well. The hyperparameters of anchor bag size, IoU threshold, beta, and gamma are swept and verified and seem to match the results presented in the paper. Beyond the existing saturated linear function, the author tested sigmoid function. The author also provided valuable discussion and suggested possible extensions, which is welcomed.

Cons:
The third column of Table VI should be beta instead of alpha? Also, the authors are encouraged to use TeX formulas such as 
γ
 to match descriptions in the original paper.
A minor point: I'm not sure whether the author should use NeurIPS template instead of their own template. Also in openreview console, maybe the author should use their own name instead of the authors of the original paper?",19-73.txt
21,https://openreview.net/forum?id=zllZMNuMuL,0,5,3,"This paper based mostly upon the paper of ""Differentially Private Bayesian Linear Regression"" and the originally code provided by the author of the NIP's paper. Utility functions (e.g., visualization of posterior distributions, method runtimes) and metrics (e.g., KS statistics and MMD) were implemented by this paper in order to re-produce experimental results of the original paper. 

Little original ideas were presented in the paper other than re-producing experimental results of the original paper. In the conclusion, authors mentioned the potential extension of this work to Bayesian logistic regression but does not include any description or experiments. 

This paper summarized the original paper in the Sessions I and II, and the description aligns well with the original paper.  
While authors intend to re-produce all experimental results, not all curves match the original paper. In the page 6, left column, ""besides from the case of the parameter \sigma^2, our plots do not seem show the same ..."", could you please explain why? Also, explanation on the differences with Figure 3 in the original paper is lacking.

Meanwhile, in the page 6, authors mentioned ""We notice that, besides the cases of \sigma^2, the precision of non-naive methods do not necessarily improve as N increases"", discussions on this result is lacking. 

Overall, this paper did not fully reproduce the original paper and explanation on the result differences wasn't discussed. Given little new idea was presented, I believe this paper is below acceptance threshold.


 ",19-74.txt
21,https://openreview.net/forum?id=zllZMNuMuL,0,8,4,"The authors have reused code repository in the original paper while at the same time have coded add-ons for a better interpretation of figures given in the original paper. This reproducibility report contains detailed discussions on how to reproduce the original paper and is well written. 

 The reviewer could not find any communication between current authors and that of the original paper on OpenReview or any mention in the paper itself.

The authors have taken the time to reproduce and add to the existing code, snippets that run and generate synthetic data as well as posterior distributions. These are essential to understanding the original paper and the relevant code was missing in the original repository. The authors also have provided improvements to the noisy sufficient stats to further strengthen the ablation studies.

In the results, the authors do recommend that the Naive method is a good candidate irrespective of sample size.

The paper is well structured. 
> There is a mixup between references 1 and 18. Please tidy this up in Section 6 and 4. 
> In Fig 13, where is the curve related to the naive implementation (the legend mentions this though). Similarly for Fig 12, the curve for Gibbs-SS-Noisy is missing. ",19-75.txt
21,https://openreview.net/forum?id=zllZMNuMuL,0,6,1,"I am not an expert in differential privacy, and have limited ability in judging both the original work and the submission.

That said, it looks to me that the submission is an interesting ablation study to the original paper, further demonstrated and verified the original claims. The submission also went further with the experiments to show more properties of the idea, and also discusses some next directions for research.

The main concern is that the ablation study is only performed on synthetic data, which is actually pretty simple. This is compounded by the fact that the experiments in the original paper are already very simple, even considering the original experiments on real data. As a result, it is hardly a challenge to reproduce the results. This lessened the significance of the submission.

Based on these considerations as a non-expert, I will recommend a rating of 6 for the submission.",19-76.txt
21,https://openreview.net/forum?id=zllZMNuMuL,0,5,4,"Summary 

Much of the text in the Introduction section and elsewhere is almost identical to the original paper by Bernstein and Sheldon. In particular, the first two paragraphs use almost word for word sentences from Bernstein and Sheldon. However, this review focuses on the scientific contribution of the manuscript.

The authors state clearly the problem statement of the original paper. The authors list six major contributions in their manuscript that are related to reproducing the results of the original paper. The authors explain that there are several tasks that needed to be carried out for reproducibility that did not have the code provided by the original paper authors. For these instances, the authors write their own code. For example, for carrying out the calculation of “Maximum Mean Discrepancy”, the authors state that they searched in the GitHub repository associated with the original paper and in publicly available libraries, but were not able to find the appropriate code. Instead, they decided to implement their own code for the reproduction calculation of “Maximum Mean Discrepancy”. Similarly, the authors implemented a different version of the experiment based on the “Kolmogorov-Smirnov (KS) statistic” after failing to find the appropriate code in the GitHub repository associated with the original paper. The authors do not mention anywhere in the paper that they attempted to contact the original authors. I also did not see that the authors communicated with the original NeurIPS paper authors through 
OpenReview.

By using their own code, the authors were able to create new renditions of some of the plots of the original paper. The authors also reached some of the same conclusions in original paper despite using different code.

The ablation studies were comprehensive and are supported by appropriate motivation. In particular, one goal of the authors in the ablation studies that was particularly comprehensive was to isolate and improve the creation of simulated data. In the ablation studies, the authors provided new plots that were not part of the original paper. The authors were able to confirm the results of the original authors regarding which noise-naïve method generated the correct posterior asymptotically.

There were a few minor instances of grammatical errors. One example among several is on page 2, second column, approximately halfway down the page “We then more principled noise-aware inference…”.  It would be helpful if the authors reworded this sentence.


Other major comments:

1) Page 2, first column, fourth line from end: “IG refers to the inverse Gaussian”. This should be “IG refers to the Inverse Gamma”.

2) The authors cite paper [18] throughout, but this should be [1]. For example, this occurs on page 6 several times and on page 8. ",19-77.txt
22,https://openreview.net/forum?id=jUJo2RYTOb,0,4,3,"The report first describes the original article's proposed architecture and related work, as well as the datasets and evaluation set up.
Then they describe the different ablation studies they propose, and report the results at the same time.
Here are the comparisons they performed:
- CNN² vs. Vanilla CNN, with and without extra convolutional layers
- Changing the number of pooling scales, and batch size.
- Reversing the order of CM pooling and convolution in layers
- Using max pooling instead of CM pooling
- Changing the number of CM blocks, the augmentations performed, and their size.

The set of variants identified by the authors make sense, and are a good fit for an ablation study. They comprise:
- some changes reported in the original article,
- some that are alluded to in the original article, but without precise numbers
- some changes that expand the range of architectures (and other hyper-parameters) explored.

The main point of the original article was to improve generalization of object recognition to new angles, or viewpoints that were never seen before. This is why only images with a limited range of view angles were used for training (and a different range for validation), while test examples were unlimited. Moreover, the performance for each viewpoint (including seen and unseen) were presented as curves in Figure 6 (and 2, 3, 4 of the supplementary material), visualizing the difference between ""seen"" and ""unseen"" viewpoints.
Unfortunately, the authors of this report do not have these graphs for the reproduced (or new) experiments, only one final number.

These numbers are also inconsistent with what is reported in the original article, without it being discussed in the report at all. For instance:
- The original article reported Vanilla CNN performance of 90.7% on ModelNet2D, and 72.2% on SmallNORB. This reports has respectively 99.66% and 88.25%.
- The original article reported CNN² performance of 94.1% on ModelNet2D, and 86.5% on SmallNORB. This reports has respectively 99.57% and 89.50%.

This has me wondering whether the training, validation, and testing splits were done in a significantly different way. If the data provided for download by the original authors is significantly different from what was used for the original paper, or if the evaluation procedure in the current code is not the same as what was used in the original paper, this should have been discussed as a source of irreproducibility.
However, the authors only mention how some original claims do not hold up (reverse pooling, for instance), when the numbers are sometimes really close (and do not have confidence intervals), and are different from the reported ones in the original article.

If the splitting procedure was in fact different, and images of all viewing angles actually ended up in the training split, this would explain why the ModelNet2D performance got > 99% from about 94% at most. However, this means that the original authors claims (regarding the pooling order, for instance) may still hold in the context of evaluating on unseen viewpoints.

I appreciate that the pre-processed RGB-D data is in fact not present in the data file provided by the original authors, and that the original code may have been incomplete, or misleading.
The design of the ablation experiments makes sense, and they have been carried out and reported.
However, with the lack of comparability with the original set up, and the fact that it was not discussed whatsoever, I do not think this report clears the bar of a reproducibility or ablation study.",19-78.txt
22,https://openreview.net/forum?id=jUJo2RYTOb,0,7,4,"This paper presents the results of experiments that evaluate different values of the method's hyperparameters and the influence of different options (starting from the original authors' code), rather than an ablation study per say. I do not think this is a problem, as the original method did not really have parts that could be ""ablated"", and the results are interesting.

I think the paper draws several interesting conclusions, including:
- mainly that the performance of the original method (called CNN2), compared to a vanilla CNN is not that clear, even on the same datasets used in the original paper, as CNN2 is slightly better or slightly worse than Vanilla CNN, depending on the dataset used for evaluation. 

- switching pooling and convolutional layers, as was done in the original CNN2, actually degrades performance, contrary to what the original paper claims.

I think these results are very important as they show that having a specific architecture is not necessarily better than a standard one (at least for this problem), contrary to the original claims. 

The paper is also well written, and makes some interesting comments on the datasets and the motivation of the original approach.

An important suggestion. The abstract says:  ""CNN2 gives results close to the original CNN model"". If I understand correctly, the authors want to say that CNN2 does not perform as well as claimed in the original paper, but I do not think this sentence is clear enough.  Maybe say something like: ""We observed that CNN2 is not clearly superior to a vanilla CNN, by contrast with what is claimed in the original paper""

""While for SmallNORB"" -> this is not a proper sentence.",19-79.txt
22,https://openreview.net/forum?id=jUJo2RYTOb,0,4,4,"The authors present an ablation study of the paper , “CNN2 : Viewpoint Generalization via a Binocular Vision”. They reused 2 of the 3 original datasets (the third one being not available) and compare the CNN2 method with a vanilla CNN models.  The study consists in changing some hyperparameters and changing several aspects of the architecture of the network. Each time, they explain whether the conclusions are the same as in the original paper.  Sometimes it is the case, sometimes it is not. The paper challenges the original paper by explaining that the CNN2 model performs better only on one of the two datasets. 

The paper suffers from the following limitations: 

Communication with the author: it is unclear whether any communication was done with the original authors.  No mention is made of communications with them. 

Code used: even though an ablation study is supposed to use the code of the original paper, it is unclear whether it is the case or not. The authors do not mention which code they used.  No link is provided.  Also, there is no mention of which software and hardware setup was used in the experiments. 

RGB-D dataset: the authors claim that the dataset was not released by the original authors. However, the original authors go in great length into explaining how the dataset was built from the public dataset, which can be obtained here: http://rgbd-dataset.cs.washington.edu/dataset. The original authors explain that they restricted their approach to the following classes “camera,” “flashlight,” “lightbulb,” “pitcher,” and “stapler”. The classes are clearly defined in the public dataset. It was therefore possible for the authors of the ablation study to work with this dataset. 

Precision results: this paper and the original papers do not show the same precision values for the vanilla CNN.  The original paper reaches 90.7% on ModelNet2D and 72.2% on SmallNORB, while this paper reaches 99.6% and 88.25% resp. This different is significant. We would expect these figures to be the same, since they should be using the same code on the same dataset. 

Criticism about the modelization of the human system in Section 5 seems irrelevant.  The original authors mention in their abstract that “[their method] resembles the process of an object being viewed from the left eye and the right eye”. This claim seems fair. This paper should focus on the ablation study and not on the relevance of the claims of the original paper regarding the modelization of the human system. 

As a conclusion, despite the interesting technical work conducted by the authors, too many questions remain open to recommend for publication.  On the other hand, I am willing to hear the feedback of the authors regarding the limitations mentioned above in a rebuttal (or in a future submission). ",19-80.txt
23,https://openreview.net/forum?id=BMbPxn4a-A,0,6,4,"This paper empirically analyzes and aims to reproduce the results of ""Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence"" by He et al. The main claim of of (He et al. 2019) is that the ratio between the batch size to the learning rate must be controlled in order to ensure generalizability of deep learning models. They show that the generalization ability of deep neural networks has a negative correlation with the ratio of batch size to learning rate.

This paper attempts to reproduce the results. Though they do have some limits on computational power and cannot train the 1,600 models from the original paper (which is very understandable), they do make an approximation and show that the paper can mostly be reproduced, even without code being released from the original authors.

This paper analyzes the same 4 networks (VGG-19, ResNet-50, Xception and a custom CNN). They approximate the experiments by doing 10 experiments varying the learning rate and 10 experiments varying the batch size.

I found the paper to be technically sound and while they were not able to completely reproduce the results of He et al., they make some interesting observations and propose followup work that should be explored in this area. It would be interesting to see if reproduction of the results is possible with more data points (i.e. the full 400 instead of 20 as was done in this paper). I'd like to see the authors here communicate with the original authors to try and determine what possible issues could have happened that prevented full reproduction as there could be some minor change that could help.

The paper is well-written and mostly clear, though I think the presentation could be a bit better. Id like to see more informative captions in the tables so readers can tell what they are showing without much effort and also a big table at the end summarizing the results of all their experiments and how they align with the claims made from He et al. I believe this would improve the readability of the paper significantly.

Smaller nits:
1) This text might be confusing for readers without more explanation: ""They have done
so by proving a PAC-Bayes generalization bound for neural networks trained by SGD, which has a
positive correlation with the ratio of batch size to learning rate. Furthermore, they showed that the
generalization ability of deep neural networks has a negative correlation with the ratio of batch size to learning rate.
2) Typo in conclusion: ""However, the empirical evidences are not strong enough to conclude taht it can be generalized to other models trained on SGD.""",19-81.txt
23,https://openreview.net/forum?id=BMbPxn4a-A,0,7,4,"This report reproduces the experiments in the paper, ""Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence"". The neural network architectures and (learning rate, batch size) configurations that the authors use in this report are similar to the original paper. The authors in this report arrives at the same conclusion as in the original paper, i.e., controlling the ratio of batch size to learning rate not too large to achieve a good generalization ability.

I want to post one question here, for both the authors of this report and the original paper. In these experiments, what is the role of batch normalization? I believe with batch normalization, the actual algorithm that is implemented is different from standard mini-batch SGD. To see this, let's say we sample two disjoint batches of data, batch_1 and batch_2, both with size B. Assume that there is a 1/B factor in the gradient (the gradient is averaged by a factor of batch size). Let's compute:
gradient(batch_1), gradient(batch_2), gradient(batch_1 union batch_2).
Then, if we are using standard mini-batch SGD, by definition, we will have
gradient(batch_1 union batch_2) = 1/2 (gradient(batch_1) + gradient(batch_2)).
However, I am pretty sure this is not true if there exists batch normalization in the neural network architecture.

Therefore, I think if we really want to rigorously study the effect of learning rate and batch size on SGD algorithm, I think we should probably remove batch normalization. In this report and the original paper, I did not see the authors mention this point. If possible, I would like to see more careful experiments in future studies.",19-82.txt
23,https://openreview.net/forum?id=BMbPxn4a-A,0,5,5,"This is a replicability study of a NeurIPS on importance of the ratio of LR to BS for generalization. 

I am willing to increase my score conditioned on clarifying conclusions and adding additional plots. It seems to me that the correct way to interpret the experimental data is that as long the model didn't underfit, there is a linear relationship between generalization and LR to BS ratio. If the models did not underfit, then it should be reported. In either case conclusions are not clear.

Pros:
* The report is very clear and all results are presented well. 

Cons:
* To evaluate generalization, it is key to ensure that models are well trained. It seems to me that authors failed to converge models for the range of hyperparameters that authors of the original paper
used. However, no communication is mentioned. 
* There is no data on training accuracies reached. Regardless of the conclusions, it is important to report training curves. This could be added to the Appendix
* Code is missing 

Minor:

* https://arxiv.org/abs/1711.04623 could be also cited as a related work (concurrent to Smith et al) on the importance of learning rate to batch size ratio for SGD.",19-83.txt
23,https://openreview.net/forum?id=BMbPxn4a-A,0,3,3,"The authors do a ""line search"" over 10 different batch-sizes and learning rates on 1 of the 2 architectures in the original paper, as well as a smaller ResNet, and 2 other CNN architectures.
This is a total of 160 experiments (20 per architecture for each of CIFAR-10/100), in contrast to the grid search of 1600 total experiments in the original paper. 
This seems fine, since running 1600 experiments may be wasteful and/or prohibitively costly.

They fail to replicate the results of the original paper, and I didn't find evidence that they attempted to contact its authors. 
The original paper does not specify their architectural choices and is sparse on details in general, so there are probably critical differences in the architectures leading to the failure to replicate.


Major concerns:
1) The authors badly mischaracterize their results in the abstract:
""Throughour result, we arrive at the same conclusion as He et al, demonstrating a positiverelationship between the learning rate and test accuracy and a negative correlationbetween the batch size and the generalizability of neural networks. Finally, theprevious conclusions prove that there exist a negative correlation between the ratioof batch size to learning rate and the test accuracy. ""
In contrast, section 4.2 states: ""Hence, we are unable to reproduce any of He et al.’s three conclusionswith ResNet-50 on the CIFAR-10 dataset. ""
The conclusion gives a more accurate summary of the results:
""We were able to partially reproduce the correlation found by He et al. with both the VGG-19 and ourcustom CNN architectures. However, we found that there was a lower-bound for which He et al.’sthird conclusion (negative correlation between generalizability and ratio of batch size to learning rate)holds for different architectures. If the ratio is too small, the model becomes unable to train effectivelygiven the amount of epochs (200) and fails to converge. In fact, all models failed to demonstrate thatthere exists a positive correlation between the learning rate and the test accuracy. ""
This seems like a critical oversight but also one that should be easy enough to correct.  I will gladly increase my score if this is fixed.

Minor concerns and comments:
1) ""The best accuracy on thetest set for each one of them is collected for analysis""
- Early stopping on the test set requires justification, since it would traditionally usually be cause for concern about overfitting.

2) ""Furthermore, there is a weak positive correlation between the learning rate and test accuracy, asdemonstrated by the SCC value (0.115) in Table 2 when the batch size is fixed at 64.""
- Are these results directly comparable to the original paper?  Or do they do this regression over all 20 batch sizes?  (I looked at the original paper, and they do the same thing as here, but this should be clarified here.)

3) I appreciate the contributions statement, but it is not very detailed, and it doesn't say who wrote the paper.

4) There is an issue with the current citation style (citations appearing after periods) that should be fixed.

5) The discussion could be significantly improved.  Along the lines of my major concern above, the take-aways from this replication study would seem to be that the original papers' results appear highly specific to their experimental settings.  This deserves more discussion.


Overall, the writing could be improved somewhat, and I recommend the authors spend more effort on clearly conveying their results (especially in the abstract and introduction), and proposing take-aways and next steps.  What have we learned from these experiments?  ",19-84.txt
24,https://openreview.net/forum?id=dFaZe0m48s,0,3,3,"This paper tries to reproduce the NuerIPS paper ""Glyce: Glyph-vectors for Chinese Character
Representations,"" by Yuxian Meng et al.

The original authors showed how the character image representation benefits a number of natural language processing tasks, using several different font types of Chinese characters.  The tasks they have attacked covered not only sequential tagging like segmentation, POS tagging and Named Entity Recognition (NER) but also sentence and sentence pair classifications.

The current paper only tries to reproduce a part of the original tasks, that are Chinese word segmentation, POS tagging and NER with only limited test data.  The reproduction is incomplete because of their limited computational resources.   Although they test the original proposal in several different setting changing the number of font types and num_font treatments, all the computations look incomplete.  Moreover, the different settings are not clearly specified.  For example, they do not explain what font sets are used in each of the different settings.  There is no explanation about the difference of ""graphed"" and ""not graphed"" settings.  The discussion about the their experimental results is also not enough.

They did not perform any experiments on sentence classification, while they reported an additional experiment using Japanese character fonts.  However, they could not achieve any positive results in this experiment.  It is not quite understandable what insights the additional experiment put on the original proposal.

Overall, the presented reproduction experiments are incomplete and not enough.  It is at a very premature stage for checking the reproducibility of the original work.",19-85.txt
24,https://openreview.net/forum?id=dFaZe0m48s,0,3,5,"The manuscript presents an ablation study on the Glyce embeddings using historical scripts and Tianzige-CNN for Chinese characters. The authors proposed reasonable hyper-parameter search, and an interesting study on Japanese characters as a generalization of the methodology to a new logographic language. However, the authors failed to obtain meaningful results for a majority of the research questions due to limitations of computational resource and time constraints.

Itemized comments:
- Please explain the hyper-parameters (e.g., num_font_concat, glyph_ratio) to be searched and define model settings (graphed vs not graphed).
- From Tables 1-3, it's unclear how 'Chanel' and 'Concat' affects the model performance (as mentioned by the authors).
- In Tables 1-2, BERT performs best among all the experimented models. Figures 5-7 shows that BERT is able to reach top performance within by far the smallest number of training steps. This seems to be a strong advantage of BERT.
- In the CWS task, the authors conjectured that the complex models start slow but are able to ultimately reach a higher results than the simpler models. However, the conjecture is not supported by any empirical evidence.
- In the NER task, the authors conjectured that the result of Glyce-BERT in the original paper was not replicated due to the experiment not running long enough. However, subsequent experiment contradicted the conjecture. The failure to replicate the results is still not understood.
- The POS tagging task in Japanese produced unreasonably low precision, recall, and F1 score. Segmentation and no fine-tuning doesn't seem to explain this. Moreover, both BERT and Glyce-BERT were used to perform the task, but only one set of results were reported.

Overall, many components of the proposed ablation study in the manuscript are incomplete, with the research questions unanswered.",19-86.txt
24,https://openreview.net/forum?id=dFaZe0m48s,0,4,3,"Summary and problem statement:

The authors based their replication study on the work from Meng et al, “Glyph-vectors for chinese character representations. The paper reviewed focuses on performing ablation studies on two (out of three) CNN-based innovations from Meng et al. to obtain better Chinese word embeddings in NLP tasks: (1) using historical scripts to enrich the pictographic evidence in characters; and (2) using CNN structures tailored to Chinese character image processing. Although somewhat limited and vague (without too much detail for those who are not knowledgeable about this matter), the “innovations” addressed are correctly described in the paper, including the (different) architecture settings used. However, tasks and data should have been better introduced. On the other hand, it is remarkable that the authors also try to generalise their findings to a different logographic language (e.g., Japanese). Finally, what can be extracted from the study presented is somehow limited due to both the computational constraints and the restricted set of hyperparams analysed. 


Ablation analyses and hyperparameter search:

Although some computational limitations (as already said) which make the results obtained not directly comparable, the authors tried different hyperparams settings regarding the number of channels, concat fonts, hidden layers, etc for the different tasks (CWS, POS and NER). Regarding the results, Tables (1, 2 and 3) and Figs (5, 6, 7,…) are, however, hard to understand since not all the settings are ploted (why?), the legend in Figs  are difficult to match to the rows in the different Tabs, NA values are not explained, etc. 

My main concern (regarding the ablation studies performed) is that the grid of values for the different hyperparams in the different settings do not follow any logical pattern and it seems that they have been selected at random. Therefore, I cannot say whether the claims stated (e.g., “the results of altering font_channels and num_font_concat demonstrate that the model performance does not necessarily improve with a larger number of fonts”,…) are backed-up clearly enough. Also, the varying training durations issues, albeit external to the authors, also makes the contributions and conclusions incomplete and a bit weak. Authors should have look for other compute resources (https://reproducibility-challenge.github.io/neurips2019/resources/) or restrict their analysis to a minimum number of epochs/time to make the results comparable.   

On the other hand, it is noticeable that the authors have tried to test the “generality” from the original approach (as claimed by Meng et al.) and tested it with other logographic languages (e.g., Japanese).. The results were far from satisfactory, mostly due to the ad-hoc tuning performed in the original paper which were not applied when addressing the Japanese POS tagging task.


Other issues/doubts:

- Glyp_ratio is mentioned to be relevant regarding the results (jointly with glyph_decay), but it is neither further explained, not it appears in any of the tables. This whole study would improve (an would be much more useful) upon analysing these hyperparams.Why the authors just focus (maily) on chanel and concat?
- I think Fig 6 is redundant. Fig 5 is big enough to spot the differences in performance at any range of steps. 
- Why the authors have modified the hidden layers in only one task? What about the rest? Does this affect to the overall results?
- No discussion is provided regarding the state of reproducibility/replicability os the original paper. No recommendations are given.
- There is no info regarding whether the authors have reproduced from scratch or re-used author repository (code, data, etc).
- Have the authors contacted with Meng et al?",19-87.txt
25,https://openreview.net/forum?id=XibbOqrT4q,0,8,3,"This paper presents a replication and ablation study on the original paper ""Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation"" by Ke Wang et al.

The authors obtained the code provided by the original authors and conducted some ablation study.  During this task, they found some bugs, inconsistency on the loss function, and some other setting issues.  They also discuss the evaluation metrics the original authors utilized.  They showed some concern about using BLEU scores for the evaluation since it only focuses on the similarity with the gold standard sentence(s) while the style transfer tasks can have a number of possible outputs.

The papers ablation study and experimental settings are well-organized.  They found some bugs and inconsistency in the original code and execution, and some fault preparation of the testing files.

Overall, this paper is a nice sample of reproduction and ablation studies.",19-88.txt
25,https://openreview.net/forum?id=XibbOqrT4q,0,7,3,"The paper presents a good summary of the original unsupervised text attribute transfer work (FGIM) in Section.1 and 2.  The ablation study firstly shows the difference between the code implementation and what was claimed in the original work. After that, a comprehensive evaluation is organized to 1) reimplement independently the same evaluation test in the original work, especially perplexity measurement, multi-BLEU score, attribute classifier accuracy 2) traverse different parameter settings to study the parameter sensitivity, such as the value of latent space dimensions and the number of transformer layers. 3) compare the models provided by the original work with these trained independently with varied parameters and the human references. The reproducibility report points out the generability issue, inconsistency between BLEU and accuracy score and the training data flaws of the original work. However, though the authors claimed that they generated two accuracy variants in the evaluation, as the original work didn't describe the accuracy definition. We only find one accuracy score in Table.1 and 2. It would be great if the authors can clarify their use of accuracy measurement with more details: e.g. the concrete formulas of the accuracy definition. ",19-89.txt
25,https://openreview.net/forum?id=XibbOqrT4q,0,5,4,"This paper tried to reproduce the paper of Wang et al. 2019 on 'controllable unsupervised text attribute transfer' and concluded that although the overall model design is reasonably correct, there is a performance difference between using the pre-trained model and the reported results in the original paper. Some ablation study is done with variation in latent space dimension and number of transformer layers to report the robustness of the reproduced model. Overall, the efforts appear to be solid, and the findings and conclusions are well-described in general. I have a few comments:

- One major limitation of the paper is that they did not use the relatively larger dataset (Amazon review) for their experiments - making the findings not totally aligned with the original paper.

- Section 4.1: It's not clear what kind of bugs were fixed in the original codebase for the experiments. It would have been also nice to see the results using the original codebase intact (if possible) just to get an apple-to-apple comparison.

- Section 4.2: Results with/without GRU in the encoder could have been also reported as part of the ablation study to further verify the original model performance.

- It's not clear why the caption dataset test files had identical classes and how this issue was resolved. Please clarify with examples.

- It's not clear if sufficient efforts were made to communicate with the original authors regarding the implementation of the evaluation metrics - I think major performance difference is occurring due to this mismatch. 

- Section 4.4: the statement: ""reducing the number of layers had minimal impact ..."" seems to be contradictory with the results shown in the tables, as the accuracy has dropped considerably, which is a more important metric than BLEU as discussed in the previous paragraph. 

- There are numerous grammatical errors/typos in the paper that need to be revised.",19-90.txt
26,https://openreview.net/forum?id=3Jxp81KAS3,0,5,3,"In this study, the authors aim to reproduce the attack algorithm proposed in ""Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks"". The subspace attack method extends the existing Bandit attack approach for the gradient approximation of the victim model. It chooses a subspace of gradients by using the gradients of a reference model, instead of using the full basis of the sample's space to estimate the true gradient. The authors implement the subspace attack using PyTorch, without referring to the code provided in the original paper. They present results that replicate untargeted subspace attacks on the GDAS and WRN models trained on the CIFAR-10 dataset. The results show improvement in performance for the median number of queries per attack, but it is worse in terms of failure rate. 

Overall, the authors do a great job of motivating the problem and explaining the background in the paper. The critical issue is that the failure rate reported in the study is worse when compared to both the Bandit attack and the subspace attack method. The authors speculate that this is due to a different loss (cross-entropy)  function than the original paper (hinge loss), and they were unable to resolve this issue (corresponded with the authors of the original papers, but does not seem like they got a response). 

Problem statement 
The report clearly states and understands the problem statement of the original paper. The authors did a great job of covering the background and even resolving some notation differences between the bandit attack and subspace attack papers. They even describe the algorithm and the details of their implementation in the paper. 

Code: 
The code was reproduced from scratch and is available. 

Communication with original authors
The paper does not mention of any correspondence with the original paper's authors. However, I did see a question related to the loss function on the Open Review. 

Hyperparameter Search
The authors repeated the hyperparameter tuning, reported the best hyperparameters, and found a discrepancy with the best hyperparameter 
ηg
 reported in the paper and the experimental logs of the original study. 

Discussion on results
The authors added different evaluation metrics to understand the performance of their implemented model on top of the median number of queries per attack used in the original paper. These metrics allow them insights like the reason why their failed attack did not succeed is the inability to estimate the signs correctly. 

However, the main issue is that the failure rate of their implementation is much higher than the ones reported in the bandit attack and subspace attack papers. It is not entirely clear why the issue of difference in loss function could not be resolved. 

The result to support the claim ""while we managed to reduce the computational cost of the attacks"" is missing. 

The authors mention that ""we then expand our implementation to be used to attack WRN and Pyramidnet,"" but the results for Pyramidnet are missing. 

Recommendations for reproducibility
The recommendations provided are mostly about notations and descriptions instead of practical suggestions. 

Also, the authors mention that ""additional information about the used loss function could be helpful to replicate the obtained results."" Given that this is the main reason behind their implementation's high failure rate, the authors should explicitly mention the exact information that would allow the accurate implementation of the loss. 

Overall organization and clarity 
The paper is well written and clear to read. 

The failure rate for 
ηg=0.1
 in Table 3 is 3.4 but in Table 4 is 3.7

It may be useful to add error bars in plots to show averaging across overall/successful attacks. ",19-91.txt
26,https://openreview.net/forum?id=3Jxp81KAS3,0,6,4,"This report tries to replicate ""Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks"" from scratch, based on the original paper and supplementary materials. Authors state that their replication efforts do not match performance claims in the original paper.

The quality of the report is good in general. The authors understand the problems well. The code is well readable and well organized. The replication approach are stated clearly, e.g. models used, hyper-parameter search and etc.

My main concerns are on the thoroughness of the negative statement wrt the replicability of the original paper. After all, it could definitely be possible that the original paper is not replicable solely based the paper and supplementary material, however, it would be nice if the authors can perform additional steps to make the negative statement more thorough. For example, the authors could
1. run the original implementation to make sure the results are reproducible
2. if the results is indeed reproducible, having some analyses on the inconsistencies between the original authors' code and the paper
3. make suggestions wrt writings to the original author based on the above steps

(This is my first time reviewing for the reproducibility challenge. These asks might fall out of the requirement for the replicability track, but it is certainly beneficial if we want to conclude a negative statement on replicability/reproducibility.)

The authors also mentioned that they were not clear on how to use the ""Carlini and Wagner"" loss in this case due to the mismatch between settings. Based on my knowledge in the field, people usually use the loss in page 9 Section VI.A of https://arxiv.org/pdf/1608.04644v2.pdf, 
f(x′)=max(max{Z(x′)i:i≠t}−Z(x′)t,−κ)
It can also be found in major adversarial robustness toolboxes, e.g. 
https://github.com/BorealisAI/advertorch/blob/5daeeee98077afacf94728936a04cebde474259f/advertorch/attacks/carlini_wagner.py#L100
and
https://github.com/tensorflow/cleverhans/blob/5c1ccc259fa836d69e9582372b0d7f97140a66ef/cleverhans/attacks/carlini_wagner_l2.py#L249

However, I notice that the authors did try to clarify this with the original paper authors at /forum?id=S1g-OVBl8r
Although this is kind of well-known within the adversarial robustness community, I agree it is beneficial for the original authors to make it clear in the paper.",19-92.txt
26,https://openreview.net/forum?id=3Jxp81KAS3,0,8,3,"Disclaimer: I am not very well calibrated regarding the overall score metric. I'm therefore open to adjusting my rating.
- Use original reference and victim model

_Problem statement_
- The problem is stated in a clear and concise yet comprehensive manner.

_Code_
- Clearly written and well structured code is available alongside jupyter notebooks that show the experiments.

_Communication with original authors_
- This is my only criticism towards this work, since the problems with the loss-function could potentially have been fixed. Communication with the original authors, however, is not reported. 

_Hyperparameter Search_
- Hyperparameter are assessed with great care and to the point that inconsistencies in the original work are discovered and corrected for.

_Discussion on results_
- The report discussed possible reasons for the difficulty in reproducing the original results. In particular, the authors report that they are unable to implement the original loss function and therefore resort to cross-entropy loss. They cite this as a possible reason for the observed discrepancies.
- The authors provide reasonable suggestions to the original authors for improving their work

_Overall organization and clarity_
- The report is clearly written and well organized.

_Grammatical issues / organization / proper plots_
None",19-93.txt
26,https://openreview.net/forum?id=3Jxp81KAS3,0,4,3,"The paper introduces the subspace attack problem in Section 2 and related algorithm in Section 3. However, the definition of the hyper-parameters are only introduced in Section 4.3 experiments settings. It is not clear how the hyper-parameters affect the performance, such as failure rate. The code is available online with well-written docs.

The authors have posted questions on Openreview to the NeurIPS paper authors due to higher failure rate than the ones reported in the original paper. From the comments left by the authors, this work uses Cross-Entropy Loss while the Neurips paper chooses the hinge logit-diff adversarial loss. The authors are not sure how to implement the hinge logit-diff adversarial loss for untargeted attacks. It is a good point but not not emphasized in the paper. 

The experimental design requires more description. For example, the data set used for Table 3,4 is not specified. The discussion of correlation between the results and the related hyper-parameters is missing. For example in Fig 2(b), it is necessary to explain what cause the nearly overlapping of all three lines. What are the reasons to plot these figures? The reproduced results are not comprehensive. For example, the Neurips evaluates its performance on CIFAR-10 and ImageNet, while I guess Table 4 use CIFAR-10 according to the cited results.

The authors recommended to keep notations consistent with previous works, mainly [10]. It helps since [1] follows some settings in [10]. 

I think the organization of the paper can be greatly improved.",19-94.txt
27,https://openreview.net/forum?id=y7de_mZccJ,0,7,4,"Problem Statement: 5/5
The paper does a good job of restating the problem from the original paper and is concise but standalone.

Code: 5/5
New implementation of the algorithm that is included

Communication with Original Authors: 3.5/5
I did not see evidence of communication within the paper, but I imagine some communication must have occurred because the authors discuss checking with the previous logs.

Hyperparamter Search: N/A
Seemed adequate for reproduction

Ablation Study: N/A

Discussion on Results: 3.5/5
The results are discussed and presented reasonably well. However, I would have liked to seem a more in depth discussion about the areas of the approach that failed to replicate. The authors should consider expanding on the discussion they currently have

Recommendations on Reproducibility: I did not see a recommendation for the original authors

Overall Organization & Clarity: 2/5
The organization of the paper is fine, but the writing has a lot of room for improvement. I began by noting specific areas for improvement but ultimately stopped. The overall paper should be substantially edited to account for grammatical errors and clarity issues. 

Other comments
—————————————
“It becomes important” — non-grammatical
“Ensembles attacks’ … Gradient Descent” — unclear, consider revising
“fed into a machine learning … prediction.” — unclear, consider revising
“Thus, the aim ….” — unclear
“While… the victim” — consider splitting into two sentences and avoiding former/latter
“This emphasizes….” — awkward language
36-47: The content of this good, but the language has a lot of clarity and grammar issues. 
 
56: “have proven” — ‘proved’ 
72: “ease of read” — ‘ease of reading’",19-95.txt
27,https://openreview.net/forum?id=y7de_mZccJ,0,7,3,"- Summary
The author reproduced the results of the novel recent paper: Subspace Attack: Exploiting PromisingSubspaces for Query-Efficient Black-box Attacks. They have verified that the subspace attack has been successful in terms of reducing the number of queries but they found out that the successful rates were lower than the ones reported in the original paper. They suggested that this inconsistency was mainly due to the different loss used in the original paper (the hinge logit-diff adversarial loss from Carlini and Wagner). The authors also discussed about insight they found through the reproducibility (e.g., the subspace attack correctly captured most relevant gradient components for classification).  Overall, the report is well written and the experiments seem to be solid. 

- Major comment:
My major comment is upon the argument of the hinge loss used by the original paper. Since the authors have the access to the original code, would they be able to figure out how this loss is actually used in the original paper? Maybe I am missing something, I do not quite understand why there is a difficulty in changing to the hinge loss used by the original paper? If they can adopt the hinge loss, and reproduce to see whether the results in the original paper are on par, this report would have become more grounded. ",19-96.txt
27,https://openreview.net/forum?id=y7de_mZccJ,0,5,2,"This paper seems to do a good job of attempting reproduction of Guo et al. (/forum?id=S1g-OVBl8r&noteId=rU7l6FtdATH).
The authors identify a key lack of detail in Guo et al., around which loss function is used (lines 127-133).
The authors also contacted Guo et al on openreview to ask for clarification:
/forum?id=S1g-OVBl8r&noteId=ti16ibtAk9

They also note an incorrect statement of one of the hyperparameters in the original paper.

Ultimately, the authors are unable reproduce the results of Guo et al., and attribute this to the issue with the choice of loss function.  And they achieve much higher failure rates (e.g. 3.7% vs. 0.3% in Guo et al., see Table 4).

I wasn't able to easily determine what loss function Guo et al. used by referencing the Carlini and Wagner paper they cite (https://arxiv.org/abs/1608.04644).  Someone with more knowledge of the field might be able to, but I think this is still a genuine issue for the reproducibility of their paper (although a glance at the code could probably clear things up).

I have a few moderate concerns:
1) Limited scope: Guo et al. considered both targeted and untargeted attacks, the replication only the later.

2) 78-80: 
""In order to verify that we use the79 same settings – hyper-parameters and pre-trained models – as the original subspace experiments,80 we checked their log files and loaded the pre-trained models from their source code repository"" 
^I think this contradicts the literal specifications of the replication track, but is probably still OK (I defer judgment to the AC).
(From the reviewer guidelines: ""participants should not use any code which were released by the authors (if any). The objective of this track is to analyze the replicability of the presented idea just by reading the paper and supplementary materials."")

3) Lack of clarity around the issue of the loss function.  The authors comments on openreview suggest that they know what loss function Guo et al. refer to, but don't see how to apply it to untargeted attacks.  The authors should state explicitly what loss function they believe Guo et al. refer to, and why they don't see how to apply it to untargeted attacks.  Since this is one of the main issues raised by the authors, they should make sure what they do and do not understand is clearly communicated.  Stating: ""However, we did not manage to understand how to implement it as part of the subspace attack algorithm"" does not give enough detail.

Besides that, the paper and code release seem well done overall.

Minor concerns:
1) The ""Project structure"" section of the code release README is inaccurate.
2) 192-193: ""Clearly, the reason why the failed attack did not succeed is the inability to correctly estimate the signs."" <-- this should be justified.
3) Discussion / conclusions could also be fleshed out a bit more.  For instance, do the authors have any ideas why the difference in loss function could lead to an order of magnitude difference in failure rate?  Or other hypotheses for what might be causing the discrepancy?",19-97.txt
27,https://openreview.net/forum?id=y7de_mZccJ,0,7,3,"This report is well-written and communicates the problem in the original paper well. The original paper introduced a technique for making black box adversarial attacks more data efficient by using subspaces informed by the gradients of similar models known to the attacker.

This work is an independent implementation made without reference to the original code and carries out a reasonable search over hyper-parameters and ablations.

The reproduction attempt is not able to reproduce the failure rate (i.e. successful attack rate) of the original paper or the baseline that paper built on (Ilyas et al.). They do, however, find that this subspace method improved computational efficiency as claimed.

Because they were not able to reproduce the baseline used in the previous paper (which has a different set of authors), it is unclear how informative that lack of reproduction of Guo et al. is.

The issues around the reproduction are discussed well and the work is well-written overall. One thing that should be changed, is to make sure that in the abstract and conclusion, the lack of reproduction of the baseline method is discussed (Ilyas et al.), as this is important when interpreting this result.

This report highlights some ambiguity in the original paper regarding hyper-parameters and losses. For the loss in particular, it would be helpful to examine whether this is easily clarified by the source code or direct communication with the authors. This is especially important, since, as discussed, this could be the source of the discrepancy in results.",19-98.txt
28,https://openreview.net/forum?id=_P25N7pfXJ,0,6,4,"Good reproducibility effort, adds to the understanding of the paper by adding extra experiments. However, overall reviews are not quite good enough for the AC to recommend to the journal.",19-99.txt
28,https://openreview.net/forum?id=_P25N7pfXJ,0,7,4,"+ A few more citations are needed in the introduction and some typos pointed out below
+ I would point out the lack of access to the mouse dataset in the conclusion since that affects reproducibility
+ It would be great if you can provide the scripts to download and process the data that you used so that people can follow along with any process that you did that the authors didn't include in their work.
+ It is great that there were more experiments run beyond the original paper with more insights. Additionally, there was a good catch of a possible minor error in a figure from the original work. I would suggest contacting the original authors and asking about this labeling mismatch before final submission to ensure that this is truly the case. Similarly, I would ask them if they can make the mouse dataset available publicly so that you can address this.


Typos/Citations:

RECENTLY,    intense    discussions    have    focused    on individual’s  right  to  remove  their  personal  data  from internet  search  or  other  directories [cite, which discussions?].  Many  efforts  have  been put  towards  this  direction  including  the  EU’s  Right  To  Be Forgotten [cite].   The   Right   allows   people   to   remove   personal information from the Internet or other directories by request. If these policies come into [[effect]], it could bring a challenge to  data  science  field  and  any  related  fields  since  research and  outcome  based  on  the  affected  data  would  [[have to]] be  redone  or re-evaluated. As a recent[[ly developed]] topic, there [is not much] research done about efficient data deletion problems. Thus, a framework  studying  how  to  efficiently  remove  specific  data applied in machine learning models needs to be formalized. In  the  original  paper,  Ginartet al. [[1]] developed  a  notion  of deletion efficiency for large-scale learning systems. They gave [[a]] formal definition of data deletion, approached data deletion as an  online  problem  and  rose  the  notion  of  deletion  efficiency. [[Specifically]], they proposed two deletion efficient unsupervised clustering   algorithms,   Q-k-means,   a   quantized   variant   of Lloyd’s   algorithm   and   DC-k-means   (Divide-and-Conquer k-means),   which   works   by   partitioning   the   dataset   into sub-problems   and   recursively   merging   the   result   of   each subproblem. Also, the authors provided detailed mathematical proof[[s]] of the runtime of algorithms
+ reproduce   the   deletion   efficiency(amortizedruntime)  --> reproduce   the   deletion   efficiency (amortizedruntime) 
+ using  datasets  found  from  sites,  since  they  are  not  providedalong.  --> using  datasets  found  from  sites,  since  they  are  not  provided along [[with the code]]. 
+ e  experiments  presented  inthe original paper is easy to follow and implement --> [[are easy to follow and implement]]",19-100.txt
28,https://openreview.net/forum?id=_P25N7pfXJ,0,7,4,"Problem Statement:
The problem statement is clear: to replicate the results found for the algorithms proposed in the original paper. The results found are similar to those found in the original paper, save that the replication is run only once per setting while the original paper ran each seeing multiple times, generating a measure of variance in each experiment, and the exclusion of several datasets due to computation/time constraints.

Code:
It appears that the authors used the code from the original paper, though it's not entirely clear: ""we referenced github code for the two algorithms"". This doesn't seem to meet the requirements of the reproducibility track, where code should created from scratch? No link to the new code created is provided.

Communication w/ Original Authors:
No communication is mentioned, and no communication was found in OpenReview.

Hyperparameter Search:
A hyperparamter search is performed beyond what was performed in the original paper, showing effects of varying DC-tree height and width.

Ablation Study:
No ablation studies performed.

Discussion on Results:
The authors clearly discuss the steps they took to reproduce the results found in the original study.

Recommendations for Reproducibility:
No recommendations given save an error correction.

Overall Organization and Clarity:
The paper is clear and concise, replicating and validating results found in the original paper. The only improvements might be to 1. repeat experiments to replicate the variance found in the original study and to 2. confirm results on all datasets included in the original study.",19-101.txt
28,https://openreview.net/forum?id=_P25N7pfXJ,0,6,4,"This paper aims to reproduce the results reported from the paper: ""Making AI Forget You:
Data Deletion in Machine Learning"".

The paper references the GitHub code released with the paper and apply on three out of six datasets which are leveraged in the original paper. Reasons are some of the datasets are not publicly available any more while some are too large which is time inefficient to deal with given limited computation resource.

The three adopted datasets are covtype, MNIST and Gaussian datasets, The three omitted datasets are celltype, posture and botnet. 

This paper try tp reproduce the most of the experiments reported in the original paper, while it also proposed to analyze the relationship between the tree height and the loss ratio.

Besides, there are some aspects that may need some improvement.

(1) It lacks the evaluation on Silhouette Coefficients and Normalized Mutual Information metrics on the three datasets.

(2) In Table I of this paper, it shows the same trend as reported in the original paper. However, it does not include the standard deviation, which is a significant lack, because the runtime each time cannot be exactly the same. 

(3) For the numbers reported in Table I, it is necessary to further show how similar the ""trend"" it is with the numbers in the original paper. And it needs to highlight out the computer system difference, i.e. cpu, memory, etc.",19-102.txt
29,https://openreview.net/forum?id=kxF_scG4rW,0,5,4,"This review analyzes the ablation study conducted by Verecken et al. of the manuscript “Spike-train level backpropagation for training deep recurrent spiking neural networks”.

In this review, I will refer to the original ST-RSBP paper and its authors as the “original paper” and “original authors”, respectively. In reference to the report being reviewed, I will refer to it as “Verecken et al.”.

The report is well written and uses the original implementation of ST-RSBP. Through their study, Verecken et al., analyze a key improvement postulated in the original paper, namely that ST-RSBP outperforms HM2-BP with a more precise computation of error gradients. Although Verecken et al. peformed the analysis only on MNIST, they were able to show that the test error decreases with the new gradient calculation presented in the original paper.

Despite this being an ablation study, Verecken et al. did not perform a thorough investigation of the hyperparameters shown in Table 1 of the original paper. This is disappointing. The original authors mention that these hyperparameters were “empirically tuned” and the goal of the ablation study should have been to get more insights into how sensitive the model is to the choice of parameters. Verecken et al. only explore 
τm
. Their analysis seems to show that the model performs better for larger values of 
τm
, even larger than 
τm=64
 as chosen by the original authors. These findings should be taken with a grain of salt since Verecken et al. did not train their model using the full MNIST dataset.

Of greater interest in this report is the analysis of computations of the spike-train post-synaptic potential (S-PSP). Verecken et al. show that there were errors in the original implementation when using 
τm
 and 
τs
. After correcting the original code, they acknowledge that the effects of mistakes in the original code can be compensated by changing other hyperparameters. Nonetheless, they provide a corrected version of the MATLAB code.

In summary, this report provides interesting insights with respect to i) the reproducibility of the improved error gradient approximation and ii) identification of errors in the original implementation of the S-PSP model. Where the report falls short is in the overall ablation study because only one hyperparameter is explored and only one of the five datasets in the original paper is used. This is insufficient and, in my opinion, limits the applicability of this report.",19-103.txt
29,https://openreview.net/forum?id=kxF_scG4rW,0,6,3,"The report presents an ablation study of the inference algorithm proposed in the NeurIPS paper.

The aim of the ablation track of the challenge was to make a rigorous ablation study and hyperparameter search. While the authors did get an interesting ablation study for the error gradient approximation and found a mistake in the code released by the authors of the original NeurIPS paper and the way how to fix it and adjust the hyperparameters accordingly (well done!), the hyperparameter search part of the report is not very exhausted. Out of 10 hyperparameters listed in Table 1 in the original NeurIPS paper, the authors of the report only consider one hyperparameter. 

Although this is neither directly an ablation study nor hyperparameter search, but this is an important point the authors only briefly mentioned: “We then evaluated test error on the MNIST dataset, which was the only dataset we could access and use without errors in the code.”  The original NeurIPS paper considered 5 different datasets, if the authors could reproduce results with only one of them, it should be more emphasised in the report since it is very important from the perspective of reproducibility. It is especially important for the ablation track of the challenge, since this is the only track that is supposed to use the code provided by the original authors. Or the issues with the other datasets were not related to the code bugs but rather, for example, technical issues such as computational time (it is mentioned in the report that even full MNIST experiments were not feasible)? The computational issue with MNIST should also be elaborated more in the report. MNIST is not a very big dataset, and if the algorithm is too slow even on MNIST then the algorithm is too slow in general and it is worth mentioning.

The statement “We are interested to learn how the authors determined their default values for these parameters.” and the inability to reproduce results on datasets other than MNIST suggest that the authors of the report have not engaged too much with the authors of the original NeurIPS paper as expected in the challenge call. Moreover, this sentence “We are interested …” does not seem appropriate at least in the current form for a publishable report.

Other suggestions:
1. Variables in eq. 1-2 are not all defined
2. RSNN is not defined
3. Figures 1 and 7 are not readable: where we see blue or red curve we have no idea how far the other curve is. I suggest to plot data, fit and difference in different subplots. Since Figure 7 is the main part of S-PSP Computation part of the report, it is essential to have it readable, otherwise this whole part is not justified
4. “Fig. 2: Test Error over 10 training epochs using new ST-RSBP method and old approximation method” – it is better to name “old approximation method” to connect to the legend names in the figure. The same for the caption of Figure 3.
5. It is not clear why to have Figure 2 if there is Figure 3. Also, Figure 3 have different results for the first 10 epochs in comparison to Figure 2. I would expect them to be the same and Figure 3 presenting just more epochs than Figure 2. This is apparently not the case. Could the authors please explain what then the difference is between Figures 2 and 3 apart from Figure 3 having more epochs? 
6. Missing explanation why exactly these values for \tau_m are considered for the hyperparameter search
7. A bit confusing that the legend names in Figure 6 does not match the names in the text, i.e. it requires extra thinking that “the spike train length” in the text corresponds to “time” in the legend

Minor:
1. Introduction. “An alternative to artificial neural network architectures (ANN)” – move (ANN) after “network”
2. Introduction A. “The differential system of \alpha_i(t) is has as input an weighted sum of spiking trains of neurons j.” – couple of typos: “is” is redundant, “an” -> “a”
3. Introduction C. “We show the effect of pre-synaptic neuron j on post-synaptic neuron i with e_{ij} (the S-PSP)” – missing full-stop
4. Section II. “backpropagation technique of the [2].” – “the” is redundant
5. Make reference to figures consistent: “Fig. 1” vs “figure 3”.
6. It would be better to have tau as a Greek letter in Figure 4.
7. Section V. “We believe a mistake has been made on the computation of the e_{ij} factor its influence can be balanced with a new set of hyper parameters because it is mainly a scaling error” – a full-stop between “factor” and “its”?




 ",19-104.txt
29,https://openreview.net/forum?id=kxF_scG4rW,0,7,5,"- Summary
This reproducibility study is to validate the novel results from the recent paper of spike-train level back-propagation algorithm (ST-RSBP) by performing ablation and parameter search. And the authors are able to find one error Matlab simulation of the S-PSP fitting of the original paper and correct it. The final results are strong and convincing. A good report overall. Here are some of my comments for further improving the quality:

- Clear definition
  - The D term in Equ.(1) is ambiguous. Would you mind defining it? 
  - Please define those t_i, t_j's in Equ.(2). Or at least reference to the HM2-BP paper.

- Result quality
  - Results- A/B : need error bars in Fig. 2, 3 and 4 to show the statistical significance.
  - Results - C: For inconsistency issue, did you inform the authors of the paper as well?

- Small grammar issue:
  - ""The differential system of αi(t) is has as input an weighted sum of spiking trains of neurons j."" of Section I-A should be ""The differential system of αi(t) has the input as a weighted sum of spiking trains of neurons j.""",19-105.txt
29,https://openreview.net/forum?id=kxF_scG4rW,0,9,3,"The idea of Spiking Neural Networks (SNNs) is to use sequences of spikes to represent different kinds of signals in artificial neural networks. In the original NeurIPS paper, a novel Spike-Train level recurrent spiking neural networks (RSNNs) backpropagation algorithm was proposed for training deep RSNNs. The main purpose of the research was to address the problems of gradient computation. In the current Reproducibility Study, the authors mainly take the ""Baselines Track"" and the ""Ablation Track"", and studied the following three aspects.

A. Improved the computation of gradient with respect to weights.
B. Searched the hyperparameter 
τm
C. Simulated and provided a way more precise approximation (using polynomial fitting) of the interactions between S-PSP (Spike-train Level Post-synaptic Potential) and the spike firing rates.

So we believe that this is an inspiring reproducibility study possessing value on its own.",19-106.txt
30,https://openreview.net/forum?id=8SyaA--yA4,0,4,4,"The lookahead optimizer introduced in ""Lookahead Optimizer: k steps forward, 1 step back"" is a meta-optimizer that takes as input a method such as Adam and then adds a simple modification -- linear combination with older weights every few iterations. In the published paper the authors did many experiments including Resnets on Cifar-10/Cifar-100 and LSTM on Penn treebank.
In this submission, however, only Resnet-18 is considered for Cifar-10 and Cifar-100. Moreover, only one optimizer is combined with it in each of the two experiments and it seems that little hyperparameter search is done. I'm especially confused by the fact that the authors didn't provide the values of the slow weights step size and 'k' that they used in their figures. 
The results are strange as well. I'm not sure if that's an issue with the implementation of Resnet-18 in Keras, but the achieved test accuracy is below 90%, which is strange---usually Adam gives at least 93% with no tuning whatsoever.
The work is also not well written, there are many typos such as ""the the"" or missing commas and periods.
I feel that this should not count as a reproduction. ",19-107.txt
30,https://openreview.net/forum?id=8SyaA--yA4,0,4,4,"This paper looks into reproducing the results of the lookahead optimizer, which takes k greedy steps and updates the ""fast weights"" k times using any standard optimizer before updating the slow weights once. The paper reimplemented the optimizer through Keras and reported the training/validation accuracy on CIFAR-10 and CIFAR-100 with ResNet-18. 

There are several concerns about the paper. First, the evaluation was only done on a very small subset of models and datasets. The paper claims that it is because of lack of resource and time, which could be true for the transformer model, but not necessary for the language modeling, e.g., penn-tree bank, and many other models. It would be better to at least show one model from the NLP side, as the generalizability to different tasks is an important claim from the original paper. 

Second, the paper lacks details on many settings. It does not provide details on how hyperparameter search is done. For example, it is unclear how the 40 epochs of hyperpameter search (Section 3) results guide the 200 epoch training (Section 4.2). It is unclear how the results from multiple random seeds are reflected in the reported results. And it lacks detail on the hyperparameter setting of the baseline Adam training results, such as the batch size, learning rate schedule, the decay factor, etc. The paper also does not give a reason why it chooses to train with a different number of epochs, and how to interpret the results that are different from the original paper.  It would be better to add those details. 

Third, the paper gives no ablation analysis on how to select the slow weights step size (Section 2.1), and there is no evaluation on the robustness of lookahead optimizer to inner optimization algorithms, k, and alpha, all of which are all important properties from the lookahead optimizer. 

Fourth, the writing seems to be done in a rush, with many typos and grammatical errors. See some of those below. 

Minor:

The accuracy curve on the training and validation datasets look almost exactly the same in terms of the trend and magnitude. It would be better to double check whether the same data is accidentally used to plot both figures or give explanation on why this is the case. 

In section 4.4., the paper explains that Adam does better than lookahead in the initial phase and explains that this is trivial because in the initial phase the slow-weights update only once very k epochs. However, isn't k a constant throughput the entire training process in the lookahead optimizer? If so, why is it the case that Adam performs better in the beginning? Furthermore, it should be k ""steps"", not ""k epochs"".

The paper says the accuracy obtained from Adam is much noisier because it is a property of Adam. It is unclear what property the paper refers to, and it would be better to state the reason explicitly. 

In the first paragraph of Section 1.3, ""we start will a set of"" -> ""start with"".
In section 4.4, ""much more noisier"" -> ""much noisier"".
In section 4.4., ""this change is learning rate is leading to"" -> change in learning 
In section 5, ""for the experiments we concluded"" -> ""conducted"".",19-108.txt
30,https://openreview.net/forum?id=8SyaA--yA4,0,4,4,"This manuscript provide a keras implementation of look-ahead optimizer. The performance is evaluated on CIFAR10 and CIFAR100 data. It confirms the superiority of the look-ahead optimizer. The authors did the hyperaparameter search as suggested by the original authors, and the code is publicly available on GitHub.

However, this manuscript has the following drawbacks.
1. There is no large-scale experiment. The original paper did four experiments (CIFAR, imagenet, language modeling, neural machine translation). I think the authors should at least provide the experimental results on one large-scale dataset.

2. There is no ablation study.

3. The results on CIFAR10/CIFAR100 are not close to the original paper. In addition, it only compares with Adam, which is not sufficient.",19-109.txt
30,https://openreview.net/forum?id=8SyaA--yA4,0,4,4,"The authors participate in the replication track. They aim to replicate the results in the Lookahead Optimizer paper. They state the original problem statement clearly in the introduction of the paper and they also provide code which they created from scratch. In the report, the authors do not indicate whether they talked to the authors of the Lookahead Optimizer paper. It seems that they did not contact the authors through OpenReview either. 

Due to limited resources and time, the authors only perform two experiments on CIFAR10 and CIFAR100. They set up their experiments according to the details in the original paper. They only show that the Lookahead optimizer converges faster than Adam and achieves a better performance than conventional optimizers on both datasets. However, the performance of their models is much worse (89.27% versus 95.27% on CIFAR10 and 67.98% versus 78.34% on CIFAR100) than that in the original paper. Although the authors mention their hyper parameter search may have caused the difference, they do not discuss what might have missed in the original paper which caused such difference. Furthermore, in the report, it is unclear that if improvement in convergence speed is similar to that in the original paper. In the discussion on result, the authors only briefly talk about the two hyper-parameters. But they do not provide any experiment to show how the hyperparameters would affect the results or the reproducibility. Overall, I don’t think the authors have provided enough experiments, details or discussion on the state of reproducibility of the original paper.

Overall organization of the paper is fine and it is also easy to understand.",19-110.txt
31,https://openreview.net/forum?id=vEaqKSCgf9,0,3,4,"Overview: 

Code: The authors of the replication provided code to reproduce their replication experiments. Because the original authors' code was not released in time for use in the reproduction report, a direct comparison is not immediately possible.

Communication with the original authors: The replication authors did a great job of communicating with the authors of the original paper.

Discussion on results: The replication study focusses reproducing Figure 1 from the paper, but fails to fully reproduce everything due to claimed time constraints and the possibility of errors in the reproduction. Unfortunately, I do not view these as making a strong reproduction report.

Recommendations for reproducibility: The authors do not make strong recommendations for increasing the reproducibility of the original paper.

Minor: There are many typos and grammatical errors in the report.",19-111.txt
31,https://openreview.net/forum?id=vEaqKSCgf9,0,3,3,"This reproduction appears to have failed to reproduce any of the original results. The author of the reproduction appears to have submitted this report before completing the work. This isn't stated directly or at the beginning of the report, but is hinted at throughout the report and acknowledged at the end: ""We couldn’t implement value computation under noisy environment due to time constraints"". This makes the report frustrating to read, as it is unclear how to interpret the results that are shown. I encourage the author of the reproduction to be more direct in acknowledging this. If the failure to reproduce the original is primarily due to lack of time, rather than flaws in the original method, I recommend the author withdraw this report. 

Bearing this in mind, it is possible that the original paper may contribute to the difficulty in reproduction. The code from the original paper appears to have been released after the reproducibility challenge began, but only partially. This reference code does not appear to implement model training, agent-environment interaction, or analysis.

The author of the reproduction appears to understand the main structure of the problem and algorithm in the paper. They appear to be unsure about several details of the original paper, such as the exact structure of the decoding scheme and the process for generating samples. The author of the reproduction thanks the first author of the original paper for helpful clarifications, but does not give further details. I thank the author for highlighting their difficulties, but these points need to be resolved in order to assess the reproducibility of the original paper.

Finally, the reproducibility report only includes qualitative results, which makes it difficult to evaluate the results presented here. Other than Figures 1/2, the model outputs are not compared to ground truth data or baseline methods, so it is difficult to tell if the method is successful. This is partly an issue inherited from the original paper, which also does not include quantitative results or baselines. However, if the author of the reproduction had added results of this kind, it would make it possible both to assess their work as well as the contribution and reproducibility of the original paper. ",19-112.txt
31,https://openreview.net/forum?id=vEaqKSCgf9,0,2,4,"The submission attempts to reproduce the paper ""A Neurally Plausible Model Learns Successor Representations in Partially Observed Environments"" by Eszter Vertes and Maneesh Sahani. From a presentation standpoint, the submission requires substantial revisions to improve clarity and correct typographical errors. The author's GitHub repository that is linked to in the paper is sparse, and does not aid further reproducibility efforts; there are minimal comments and documentation for how it could be used by other researchers. Also, the core.py file in the repository is a subset of the ddc_SF.py file provided by the authors of the original paper. The choice of values for the parameters in Section 4.1  (Environment implementation) is not justified, and no hyperparameter tuning is attempted in the experiments. The author of the submission did communicate with one of the authors from the original paper to ask some clarifying questions. To improve this submission, one approach would be to test the wake-sleep algorithm in different environments, with different choices of hyperparameters, etc.",19-113.txt
31,https://openreview.net/forum?id=vEaqKSCgf9,0,3,3,"The report is written extremely poorly and in rush. That makes it hard to follow the report.
I suspect if another group had tried to reproduce the results with investing more time, they would have gotten a higher success.",19-114.txt
32,https://openreview.net/forum?id=q2wW5zGp5L,0,2,4,"--------------------- Overview of the report ----------------------------

The report validates experimental results of the paper ""Making AI Forget You: Data Deletion in Machine Learning"" by a combination of reproducing results with the code provided by original authors and implementing their own algorithms. The report claims that slightly better performance can be obtained than originally presented in the paper by the original authors' code; their new implementation; or by publicly available machine learning package scikit-learn. The report also attempts to implement a new algorithm called HDBSCAN and claims that it is potentially suitable for the data deletion task.

Though it seems that the authors of the report understands main idea of the original paper, the writing summarizing original work can be improved and more completed. For example, 
- the last paragraph of page 1 says that Q-K-means ""is essentially the same as standard K-means, but quantizes clusters prior to partitioning."" While quantization is a big idea of Q-K-Mean, the original authors specifically mentioned 4 key differences (last paragraph, p.4) which are significant. 
- In second paragraph of p. 2, it can be more clear what it means that ""This process is repeated"" to readers who does not know k-means++ (it is unclear if distance is calculated from the last chosen center? Or first center? Or closest center?)
- Where did the author said that the baseline was not well optimized? (first line of Section 3.1 of the report).

One of the confusing part of the report is the introduction of HSBSCAN. To my understanding, the report's goal is to reproduce and validate baseline used in the original paper (standard K-means). HSBSCAN is another clustering algorithm used in the evaluation of this report, but I do not see much motivation explained on why HSBSCAN should be included, and if including it would be fair to the original authors (whose, I think, their main experimental contribution is comparing STANDARD k-mean, something standard that is commonly used). I also didn't see an explanation why HSBSCAN supports data deletion without retraining from scratch (the last paragraph of Section 1 claims that HSBSCAN is efficient to cluster, but no mention to support efficient data deletion). In addition, the report has not finished experimentation with HSBSCAN, and claims that HSBSCAN is a good potential given more time to optimize the algorithm. I would appreciate an explanation why HSBSCAN is a good addition to the report.

With regards to Results (Section 4), the report reproduces two novel algorithms by original papers, Q-k-means and DC-k-means, and compares the performance with that of original papers. 
- The report observes that better performance is attainable. It may be good to mention why so? Is it because of different parameters are used? Or even with exactly the same setting? If with exactly the same setting, it may be worth asking the original authors where the discrepancy may come from (I didn't see any OpenReview correspondence, but I can't say if they have reached the original authors or not). 
- The report performs on 2 out of 6 datasets used in the original paper - is there any reason not to do all 6 (or just the time constraint)? 
- Why is the report's own implementation k-mean* is much slower, even slower than scikit-learn? And why should it be implemented from scratch and evaluated to compare with the original paper?
- The report claims that scikit-learn algorithm from the standard python library is better than their reported standard k-mean implemented by original authors. While this is worth noting (to make sure that original authors use a good baseline to compare themselves to), the difference of performance between authors' implementation and standard library only exists in Covtype data and is almost nonexistent in Celltype data. Is this an accident? Or pointing to something?

In terms of organization and clarity, the organization are generally ok, but the writing can be greatly improved. There are many typos (listed below) that, while not so bad that I cannot understand the main intent, frequent enough to be distracting.

Example of typos:
- "" "" symbol
- k-means. Sometimes with letter k in type mode and sometimes in math mode
- many places that a period '.' and citation [] have to be swapped.
- missing spaces in several places
- right above Fig 2: ""to select an amount of clusters"" --> ""to select the number of clusters""
- end of Section 1: ""it's efficiency"" --> ""its efficiency""
- page 5 last paragraph ""Table 3 shows that .... runtime ...."" --> ""Table 3 shows .... runtime""
- and more

---------------------- Below are evaluations based on metric given by the reviewer guideline ----------------------------
Problem statement 

The authors understand the original papers' problem, main experimental contribution, and experimental evaluation methods.

Code

The authors use 2 algorithms implemented by the original papers' authors, 1 from public library (scikit learn), and 1 algorithm implemented by themselves.

“Hyperparameter Search”
The authors found that changing the convergence tolerance gives 100x in runtime.

Communication with original authors
I didn't find any communication on OpenReview.

Ablation Study
Not that I can see.

Discussion on results
Only mentioned that two of their algorithms' implementations were from original authors

Recommendations for reproducibility
No.

Overall organization and clarity
See overview above",19-115.txt
32,https://openreview.net/forum?id=q2wW5zGp5L,0,5,4,"This work compares different approaches for efficient retraining of clustering models after partial data deletion. This is motivated by needs in practical applications. The models include variants of K-means and density-based clustering; the authors have reimplemented the workflows based on readily available functionality in e.g. Python, and explored the possibility of speeding up the algorithms. It seems that the authors have been able to replicate the qualitative results from the original, cited work. 

The quality of the work is satisfactory. The motivation has been stated and the investigated methods are well known. At the same time, the description is highly technical and focuses only on a limited number of clustering techniques, with no clear justification for the choice of these particular models. 

The reporting is relatively clear; the language could be improved, and a wider set of alternative techniques, such as probabilistic methods, could be cited and discussed, even if they cannot be implemented within the scope of this work.  

The work is not original in the sense that it is a replication of earlier work, with some additional optimization and discussion on potential improvements. The significance remains limited to researchers who do applied work on data deletion and retraining problems, and are concerned about the execution times of such algorithms.

Pros:
- Problem and motivation clearly stated
- Comparisons are reported clearly

Cons:
- No source code provided; impossible to verify/replicate the analyses
- The number of benchmarked methods is rather limited, and not clear why these specific approaches (K-means & density based clustering) have been chosen 
- The overall language/presentation needs some improvement",19-116.txt
33,https://openreview.net/forum?id=aQorr-rCj,0,6,4,"The report follows the Ablation Track.

The report clearly states and understands the problem statement of the original paper.
Codebase with the reproducibility experiments has not been submitted

The authors use the released code-base of the authors of the original NeurIPS paper, and performed ablation studies and hyper-parameter explorations on it.

The report mentions communications with the authors of original NeurIPS paper for clarification and support discussing the new results.

The report extends the original results with novel experiments and detailed discussions  

In particular the current report introduces the following features:
1. It adds an extra baseline: Polyak's Heavy Ball method.

2. It considers the same LQ problem experiment but using an unstable system. This is particularly interesting, since the regret bounds of the original paper considered also this case but no experimental results were provided. The conclusion is that the current experiments validate the theoretical upper bound also for this case. 

3. It performs a grid search over the step-size parameters of the different gradient descent methods, concluding that the parameters used in the original paper are close to optimal.

4. For the nonlinear system, a robot following trajectory, it studies multiple combinations of time resolution and horizon length. In addition, it considers three trajectories, two of them available in the original code (but only one reported in the original paper), and a novel ""petal"" trajectory with multiple loops.
The addition of the ""petal"" trajectory is much appreciated as it allows to see a different trend in how the length of the horizon influences the performance. In particular, for the linear system, the longer the horizon, the more accurate results, as illustrated in the original paper. However, the current experiments show that the performance seems to be a nonconvex function of the horizon for the nonlinear system.
On the other hand, the report concludes that longer horizon is preferable only when the reference trajectory involves singular points, but this seems a strong claim in general and more empirical evidence or theoretical support would be required to support such claim.  

5. Also for the nonlinear system, the authors provide RMSE as quantitative metric. This is in principle a great idea in order to objectively evaluate the effect of the different parameters. However, it is not clear whether the authors included the control cost in the objective for these experiments too. Since the RMSE only considers deviations from the reference trajectory, the objective minimised in these experiments shouldn't include any other cost. Otherwise, it is not clear that the RMSE does not capture how well the minimisation has been performed.    


In summary, the report is interesting, reproduces the results of the original paper, and provides novel insights.  ",19-117.txt
33,https://openreview.net/forum?id=aQorr-rCj,0,8,3,"Problem statement 

The report clearly states the main problem statement as well as the many simplifications performed in the original paper for efficient computation.



Code

They re-used the code obtained from the author repository.

Communication with original authors
Check whether the report mentions fair communications with the original authors for testing reproducibility.
Also check if the authors have communicated with the NeurIPS paper authors through OpenReview.

Reading through the report and the open-review submission site, it doesn't look like there has been any communication between the authors of this report and the original paper. They were able to successfully reproduce the figures from the original author repository without this communication.


Hyperparameter Search
Check whether due diligence is shown in hyperparameter sweep. Bonus if the report goes above and beyond to try new hyperparameters not experimented by the original authors. (Mostly relevant for Track 1 and 2)

The authors do claim that they try different initialization strategies. However, since this does not significantly improve the results they do not report this. As per hyper-parameter search, they perform more extensive hyperparameter search and show that experimentally, the regret can further be minimized thus obtaining improved results.


Ablation Study
Check if the report provides comprehensive ablation studies. Bonus if the ablations are backed up with proper motivation. (Mostly relevant for Track 1 and 2)

The authors consider fine-grained time-scale to see the effect of the algorithm. They report that this improves regret however leads to more instability and oscillations. Additionally, they also try other methods as baseline/algorithm to see if it improves performance. However, they report that this is not the case.


Discussion on results
Whether the report contains detailed discussion on the state of reproducibility of the original paper

Yes, the paper contains a comprehensive discussion on the state of reproducibility.

Recommendations for reproducibility
Whether the authors provide any recommendations to the original authors for improving reproducibility

They do not seem to provide any recommendations for the original authors.

Overall organization and clarity
Grammatical issues / organization / proper plots

The paper is well-written and organized. I think one minor point is if the authors provide the link to the code repository of the original paper. They only link the original paper and one has to go through that to find the code. It will be convenient if that link is present in this paper.",19-118.txt
33,https://openreview.net/forum?id=aQorr-rCj,0,6,4,"Thanks for participating in this reproducibility challenge. This submission takes the paper ""Online optimal control with linear dynamics and predictions: Algorithms and regret analysis,"" and tries to reproduce the numerical results in the paper. Most figures from the original paper were reproducible, and further comments on how to improve the regret bounds through parameter tuning have also been discussed. However, all results reported by this submission were obtained by modifying the codes provided by the original paper. It would have been much more convincing if the authors have coded the algorithms of the original paper themselves, and were still able to reproduce the results provided by the original paper. ",19-119.txt
33,https://openreview.net/forum?id=aQorr-rCj,0,6,4,"This report does an ablation study of a theoretical paper called ""Online Optimal Control with Linear Dynamics and Predictions: Algorithms and Regret Analysis"". Thanks the author for reproducing the original experiments and for designing and conducting more experiments. 

I think the report clearly states and understands the results of the original paper, and it is well-written.

The authors of the report have communicated with the authors of the original paper for testing reproducibility as sell as clarifying some theoretical results, maybe by email, not through OpenReview.

The report reproduces the experiments in the original paper using the code provided by the original paper, and the results are consistent. The report also shows the results of other algorithms not tested in the original paper. In addition, additional experiments have been conducted by the author of the report. For LQ tracking problem, experimental results for unstable systems are included, and are consistent with the results for stable systems. For path tracking for a two-wheel mobile robot, new results for smooth trajectory such as the circle and petal are included, which however is not that consistent with the original results for the trajectory of heart. I mean the takeaways are different. I hope to see the discussion why this happens.

The report presents more results associated with hyperparameters that have not been tested in the original paper, and shows some different stepsizes actually lead to better performance.

I hope the authors of the report have already contacted the author of the original paper about the new experimental findings, and hopefully more detailed discussion of why the inconsistent (or counterintuitive) results happen could be included in the updated version of the report.",19-120.txt
34,https://openreview.net/forum?id=Z9FS-GRJi0,0,6,3,"The paper is generally well written, gives a brief but sufficient overview of the original paper, and attempts to reproduce the original results. 

One one baseline method (BERT) the reproduced results are slightly lower than the original results, but are sufficiently close that I could consider the method reproduced. On the other baseline method (BiLSTM-CRF), the reproduced results are notably worse. The authors suggest that this is due to some ""poorly structured"" data and missing (potentially undocumented) preprocessing steps. Though this initially seems concerning, after reading the original paper it is not entirely clear which preprocessing steps are missed. Thus, I believe error in preprocessing is not the fault of the reproducer. However, I would have liked to see that the reproducers removed the poorly structured data from the datasets in order to properly re-create the results. There is no evidence that they have contacted the original authors about this flaw in the data.

The ablation studies are well documented and interesting. The hyperparameter searches performed seem to be sufficient. 

In the future, I would suggest that the reproducers create separate plots for accuracy and loss for better visualization instead of plotting them on the same chart (as in fig 6).",19-121.txt
34,https://openreview.net/forum?id=Z9FS-GRJi0,0,5,4,"This paper aims at reproducing two baselines from the Glyph-vectors for Chinese character representations paper, which uses CNN to extract image features from several historical Chinese scripts since they are believed to contain rich linguistic information about the characters. This paper's reimplemented BiLSTM-CRF baseline underperforms the baselines in the original paper by a large margin, but the authors were able to achieve a similar (slightly lower) result for the BERT model. In addition, this paper implements several new baselines by combining BERT representations with BiGRU/Glyph-vectors.

Pros:
1. This paper performs many experiments and confirms that the BERT baseline is reasonable.

Cons:
1. Some presentations can be made more clear. For example, for Figure 2 a bar graph or simply a few numbers would serve the purpose of showing the label imbalance even better. However, I am not sure if this figure is useful or not.
2. It is unclear what's the conclusion of this paper. The main results seem to indicate that we can trust the baselines of the original paper since the numbers here are even lower than the original paper. However, in Table I higher numbers are achieved by Glyce+BERT+Transformer. What are the numbers in the original paper? Do those numbers outperform the reported baselines in the original paper or even the full model in the original paper?

Minor:
Some typos need to be fixed such as missing spaces.

Overall, despite the extensive experiments, I am confused about the ablation studies and even the conclusions of this paper to some extent. Besides, the performance of BiLSTM-CRF seems to be much lower than the reported numbers in the original paper and I suspect there might be some errors in the code. Therefore I would not strongly recommend the acceptance of this paper.",19-122.txt
34,https://openreview.net/forum?id=Z9FS-GRJi0,0,6,4,"This report is part of the baseline track, reproducing baselines for Glyce.

Two surprising results:
 - BiLSTM-CRF on the named entity recognition task is much lower than the originally reported. However, the numbers are so extreme low that I must suspect some unstable training. Wrong pre-processing seems not to be an option, because the same data-set is used for BERT and obtains comparable results to the original paper.
 - The authors of the report perform various model combinations, including one that uses BiGRU. Interestingly, they obtain very good results with this new combination. While it does not outperform the proposed approach in the paper (except for one dataset), it reduces the gap significantly and might raise doubts about the need of the specific logographic processing.

The report is not very well written, some tables are not numbered and the main points could be highlighted clearer. 
Also, the first of the surprising results should be investigated better, as no satisfactory explanation is given. ",19-123.txt
35,https://openreview.net/forum?id=hIUSq5u_i9,0,4,4,"This paper targets on reproducing the results of GRAN model proposed  for graph generation. Authors conducted two ablation studies, one tested the result of reducing the number of RNNs’ layers from 7 to 2. The other one is reducing the number of hidden units from 512, 256 and 128 to 50.  In addition, an extra layer is introduced to enlarge the inter-class distance among different classes and reduce the intra-class distances (reducing the inner variance of each class).  
The modified models are compared with the original GRAN.  The represented results are after 5000 training iterations. 

The reported results of original GRAN are not consistent with the reported ones in the original paper. Thus, it is questionable if the reported comparison results are meaningful. ",19-124.txt
35,https://openreview.net/forum?id=hIUSq5u_i9,0,6,5,"This paper performs ablation experiments on GRAN w.r.t. two hyper-parameters - reduction on latent code dimension and total number of GRU layers. Additionally, it also analysis replacing bernoulli mixture loss with angular layer loss.

Pros:
The report is well-written and it uses the code provided by the original authors. This challenge is taken on the assumption that GRAN employs large model capacity which can be redundant given the actual input graph sizes. The experiments in the paper demonstrate that this assumption holds true for most datasets except for large protein data. Further, inclusion of angular layer loss performs best for most model except on Grid dataset.

Cons:
Unlike the GRAN code, this work runs all the experiments for fix number of 5000 iterations across all the datasets. This may seems enough to validate the above three modifications. However, the plots in figure 1-4 suggest that the training of the model may not have stabilise even for the 5000 iterations. For instance, at intermediate iterations, I notice that the best performing model can keep alternating. Hence, it is difficult to draw any concrete conclusion. The author also points to the similar validity criteria under the conclusion section.

Setting aside the total runtime due to lack of GPU resources, I do feel the suggestion for pruning original GRAN model is quite interesting and should be explored further.",19-125.txt
35,https://openreview.net/forum?id=hIUSq5u_i9,0,7,4,"This report aims to reproduce and modify the Graph Recurrent Attention Networks (GRAN) [6] model for ablation study. The source codes and datasets are provided by the author. This report provides the following experiments: 
(1) Reducing the number of the layers and hidden units in the GRU components
(2) Results for adding the angular layer to explore its impact on performance 
(3) The reproduction experiment that runs the codes provided by the author.

I think the analysis is sound and the results are interesting. The results show that reduced models achieve similar performance on the lobster and point cloud dataset, while the addition of the angular layer could further improve the performance.

It’s quite unclear whether the original method would capture other characteristics of graphs than the ones used in the evaluation.   

For a few datasets, the reported results seem to be different (better or worse) than the ones reported in the original paper, it'd be better to report the variance. Also, elaborate more on the state of reproducibility of the original paper.

Additional comments: 

In Sec 3: “The lobster [2] dataset contains 100 random lobster graphs xxx with the number of nodes from 10 to 100 in each graph.”  What is xxx.",19-126.txt
35,https://openreview.net/forum?id=hIUSq5u_i9,0,4,3,"The paper does an ablation study on the Graph Recurrent Attention Network. It shows that decreasing the number of layers and hidden size does not lead to significantly drop in performance on datasets used in the original paper. The authors further extend the original model by adding another angular layer for regularization. The results looks good.

I would like to argue that this paper may not be good enough for the ablation track. 
1. The author does not perform a thorough search on hyper-parameters. It's good to see that we could get comparable performance with much fewer hyper-parameters, but it might be more convincing if the authors can take more values for the hyper-parameters to show how performance changes as the hyper-parameters change.
2. The original paper has two more hyper-parameters ""block size"" and ""stride"" that manage the trade-off between efficiency and accuracy. The authors fail to discuss these two hyper-parameters in their paper.",19-127.txt
36,https://openreview.net/forum?id=0Xr6sW7cX_,0,3,4,"This ablation study sought to determine the influence of hyper-parameters and constraints on the modeling of various point processes.

The study presented a limited form of ablation namely checking the basic settings found in neural network software:  learning rate, nonlinearity, training set size, optimization technique, and number of hidden units.  Additionally, the ablation tested whether positively constrained weights were beneficial.

The quality experimental paradigm did not appear systematic. No mention was made of multiple random seeds. Box and whisker style plots or confidence intervals would be useful for comparing the significance of differences.

Results of ablation study are limited to the unavailability of some of the data sets. It may be suggested to contact the authors of the paper to get their version of the datasets. Results of the study were also limited by the fact that Google Colab was used as a computation environment. Thus, the efforts are concentrated on the synthetic data. 

The clarity of the work was limited. Original acronyms and equations for the cost functions were not given. (In section 4.1 it would be helpful to define MNLL for the reader. MAE is also not defined. ) The organization of the work was unnecessarily broken into many subsections.  I am not sure of the role of section 2.2

The first two studies of training set size and learning rate seem to be concordant with original work, but the insight and discussion is limited. The most interesting plot is the number of hidden units. In this case, the authors show that the performance is generally better with half or a quarter of the units.

Another point is the removal of the constraint of nonnegative weights improves the performance.  However, in section 4.2, the rebuttal states that the weights do not need to be positive to ensure monotonicity, but this seems unfounded based on references to prior work in the original paper. It would seem the importance of enforcing this choice would depend on the application. 

Overall the ablation study does not add to the original work. Minor items such as the formatting and grammatical errors detract from the presentation. 

Minor: 
In abstract clarify that the network is producing the cumulative conditional intensity function. 

Last sentence of abstract ""purposed"" -> ""proposed"".

In Section 1.1,  ""Ablation study is"" -> ""An ablation study is"". The second sentence ""it"" is used in two senses. First is for features and second is for ablation. 

""of the time of isolated events"" -> ""a set of event times"" 

I disagree with the basic introduction regarding a ""more widely adapted understanding"" on ""how long events occur"" including the notation that 
λ=λ(t)
. This should refer to a homogeneous Poisson point process.  

""is applied to integrate the intensity function""  -> ""is applied to generate the integral of the intensity function"".

Capitalization of sub sections as titles is off for 2.1 and 2.2 

Bottom of page 3, ""synthesis data"" -> ""synthetic data"" 

""in GPU environment"" -> ""in an GPU environment"" 

Bottom of page 5, start of line there is a extra ""I""",19-128.txt
36,https://openreview.net/forum?id=0Xr6sW7cX_,0,3,5,"The paper is about performing an ablation study on the model proposed by Omi et.al. [1] by changing the hyper-parameter settings of the proposed model. The authors do a good job of identifying the parameters that should be optimized for the proposed model in [1] to verify how robust or varied the reported results are in [1]. 

However, the implementation and evaluation details presented in the paper are ambiguous. For example, authors report they use MNLL to evaluate the performance of the model under different settings but they firstly do not state what MNLL, which I assume is mean negative log-likelihood, and more importantly authors do not state whether MNLL is evaluated on training or testing data. The claims of authors could have been verified using the code provided but they fail to provide code that can reproduce the results reported. The code provided in the repository (I mean code by authors of ablation study) uses hardcoded values or pre-processing code. 

Moreover, the authors state in section 4.2 that they modify the model to create three new models but no references or experiments are reported for those models. Furthermore, experiments for changes in activation function are unclear as only the NN model should be tested for the changes unless the idea was to change the activation functions used outside the hazard function. Moving further one more ambiguous point is the description of real data and related code in the manuscript as there are no results or experiments related to it in the ablation study. 

1. Omi, Takahiro, Naonori Ueda, and Kazuyuki Aihara. ""Fully Neural Network based Model for General Temporal Point Processes."" arXiv preprint arXiv:1905.09690 (2019).",19-129.txt
36,https://openreview.net/forum?id=0Xr6sW7cX_,0,7,3,"The authors have reused code repository in the original paper while at the same time have coded modules for a better understanding of the hyperparameters and for implementing a simplified model, that captures the essence of the original model without loss of accuracy. The authors provide detailed discussions on how to reproduce the original paper.

 The reviewer could not find any communication between current authors and that of the original paper on OpenReview or any mention in the paper itself.

The authors have taken the time to reproduce and add to the existing code, modules that check the hyperparameter space such as for the number of NNs in the hidden layers and their sizes, different activation functions, optimisers and learning rates. The authors show that the learning rate has no much effect of the results of the original paper. Further they depict that for complex datasets, the NN is beneficial. Overall, the authors have been able to confirm the results of the original paper while also providing simplifications to the original model. They also show instances where the original model is over-complicated.   

The paper is structured alright but it would help if the paper is proofread by a native English speaker. Otherwise the paper reads well. ",19-130.txt
36,https://openreview.net/forum?id=0Xr6sW7cX_,0,6,4,"This paper is an ablation track contribution to the Reproducibility
Challenge, performing further experiments on NeurIPS 2019 the paper
titled ""Fully Neural Network based Model for General Temporal Point
Processes"" by Takahiro Omi, Naonori Ueda and Kazuyuki Aihara.  The
original paper proposes a model for temporal point processes based on
recurrent neural networks. The main contribution of the original study
is predicting the cumulative hazard function of a temporal process,
rather than directly predicting the hazard function. This avoids
integrating the hazard function during likelihood calculation, and
authors show more accurate predictions by the model compared a few
(strong) baseline models on a number of synthetic and real-world data
sets. The model includes a recurrent network, and a multi-layer
feed-forward network whose input is output of the RNN cell as well as
the elapsed time since the last event.  The present
reproduction/ablation study involves investigating effects of the
training size, learning rate of the optimizer, alternative activation
function, alternative optimizers, number of layers in the feed-forward
network, number of units in each layer, and a few other alternations
to the network structure. The ablation study reports experiments only
on the synthetic data using the code from the original study with
minor changes.

It is clear that authors (I use 'authors' to refer to the authors of
the reproduction paper, otherwise state explicitly as 'authors of the
original paper') put a serious amount of effort into the study, and
there are some interesting findings. However, the motivations of the
experiments performed/presented is not always clear, and the paper has
quite some structural, linguistic, and typographic issues. Below are
more detailed notes on the individual issues.

- The motivation of the choices of hyperparameter exploration is not
  always clear. For example, I do not see why exploring different
  learning rates are interesting, but, for example, batch size is not.
  The authors do not provide any clear motivation either.
- On a related note, the model alternations investigated are not
  specific to the target study, but are valid for almost any neural
  network model. Since the original paper have a rather focused
  contribution, it is difficult to find specific parameters/parts of
  the model to explore, but, nevertheless, a discussion of why these
  parameters are important to study (especially for the task/model at
  hand) is important.
- The authors note that they ""replicate the result from the paper""
  (p.3, last line, I take replicate here means ""reproduce""). However,
  they do not present the results in a easily digestible form, but as
  data points in each experiment (since the hyperparameters explored
  includes the original values). It would be informative to see the
  results of reproduction showing whether there are any divergences
  from the values reported in the original paper. Even better, it
  would help the reader a lot if the authors had presented the
  expected variation due to random initialization by running the
  original code multiple times (removing potential fixed random
  seeds). This is also important for comparing the performance of the
  model with different settings.
- The authors do not say much about their own work in the
  introduction. I suggest introducing the experiments done (and
  motivating them as noted above) in the introduction.
- The study paper is not described in sufficient detail. For example,
  without looking at the original paper, it is difficult understand
  what are the models compared (constant, exponential, piecewise
  constant, neural). Similarly evaluation metrics are not explained
  either. The authors should provide a more comprehensive summary of
  the original paper. On a related note, section 2.2, in my opinion is
  redundant. It is a relevant work to the original paper, and it
  probably deserves a mention, but not a whole section. After all, the
  present work (ablation) does not involve any experiments or
  comparisons involving the study introduced here (sect. 2.2).
- Similarly, the description of the real (non-synthetic) data sets,
  and the detailed reasons for their non-inclusion does not provide
  much information to the reader. A short statement indicating that
  they were not included due to difficulties in obtaining/processing
  them would be enough.
- I have difficulty understanding the conclusion drawn from training
  size experiments in section 5.1.1. My take from the results here is
  that the model seem so learn what it can learn from the minimum
  training set sizes used in the experiments, and does not seem to
  overfit (maybe slightly in a few cases). A somewhat interesting
  contribution here would be to show the learning curve up to the
  amount of data that is sufficient for learning.
- The visualizations on figures 3-7 are unconventional without a clear
  reason. It would be better if the authors put the variable
  investigated (e.g., training size in fig. 3) on the x axis (rather
  than the type of the input/random process), and draw a line for each
  random process as they did in figure 8 (in fact, for categorical
  parameters, the this still is somewhat odd, but better than the
  current alternative).
- In section 5.2 the experiments performed were not clearly explained.
  For example, it is not clear what 'input layer' here refers to. My
  guess is that this is the first layer after the RNN cell, but
  authors should be clear about which layers/inputs were ablated in
  these experiment, possibly by referring to the schematic description
  of the model presented earlier.
- I found the concluding remark ""... although NN model proves to be
  useful in the certain situations, it highly depends on the nature of
  the data. Therefore a well-knowledged individual is still needed to
  determine whether NN model is appropriate to employ.  Therefore, in
  the short term, NN model cannot yet replace a specific model driven
  by domain knowledge for point processes."" rather unfounded/not
  supported by the experiments presented in the paper.  The conclusion
  may well be true, but for that the authors would need to present
  more model-comparison results within the article. The only model
  comparisons (section 5.1.2) actually in favor of the NN model (in
  comparison to constant models), and the figures in the external
  document cited does not seem to support the claim either.
- The resolution of the graphs is rather low, and the font sizes on
  the graphs are smaller than the text, making them difficult to read.
- When referring to the authors of the original study, the authors use
  singular ""author"" in many places, should be authors.
- The authors use parentheses (not square brackets) for numeric
  citations. I suggest following NeurIPS guidelines more carefully.
- Section 1.1, line 5: purposed -> proposed.
- Section 1.1 is the only subsection of the section 1, I think it is
  unnecessary to divide it to subsection(s).
- Section 1.1, first paragraph: It would be better not to break the
  inline math formula 'λ = λ(t)'
- Please use the full form of the abbreviations at their first use.
  For example, MNLL and MAE in section 4.1 was not defined before
  their use.
- It is a good idea to submit the appendix as a supplementary material
  with the submission, rather than relying on an external web page
  reference.
- The graphs in figure 4 an figure 5 has different y-scales, which
  may mislead the reader at fist look (e.g., giving the impression
  that fig 4 has much more variance).
- The tables should not overflow to the margin.
- There are many more linguistic/typographic issues. A careful
  proofreading is necessary before the paper gets into a publishable
  shape.",19-131.txt
37,https://openreview.net/forum?id=u0UrD4Do_n,0,6,3,"The report focuses on Augmented Neural ODEs (ANODE) by Dupont et al. and gives an overall description of how ANODE is different from NODE (the method that ANODE is based on). The intuition is clear but there are gaps in details (for example it is hard for readers the understand what ""NFE"",  ""augment"" etc. actually mean without reading the original paper. 

The report provides ablation studies, for variables like number of dimension and filter size. There are some details on the motivations behind the results and simple hypothesis tests.

The reports show that the reproducibility of the original paper is good (on smaller datasets. For large datasets the report did not do experiments due to limited computation resources).

Overall the paper reads smoothly with relatively high quality plots, and shows results in a good structure. But the experiments are not very comprehensive due to limited computational resources, which makes it hard to validate all results in the original paper.",19-132.txt
37,https://openreview.net/forum?id=u0UrD4Do_n,0,4,4,"The authors performed an Ablation study of Dupont et al. ANODEs by using the released code. In general, the authors provided a nice introduction and statement of the problem showing their general understanding of the concept. Nevertheless, the text presents several issues that should be addressed:

- There are some typos in the text. 

- In the background section, the authors should keep the pristine notation adopted by Dupont et al.  to describe the mathematical expression. 

- Chen et al.’s (2018) NODEs and Dupont et al. ANODEs articles should appear properly cited in the bibliography section.  
In section 3.3.1 the authors state: “The codes provided by Dupont et al. 2019 were simple to implement and generated results in a reasonable amount of time.” Do the authors mean ‘implement’ or ‘query/deploy’? The code was already provided.   

- Fig. 1 shows the case of “Class overlap” of 30% but this is not mentioned in the caption. Additionally, there is an inconsistency between the colors in Fig. 1b and 2 regarding NODEs (Fig. 1b in orange and Fig 2 in blue) and ANODEs (Fig. 1b in blue; Fig 2 in orange). 

- The authors should provide evidence of all the experiments that they have performed. Therefore, they should show a plot or table showing the experiments with unbalanced datasets in section “Number of points per class:”. Along the same lines, the authors should actually provide evidence of the statements instead of writing “data not shown”.

- The authors should avoid using terms such as ‘quasi-linear’ since it not a quantitative measure and it is purely subjective. It would be better to give a more descriptive explanation. 

- Section “Others” is meaningless without the appropriate evidence. Consequently, it should be removed.  
A crucial part that is missing is the fact that ANODEs use a larger number of parameters due to the extra dimensions so it is natural that it performs better than NODEs. This was already mentioned by Dupont et al. but it is missing in this report. This should be included in section 3.1.2. Therefore, the statement “This finding supports the claim that dimension-augmentation decrease solution complexity.” should be put into proper context since also the neural networks are more complex and it is not a fair comparison. Additionally, Figs. 3-Left and -Center are not clear, and statistical analysis, as in Fig. 2, should be performed to display converged/representative results. 

- Clarify the statement “Generally we find that training accuracy and loss are comparable when using more augmented dimensions”. 

- Unfortunately, the results in Fig. 5-Centre are not conclusive due to the lack of statistical validation. No definitive conclusion can be obtained from it since there is no clear tendency. Therefore, the statement “However, using 75 augmented dimensions required many more NFEs compared to the NFEs required when training with 5 dimensions (Figure 5, center).” should be removed.

- Section 3.2 is, in general, not clear and does not present a proper analysis of the experiments. Figure 4 is not referenced or discussed, only in the Discussion section is marginally referenced.

- The authors should find a better way to plot figures like Fig. 8, since being all the dots completely opaque you can not properly read the information on the plot. 

In conclusion, this text still needs more work. One of the requirements of the Ablation Track is to perform a rigorous ablation study and the authors did it only partially leaving out the image classification part. Additionally, the experiments performed are not clearly described nor analyzed in a proper manner. Consequently, crucial insight on the trends shown by the results are not discussed. Furthermore, a key part in the direct comparison between NODEs and ANODEs was not mentioned nor discussed: the difference in their number of parameters. ",19-133.txt
38,https://openreview.net/forum?id=cs7fGYrVW,0,4,4,"Problem statement: 
The report attempted to reformulate the main contribution of the original paper (Dragonnet architecture and target regularization steps) as well as a brief background on average treatment effect estimation and related work. However, other than small changes in notations in the equations (for example, T for treatment in original paper -> W for treatment in report), the reformulation led to expressions that do not make sense which leads me to believe that the authors may not be familiar with the causal inference literature. These examples appear throughout the paper. For example: ""we consider the average effect of binary treatment"" to mean we consider the average effect estimation of a binary treatment, ""neural nets can help detect treatment effects"", ""applied a deep neural net to solve for causal effects"", etc. 
Given that the report runs an ablation study, it would have been better to synthesis the contributions of the original paper by focusing on the architecture, hyper-parameters of interests and additional steps needed for adding target regularization instead of copying/reformulating sections from the original paper with small notation changes.

Code:
- The authors used code made public by the authors of the original paper. However the authors of the report state that they had to change the test_size parameter from 0 in the original code to 0.0001 in the reproduced experiments to avoid a ""compile error"". This was due to the version of tensorflow used. Can the authors expend on this and provide a list of values for the test_size that would be reasonable for reproducibility purposes?
- Question: in the appendix, the report mentions "" Since the approximately training time for this dataset is around 17
days GPU time, we directly use the trained data she provided us for reproducing their experiment
results for simplicity."" What does 'trained data' mean? Do the authors mean they used the trained model from the authors of the original paper?

Communication with original authors: 
The authors of the report had fair communications with the authors including getting training datasets, answering the authors questions, etc.

Hyper-parameter Search:
The report diligently identified hyper-parameters of interest and ran sweeps across these hyper-parameters. However, for most sweeps, only 3 or 4 values were explored. Given the small size of the dataset used, I would have expected more explorations. Additionally, running cross parameter sweep would have been good to identify the best setting and compare with the default setting from the original paper. 
Questions: for the \alpha parameters (Section 5.2, table 2), the report states that \alpha = 1 leads to better results but the table does not contain results for \alpha = 1. Is this a typo? 


Ablation study:
The report show an experiment adding more hidden layers to the Dragonnet architecture. While the hyper-parameter explorations were extensive, the ablation study is lacking.  

Discussion on results:
(1) The report only used one benchmark dataset (IHPD) out of the two present in the original paper. The reason was running experiments on the ACIC dataset would require GPU resources that the authors did not have. 
(2) Additionally, all the experiments in the report are run on a small sample of the IHPD dataset (50 out of the 1000 realizations from the NPCI package that the original paper used). This choice was also justified by time constraints.   
These two facts make most of the experiments inconclusive. This presented a good opportunity for the authors to discuss the reproducibility of this paper.

Recommendations for reproducibility:
The authors of the report did not provide recommendations to the original authors for improving reproducibility.

Overall organization and clarity: 
- The report is well organized. The attempt to reformulate sections of the original paper made it unclear but the experiment section is clearly written and well organized. Section 2 and 4 could have been summarized to focus more on the experiments and discussion of the challenges they encountered and recommendations for the original authors. 
- It would make sense to switch section 3 and 4 since section 3 refers to the datasets used in the experiments in section 5. 
- Comments: 
    - Citations: please refrain from using first name of authors in citing papers. Instead, use standard citation styles such as <last name of first author> et al +  <year of publication> + link to reference page.
    - Typos: 
        - first line after section 3, observation data -> observational data
        - Section 5, baseline settings, ""implication of TARNET architecture"" -> implementation of the TARNET architecture
        - Section 6, cause 2, ""the authors do not sure..."" -> ""the authors are not sure...""",19-134.txt
38,https://openreview.net/forum?id=cs7fGYrVW,0,6,4,"-Problem statement: The authors do a very nice job of describing Dragonnet, and the problem that it is seeking to solve in the introduction. They also do a commendable job of clearly laying out the aims of the replication in the beginning of the paper so it is plain for the reader. 
-Code: The authors reused code provided by the original work. 
-Communication with original authors: The authors have indicated that they have corresponded with the authors of the original work. 
I did not see correspondence on the open review site, so I assume correspondence was private.
- Hyperparameter Search: The authors do a decent job of looking at the effect of hyperparameter tuning, and indicate that the effect is largely negligable. 
-Ablation Study: The authors do a nice job of looking at the performance of Dragonnet with different aspect being omitted and show that the effect is largely negligible on final performance. 
-Discussion on results: The authors have less of a discussion than I would have hoped for. My reading of this replication is that the results seem to indicate that most of the proposed advantages of Dragonnet do not seem to appear. The provided explanations don’t do a lot to shed light on this–if a causal inference method is susceptible to random seeds and small differences in train / test splits this is concerning given downstream tasks the model may be applied to in the future. It is also a little surprising that the authors chose to use the variability of limiting to 50 replicates as an explanation, rather than just running more iterations. 
- Recommendations for reproducibility: 
* It would be great to see evaluation on a dataset other than IHDP. 
* The authors only compare to other deep learning methods. It would be great to see the comparison to other much simpler models, e.g. parametric models or Bayesian additive regression trees (BART).
- Overall organization and clarity
* Overall I thought the organization of the paper was excellent. 
* The authors do not have a bibliography and use inappropriate citations in more than a few places. References should be given as [last name], et al. not [first name], et al. 
* There are a number of typos strewn throughout the paper. ",19-135.txt
38,https://openreview.net/forum?id=cs7fGYrVW,0,6,4,"problem statement: 
the original paper aims at estimating the average causal effect in presence of no unmeasured confounders. It is known that adjusting for balancing scores such as propensity score is sufficient to identify the average causal effect (under the no unmeasured confounder assumption.) Hence, they propose to estimate the propensity score via neural nets and only use that in the estimation of the regression model. Even though the problem sounds appealing, I strongly disagree with the intuition behind the original proposal. The original authors say ""it suffices to adjust for only the information in X that is relevant for predicting the treatment. Consider the parts of X that are relevant for predicting the outcome but not the treatment. Those parts are irrelevant for the estimation of the causal effect, and are effectively noise for the adjustment. As such, we expect conditioning on these parts to hurt finite-sample performance —instead, we should discard this information."" Conditioning on the propensity score is a sufficient thing to do, however it should not be sold out as the most efficient thing to do as it's discarding the ""noise""; since oftentimes adjusting for covariates that are highly correlated with the outcome would highly improve the variance of the estimator. 
The authors in the replication reports follow the original proposed target of inference and proceed with their experiments. 

Code:
They reused the author repository (this is submitted for track 2 evaluations)

Communication with original authors:
Authors acknowledge communications with the original authors. 

Hyperparameter Search:
In their 3rd task, they modify the hyperparameters to explore how the proposed Dragonnet architecture and target regularization procedure perform on the treatment estimation. The set of hyperparameters that they consider include:
alpha - weight the loss components,
min_lr - lower bound of learning rate,
cooldown  -  the number of epochs to wait before resuming normal operation,
momentum - used in the optimizer SGD, 
and the learning rate. 

Discussion on results:
The discussions on the provided tests are relatively comprehensive. However, some details are missing. For example, the comparison with two other plugin estimators are not well-reported; e.g. TMLE involves a lot of tuning as well and not much is said about it. 
Moreover, there are multiple important comparisons that should have been analyzed more rigorously: (1) estimating the outcome regression using all the covariates, (2) estimating the outcome regression by using all the covariates plus  the propensity score, and (3) comparing the results to stabilized AIPW. 

Recommendations for reproducibility: 
There is no specific recommendation provided. 

Overall organization and clarity:
The paper is relatively well-organized and clearly written. ",19-136.txt
38,https://openreview.net/forum?id=cs7fGYrVW,0,8,4,"The authors 
-- re-use the code published with the original paper
-- experiment with various hyperparameters
-- experiment with various optimizers, and find adammax outperforms sgd with momentum as used in the original work
-- experiment with network depth, thus finding a deeper architecture that is more accurate
-- implement an additional baseline, Nednet.

The authors were able to approximately reproduce the original results.
They discussed reasons for deviations with the original authors, revealing sources of randomness as well as number of repeats as potential causes. 
The results of the replicated method outperform results obtained with the additional baseline Nednet.

The authors suggest further hyper parameter tuning for improved performance. 

I have just one comment concerning clarity: Please cite the original work at the beginning of the introduction. So far, this doesn't happen till the 2nd page. It will make it more convenient to find the original work without searching for the title. ",19-137.txt
39,https://openreview.net/forum?id=aCt-q9sJ_X,0,8,3,"The authors did a nice job in reproducing the performances of the baseline algorithms in the paper ""Primal-Dual Block Frank-Wolfe"" on all the datasets. The authors confirmed that, the PDBFW algorithm by Lei et al. converges more quickly than other algorithms, especially for large data sets that are sparse. However, for smaller data sets, algorithms like Accelerated Projected Gradient may perform better, if the algorithm is implemented differently using a different language (C++ instead of Python). The authors found a proper tuning of hyperparameters for the smaller data sets will improve the performance of some of the baselines, e.g.,  Accelerated Projected Gradient method. The paper is clearly written and the setting of the experiments are described in full details. The discussion about the performance of the baseline algorithms on datasets of different sparsities is interesting.",19-138.txt
39,https://openreview.net/forum?id=aCt-q9sJ_X,0,5,5,"This manuscript investigates the baselines in the paper ""primal-dual block frank-wolfe"", which belongs to the baseline track. Besides verifying the proposed algorithm, the authors implement the python version of it and also other baselines. 
The authors also provide a new baseline Stochastic Variance Reduced Frank-Wolfe (SVRF). In experiments, the authors show that accelerated projected gradient descent may have better empirical performance than the primal-dual frank-wolfe. The hyper-parameters tunings are comprehensive. 

Overall, this is a good paper. My major concerns are
1. The authors do not provide source code for verification purpose.
2. There is no ablation study.

Minor comments:
Typo in formula (1). It should be 
fi(ai⊤x)
.",19-139.txt
39,https://openreview.net/forum?id=aCt-q9sJ_X,0,6,4,"The report identified the contributions in the original contribution as the utilization of the unique structure of sparse matrix solutions, which reduces expensive matrix computations into dimensionless calculations and achieves linear convergence for sparse solutions.

The reproduction is comprehensive: the authors first ran the provided code written in C++ to verify findings in the original paper, then re-implemented all the baselines and the proposed new algorithm (PDBFW) in Python to gain additional insight into the impact of the choice of programming language. Also, the authors implemented a Stochastic Variance Reduced Frank-Wolfe (SVRF) baseline due to its outstanding performance that was discussed in related literature.

The author conducted further optimization tests on the hyperparameters (learning rate) of these algorithms. They were mostly able to confirm relative convergence rates between each respective algorithm. However, they discovered that their Python implementation could significantly speed up the Accelerated Projected Gradient Descent (AccPG) for smaller datasets. To this point, they offered some interesting insights on how PDBFW relies on the speed of running large sequences of nested loops, which runs slower with a Python interpreter. They also identified that SVRF did not perform as well as the other baselines on smaller datasets for a similar reason. 

However, the report only reproduced results on four out of the six datasets used in the original paper, leaving the two larger and sparser datasets untested. There were no ablation studies in the report. However, this might not be necessary for the given subject. It is not clear if there is communication with the original authors for testing reproducibility.

Overall, the report is a solid reproduction. It verified the main contributions of the original paper and gained insight into the performance gap related to dataset size and language choice.",19-140.txt
40,https://openreview.net/forum?id=9jTbNbBNw0,0,7,5,"The report examined various aspects of MelGAN model. The implementation of baseline model is provided by original author. The report investigated the choice of generator, discriminator, optimizer and loss function. 

Max pooling and replication padding were found to improve the model if either is applied alone. Please explain why replication padding ""expand"" the feature compared to reflection padding. Please also use graph to show the difference of the two padding method.

Overall the report offers a better understanding of MelGAN.",19-141.txt
40,https://openreview.net/forum?id=9jTbNbBNw0,0,5,4,"Despite the claim that the report makes about successful reproduction of the results from the paper, I unfortunately don’t find the provided results convincing enough. Despite the amount of work and effort made by the authors of the report, there are some problems that seem to be very hard to address. However,  the report might still be valuable in providing additional insights.

The main concerns are the following:
1. The training time is significantly less. They perform 100 epochs instead of 500 and 3000 epochs (as in the original paper) for each experiment. It is understood, that the cause of this is the limitation of computational resources, but at the same time this renders all results impossible to match the results of the original paper. Additionally, it renders the results of the ablation study questionable, since it is not trained to convergence. There is no way to guarantee that once trained to convergence, all experiments would show the same relative performance. It would be informative to run at least one experiment till convergence (3000 epochs) and compare the result with the one obtained with 100 epochs, but this was not done.  
2. The original paper uses a subjective evaluation metric MOS, which already makes it problematic to reproduce. The original paper used 10 samples, which seems to be a small number for a test set. Instead of conducting surveys, the report has used the PESQ-MOS metric, which is an automatic speech assessment method. While being non-subjective, it is not clear if PESQ-MOS is enough for an accurate assessment of the quality. PESQ was developed for telephony to assess voice quality. Because of this, I have reservations that this metric can adequately be used for assessing the quality of generated speech and of how real/fake speech is. There is no comparison between MOS and PESQ-MOS under the same experimental setup.
3. The obtained PESQ-MOS significantly differs from what is reported in the paper. The paper reports a MOS score of 3.09 for 500 epochs and 3.61 for 3000 epochs. The reproduced PESQ-MOS score is 1.75 for 100 epochs.

It is virtually not possible to draw any conclusion from the report on the accuracy of the reported result in the original paper. Training time is order of magnitude less, evaluation metric is different, and reported scores differ significantly. 

It is also worth noting, that the size of the test set seems to be small, only 10 samples, while I could not find information if the official train/test data split was used, or not. If there is no official split from the original paper, then reproducing results could be impossible, since scores will depend a lot on which samples were picked as a test set. The report should explicitly state whether the official split was used or not. 

However, even for ablation purposes, I think it is still important to understand what causes such a drastic deterioration of the score. Is it due to the limited training time? Is it due to the different metric used? 

Both however can render the ablation less meaningful. If trained to convergence, the scores might differ significantly from those obtained at 100 epochs.

Since PESQ was developed for the purposes of telephony to assess voice quality, it might not capture all the GAN artifacts that perceptually negatively affect voice quality. 

Assessment of the quality of generative models is a hard problem, because of its subjectivity. In computer vision, it is more common to report FID score or Inception score, however I don’t know if there are any equivalents for voice quality assessment.

Since the track of the report is “Ablation”, I assume, that the ablation study is more important in this track rather than accurately reproducing the results. The guidelines are not clear enough on this topic though. 

These are issues of lower relevance:

• The report is written clearly and provides a proper problem statement.  The authors of the report re-used code available from github. The guidelines say that “In any case, codebase should be submitted with readable code/docs”, but the report does not include codebase. I’m not sure about the reasons why codebase should be submitted for the “Ablation” track, the guidelines are not clear about this.
• The report does not mention any communication with the original authors.
• I don’t see a hyperparameter sweep. The report shows a lot of experiments with different architectural modifications, such as varying the number of residual blocks, using ReLU activation instead of LeakyReLU, adding batch normalization, spectral normalization, dropout, symmetric padding and so on. The report shows results for different optimizers and different learning rates.  However, there is one hyperparameter in the loss function - 
λ
, and there are no experiments that show its effect. This hyperparameter is a weight of the L1 distances of the feature maps in the objective. There is even no discussion in the original paper about that hyperparameter, but just mentioned that it was picked to be 
λ=10
.
• The ablation study is very extensive. However, I see some experiments as excessive. 
In particular, the Adam optimizer is the default choice for training GANs, because it was proven to work better than others. Since the main idea of the original paper is not related to the particular choice of the optimizer, I don’t see reasons for trying out other optimizers. As expected, other optimizers perform worse, and this result does not tell us anything new about the original paper.  The same is true for adding dropout, switching leakyReLU to ReLU. LeakyReLU is a standrad setup for the GAN training, so I don’t see reasons for altering it. Other changes, such as adding Batch normalization/Spectral normalization will require adjustments of learning rate, hyperparameters, maybe even architectural changes to make them work properly. It is not clear, if that was done or not. Just adding BatchNorm might deteriorate the score if not tuned. 
I think that even adding Batch Normalization, Spectral Normalization is not that important, since it is not directly related to the proposed ideas in the original paper. Removing weight normalization, varying number of residual blocks, varying number of discriminators, stride sizes, padding, learning rate, testing other feature matching losses are indeed useful studies. 
• The report contains an adequate Discussion and Conclusion sections.
• I don’t see recommendation to the original authors on reproducibility.
• Overall structure and organization of the report is good.

About maxpooling:
• The report claims that maxpooling works better compared to average pooling. This is an interesting finding, though not directly related to the new ideas proposed in the original paper.
• Maxpooling is a popular choice, and it was successfully utilized for a number of tasks. However, for image generation, it was shown (DCGAN), that replacing it with average pooling significantly improves results. It was shown, that maxpooling is responsible for creating specific artifacts in GANs. I didn’t see maxpooling in any recent works on GANs.
• Since the only evidence that maxpooling performs better is PESQ score, which was not designed for evaluation of the quality of synthetic speech, I think that this needs additional research.

Additional comments on ablation study:
• There is a missing experiment for the ablation study: with/without feature matching loss. The original paper has this experiment in their ablation study.
• Varying type of feature matching loss from L1 to L2, and Pearson correlation almost certainly will require tuning the 
\lamba
 hyperparameter, which as I understand, was not done.
• The original paper was claiming that the hinge loss version of the GAN objective was performing better than the vanilla GAN objective and the least-squares formulation. It would be useful to check that claim, since even the original paper was not showing experiments supporting that. The hinge loss is known to work better, as described in “Which Training Methods for GANs do actually Converge?” Lars 2018. It would be interesting to see what effect would have changing the hinge-loss to it’s smoothed version – softplus function and also adding gradient penalty.
• It would be also useful to see the effect of the discriminator window size.

Some comments on explanations to the observed behaviors that the report provides:
• The explanation for why mirror padding and max pooling does not work well together, but works well separate:
“As is discussed in the related work section, pooling techniques aim to ’shrink’ (summarize) the feature locally while padding operations ’expand’ the feature to catch more details of information. These two operations act in the opposite direction, which is why combining them cannot make an improvement.”
might be too far-fetched. It is not clear, why padding necessarily expands the feature (and it’s not clear what exactly is meant by “expanding feature”), that it is not clear why their combination doesn’t work. This is no experiment designed to check this hypothesis ether.
•  In the discussion of Experiment 7:
“The overall performances (other optimizers compared to Adam) are worse than that of the baseline. One reason is that some optimizers (e.g. SGD) relay on the concave loss function to converge to the optimum point,which is not the case in the GANs. “
This explanation is also too far-fetched. Adam is also a type of gradient descent optimization technique. The surface of the loss function is rarely concave (indeed, it is quite far from concave), for the majority of the cases where SGD performs quite well. The reason is that SGD is stochastic, which allows it to overcome local minima. In addition, there is no optimum point for the GAN case, but there is a Nash equilibrium state.
• Explanations for why the Pearson correlation coefficient may not work well are also obscure:
“The reason is that the compacted range of coefficient is between -1 and +1, which is hard to describe the variation of the loss.”
I don’t know what the range has to do with “descriptive capabilities” of the loss. It is unclear what is meant by this, and there is also no evidence shown that it is actually true.",19-142.txt
40,https://openreview.net/forum?id=9jTbNbBNw0,0,3,5,"This review analyzes the ablation study conducted by Zhao et al. of the manuscript “MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis”.

In this review, I will refer to the original MelGAN paper and its authors as the “original paper” and “original authors”, respectively. In reference to the report being reviewed, I will refer to it as “Zhao et al.”.


1. Presentation

There are serious problems with the presentation of the text in the report. In some cases, text and figures are copied verbatim without acknowledgment from other sources, in what can be construed as plagiarism. Some examples follow:
- Figure 1 has no proper acknowledgement. Should be referenced as being extracted from [1].
- Figure 2: Extracted from the original paper (Figure 1) without acknowledgement
- Text copied verbatim or with minor rephrasing, without acknowledging the source:

Text in the Zhao et al.:
Extraction: The initial stage consists of the decomposing waveform into time and frequency using the Short-Time Fourier Transformation (STFT). Then the phase information is discarded leaving only the linear-amplitude magnitude spectrogram. The linear-spaced frequency bins of the resultant spectrogram are then compressed to fewer bins which are equally-spaced on a logarithmic scare (Mel scale).

Text in [1]:
Extraction: The initial stage consists of decomposing waveforms into time and frequency using the STFT. Then, the phase information is discarded from the complex STFT coefficients leaving only the linear-amplitude magnitude spectrogram. The linearly-spaced frequency bins of the resultant spectrogram are then compressed to fewer bins which are equally-spaced on a logarithmic scale (usually the mel scale)


2. Dataset

With respect to the dataset of choice, Zhao et al. say: “...In this experiment, we choose the LJ Speech Dataset, one of the most popular datasets for audio modeling training experiments...”. This justification is somewhat misleading as the dataset happens to be the same one used in the original paper. I would suggest to rephrase the sentence to: “in accordance with what the authors of the original paper did in their publication, we use for our experiments the LJ Speech Dataset”.



3. Normalization

In the original paper the authors mention that the choice of the normalization technique for the generator was crucial. Spectral normalization was one of the methods the authors evaluated, indicating that the results were poor. In Section 3.1 of the original paper, the authors present an ablation study to justify their model decisions and indicate “...Using spectral normalization or removing the window-based discriminator loss makes it harder to learn sharp high frequency patterns, causing samples to sound significantly noisy...”

Zhao et al use spectral normalization in their study and their results agree with those of the original paper. This agreement is not mentioned in the report. In fact, the way it is presented in Sections 2 and 3, it gives the impression that spectral normalization is first proposed in the ablation study of Zhao et al., when, in actuality, it was evaluated in the original paper. 

4. Performance evaluation

In the original paper, the evaluation of the method and its competitors was done using MOS scores computed from 200 individuals.

Zhao et al. use PESQ scores, and this is understandable, as they will not be able to crowdsource the tests to 200 individuals as done in the original paper. This clearly indicates that there could not be any comparison between the results presented  in this report and results obtained in the original paper. Nevertheless, there are statements that I find troubling in the report. For example:
- in Experiment 5 it says: “...and it is noticeable that the model using replication padding technique achieves better performance than the original model…”.
- in the Conclusion it says: “...We also show that our modified models can improve the performance significantly (with replication padding operations on 4 residual blocks)...”
Given that the metrics to report results are different and, as mentioned in Section 5 of this review, the experiments in Zhao et al. are based on a small number of epochs, I would suggest Zhao et al. to steer away from absolute statements such as this one.

5. Experiments

Zhao et al. conduct numerous experiments and present the results in Sections 4.2 and 4.3. They use a very small number of epochs, compared to the epochs used in the original paper, which casts doubts upon the validity of the results. Moreover, the PESQ scores shown in Table 1 through 9 are averages and it would also be useful to also show the standard deviation.

Despite this being an ablation study, the report does not put emphasis in the hyperparameter search, which should be essential. For example, for the optimizer (Adam) only the learning rate was evaluated (Experiment 6). This is insufficient.

Zhao et al never address the results of the ablation study presented in the original paper (Section 3.1). My recommendation is to first replicate the study presented in the original paper before conducting an ablation study of their own. More importantly, give credit to the original authors for the parameters and models that were tested in the original ablation study.

6. Conclusion 

In conclusion, I do not find this report to shed any light into the choice of parameters in the original paper. As an ablation study, in my opinion, is inconclusive. There are serious problems with the presentation of the text and figures (e.g., figures and text copied verbatim from other sources without acknowledgement). As a result of this, I do not find this report suitable for publication. 

7. References

[1] Paarth Neekhara, Chris Donahue, Miller Puckette, Shlomo Dubnov, Julian McAuley. Expediting TTS Synthesis with Adversarial Vocoding. E-print arXiv, 1904.07944, 2019
https://www.groundai.com/project/expediting-tts-synthesis-with-adversarial-vocoding/1",19-143.txt
40,https://openreview.net/forum?id=9jTbNbBNw0,0,7,3,"This report conducts an ablation study of the model presented in the original MelGAN paper.

The authors used the published repository for MelGAN and modified it to conduct a series of ablation experiments. The code to implement these modifications has not been submitted. The authors ablated many aspects of the model, including optimizer choice and optimizer hyperparameters, pooling and padding strategies, normalization schemes for the generator, discriminator losses, number of residual blocks in the generator. Some of these ablations were informed by empirical results from previous related work.

The results from these ablations match some of the original paper comments, as well as overall trends in the field. In particular, results indicate that the original choice of optimizer (Adam) and learning rate, as well as normalization schemes (weight norm), among other choices, were superior to other alternatives. Furthermore, the authors of the report show that increasing the capacity of the model improves results, and they also report positive results using replication padding and max pooling instead of average pooling.

Overall the report is clear and well written and conducts a proper ablation study of the model. The main negative aspect is that the results are only reported for a single dataset and task - it would be interesting to see if the findings here generalize to other tasks and datasets or, on the other hand, are only valid for the chosen dataset.",19-144.txt
41,https://openreview.net/forum?id=j5ScDyYXrN,0,6,3,"1. Problem statement - whether the report clearly states and understands the problem statement of the original paper.

The authors clearly state what they tried to produce. They reproduced a subset  of  the  baseline  experiments and extended the baseline experiments with SMOTE oversampling experiment. They conducted two  groups  of  experiments:   one  groups on IMDB movie reviews and the other one  on  CIFAR-10. 

2. Code: whether reproduced from scratch or re-used author repository.

The original paper released their code. This paper does not explicitly state whether they reimplemented from scratch. I would assume the answer is no since no code was submitted.

3. Communication with original authors
There is no mention of communicating with the original authors for testing reproducibility.

4. Hyperparameter Search

The paper proposed a scheme as stated in Section 6.3.3 for hyper-parameter tuning to improve the baseline results. 

5. Ablation Study

It provides details on how to find optimal stage transition points in DRW. 

6. Discussion on results
The report provides detailed discussion on the state of reproducibility of the original paper for a subset of the baseline experiments.

7. Recommendations for reproducibility
The authors reproduced results from the original papers and achieved slightly better results with more careful hyperparameter search. There is no explicit recommendations for reproducibility except that some hyperparameters are reported missing in the original paper.

8. Overall organization and clarity

The paper has grammatical errors in quite a few places which makes reading difficult. For example ""Our approach it to run the model """,19-145.txt
41,https://openreview.net/forum?id=j5ScDyYXrN,0,6,3,"Problem Statement:
Clear statement regarding the reproduction of the results found in the original paper, with improvements found during hyperparameter search and extensions to the original paper invesigated (SMOTE oversampling and cyclical learning).

Code:
Reimplemented based on the original code. A reference to the new implementation does not appear to be included (would be especially useful due to the new extensions introduced) .

Communication w/ Original Author:
No communication with original authors mentioned or found in OpenReview comments.

Hyperparameter Search:
A new search over hyperparameters found improvements in the baseline algorithms used in one of the experiments (IMDB). Unfortunately, the authors don't include the metrics associated with the original papers proposed algorithm along with the new baseline measures, under the new implementation.

Ablation Study:
No ablation studies have been performed here.

Discussion on Results:
The discussion related to IMDB is interesting given the significant improvement of baseline performance. Would be very interesting to know how LDAM+DRW performed under the new implementation. For the CIFAR dataset, only the 10 class setting is investigated, with baseline results confirmed.

Recommendations for Reproducibility:
No recommendations given beyond the implied recommendation of including the hyperparameter settings for all experiments.

Overall organization and Clarity:
The proposed methods going beyond what was proposed in the original paper are interesting but a clearer separation between reproduction and extension would be helpful. Thinking specifically of the work regarding cyclical learning and SMOTE.",19-146.txt
41,https://openreview.net/forum?id=j5ScDyYXrN,0,6,3,"This paper reproduces baselines about label-distribution-aware margin loss and deferred reweighting schedules. Those techniques are developed for handling inter-class imbalances. In the report, participants achieve the performance of the baseline reported in the original paper, then conduct ablation studies/hyperparameter tuning to inspect the reference paper. Also, participants introduce several techniques to increase baseline performance including cyclical learning rate schemes. Finally, participants conclude the report with a brief discussion.


Official implementation from original authors is in below link:
https://github.com/kaidic/LDAM-DRW

However, I cannot find supplementary materials with codes of participants. I guess that participants implemented codes using Pytorch because original codes are based on Pytorch.

Since codes from original authors are available and this report is on baseline track, it is important to perform plenty of “hyperparameter search” and “ablation”.

Experiments were performed on IMDB and CIFAR-10 datasets. Participants include the following ablation and hyperparameter search for each dataset.

For IMDB dataset:
Baseline reproductions including ERM, RS, RW, and SMOTE.
Hyperparameter tuning for baselines according to the number of epochs, max number of features, batch size and methods. 
Linear model investigation and hyperparameter search.

For CIFAR-10 dataset:
ERM baseline tuning with different learning rate policy (step decay and triangular)
Different data resampling, data reweighting and borderline-SMOTE over-sampling.
Experiments about finding optimal stage transition points in DRW.

There are several comments as follows.

In Table4, why do XGBoost and KNN produce poor results? It seems to absolutely fail to classify. And is there a reason or guess?

Are there experiments about learning rate policy for IMDB dataset? And are there hyperparameter tuning about the number of epochs, max number of feature and batch size for CIFAR-10 dataset?

Ablation studies about the backbone network. (ex. VGG instead of ResNet)

In section 6.3.3, please explain more details about how to estimate reasonable boundary values and how to find the right time for rate decaying. The text is a little abstract, and I would like participants to mention the numbers used in the experiment specifically reported. I hope that the reader who reads the article can reproduce codes without troubles.

Here are my opinions on each criterion.
Problem statement: I think that participants clearly mention and understand the problem of the original paper.
Code: participants refer to original codes. Thus, they focused on hyperparameter search and ablation. Codes for this submission is unavailable.
Communication with original authors: I found one claim from participants in OpenReview site.
Hyperparameter Search: Participants performed several experiments, and also improve original performance.
Ablation Study: There are ablations about the learning rate schedule policy, but it is needed to include more details.
Discussion on results: Brief discussions are followed after each result. Also, there are concluding remarks.
Recommendations for reproducibility: In 6.3, participants point out the author’s hyperparameters are overfitted, then propose a learning rate policy method.
Overall organization and clarity: see the minor points below.

Overall, I feel that this report performed several ablation experiments and hyperparameters searches. Then they achieve better performance than the original paper. However, there are some mistakes in the organization and the scope of experiments is not very big. Therefore, my first score is slightly above the threshold. 

Minor points
It is good not to put reference citations in the abstract.
I recommend to arrange the figures in the order they are mentioned in the texts (in 6.2.1 Table5 is mentioned before Table3). 
In Figure3, there are typos (left-->top, right-->bottom)",19-147.txt
41,https://openreview.net/forum?id=j5ScDyYXrN,0,5,4,"Note: This review follows the structure proposed by the guidelines here (https://docs.google.com/document/d/1Fj4E1IHFBGdXp19A93_sI06SlpPEAw4YMWcFDMAtb5U/edit
)

Problem statement. The original work of Cao et al. tackles the problem of learning in the presence of an imbalanced class distribution.  They propose two techniques that are suitable for this scenario: a margin-based loss which adapts the margin according to the label distribution and a training scheme which defers the classical method of class reweighting until completion of an initial period of unweighted training. In my judgement, the reproducibility report clearly understands both the nature of the problem and the solutions proposed in the original work.

Code. This reproducibility report targets the “Baseline Track” (Track 1). I note that the experiments reported by the authors were based on the public codebase of (Cao et al., 2019): https://github.com/kaidic/LDAM-DRW. However, I was still expecting to see code that implements the experiments in this report and was not able to find a link to it. As a consequence, I can’t comment on the code. Perhaps the authors could provide a link (or justify its omission)?

Communication with the original authors. N/A

Hyperparameter Search.  The authors conducted considerable hyperparameter searches to find optimal settings for the baselines in both the IMDB and CIFAR experiments presented in Cao et al.   

For the IMDB experiments, they determine that model performance (following dataset cleaning) is highly sensitive to factors such as batch size and the maximum number of features used. Strikingly, they show that with appropriate choices the baseline can outperform the LADAM-DRW method (as reported in the original work). They further investigate an additional baseline for handling class-unbalanced data (SMOTE), and additional model classes beyond the BiLSTM used in the original paper (interestingly, they show that logistic regression can outperform the BiLSTM in this setting). However, LADAM-DRW is not included in the methods that receive further tuning (it may also benefit from such a search).

For the CIFAR experiments, the authors similarly demonstrate that there is room for improvement across all baselines. Nevertheless, they also show that LADAM+DRW maintains its edge over other algorithms. These results provide compelling evidence in support of the claims made by Cao et al.

In addition to the above, the authors go beyond the experiments of the original paper and further explore different optimisation schemes. They provide concrete suggestions (Sec. 6.3.3) for improving baseline performance, describing the strategy by which they determined hyperparameters.

Ablation Study. The authors reproduce the ablations of the original work (LDAM only, DRW only) together with the full method (LDAM+DRW). The results reflect the findings of the original paper. 

Recommendations for reproducibility. It is noted (Sec. 6.2.1) that the not all hyperparameters are supplied for the reported baselines for the IMDB experiments. Cao et al. do provide fairly extensive implementation details for the other datasets they use in the appendix of their paper), but it would certainly be useful if they were to do so also for the IMDB experiments.

Discussion on results. The authors provide a good discussion of the implications of their findings.  In summary, they have successfully reproduced the findings of the original work on the CIFAR-10 benchmark and demonstrated significant sensitivity to hyperparameters on the IMDB benchmark.

Overall organization and clarity.  Overall, the paper is fairly well structured and clear. There are a few minor formatting issues which can be easily fixed: (1) Most references do not contain the year or venues of publication; (2) The font size in many of the figures (e.g. Fig. 7) could be significantly increased to improve the readability of the legend/axes.

Recommendation.
If the authors provide code to support their report, I would be happy to upgrade my score to accept (without the code, it seems inappropriate to recommend acceptance)",19-148.txt
42,https://openreview.net/forum?id=ALHafSyaLs,0,6,4,"This paper tries to verify the study in the paper entitled “Adversarial Examples Are Not Bugs, They Are Features”. The paper has good introduction, and discussion of related work. Also, the main claims and key components of the original work are recited. 

Authors verified one of the two types of results in the original paper, only about the robust feature part, not including the non-robust feature part. However, it is still a valuable attempt.  Authors verified on various deep neural networks, and evaluated the impact of learning rate. More importantly, authors pointed out that the L-2 norm loss function in the original paper can be replaced by an L-inf norm one, due to the better performance observed when comparing L-2 norm and L-inf norm results.

Last, authors are suggested to carefully check the writing errors, such as:
Previous studies has shown    
a comprehensive understanding on adversarial examples are truly necessary   —— is 
Andrew Ilyas et al. prompts to view 
which aims to decreases the correlation 
can prevent a classifier from from 
both both robust and 
As the results displayed in Table 4, generally speaking, models with Linf -norm reach a higher accuracy  —— Table 4?  cannot find the comparison between L2 and L-inf in Table 4. ",19-149.txt
42,https://openreview.net/forum?id=ALHafSyaLs,0,4,4,"The report describes the replication of the robust dataset experiments from the original paper, as well as an extension to alternative classifier models.

The original NeurIPS paper has been widely discussed in the community, so reproducing this paper and shedding additional light on the phenomenon is an important task.

The write-up contains numerous experiments, including on additional classifiers. However, the results seem somewhat inconsistent with the original paper (see comments and questions below) and without a detailed discussion about the discrepancies, it's unclear what the take-away is. Overall, I would have liked to see more parallels between the current experimental setup and the original paper in order to make it clear which results are expected to be the same and which are not. See below for specific comments.

* Clarity:

The writing is generally clear, but the write-up follows closely the original paper. In particular, much of the write-up is a summary of the paper, rather than describing the motivation/scope of this particular study. E.g. the experimental section focuses only on the performance of models trained on robust datasets, and the detailed description regarding disentangling features in section 3.2.1 creates confusion (to my understanding Figure 1 is not being reproduced).

* New robust dataset:

The authors mention in section 4.2 and then again in the conclusion that a new robust dataset is being constructed for this report. Is the robust dataset the same as the original dataset? Section 3.2.2 describes the construction of both \mathcal{D}_{det} and \mathcal{D}_{rand} from the original paper, but the experimental section mentions a single dataset, and it’s unclear which one. Given that the two datasets result in significantly different results (see e.g. Table 1 in the original paper), I think this is important to mention.

* Evaluation setup:

The ResNet-50 should be the same as the original paper (including hyper-parameters), is that correct?

Can you confirm whether the test set with epsilon = 0 matches the standard test set? (e.g. does the first row in Table 1 correspond to models trained on various robust datasets but evaluated on the same test set as the original paper?)

Are the accuracies reported in the various tables comparable?

* Results:

The result in Table 1 for the classifier trained and tested on the standard data is lower than the original paper (75.84% vs. 95.3%). This is attributed to running training for less iterations due to computation reasons. While this is understandable, it's unclear how to interpret the resulting comparisons in this regime.

Table 1 also contains results on the standard test set when training on a robust dataset (row #1). In this case, training on a robust dataset increases performance significantly (to over 96%). Instead, the original paper reports a drop in performance (to 63% and 43%) when training on the robust datasets. Could you discuss this discrepancy?

Table 4 compares results across classifier models and DenseNet-121 obtains an accuracy of 90.49%, much higher than the alternatives in the same table, but lower than accuracies reported in other tables (and lower than reported in the original paper). Can you discuss this?

* Minor comments:

The determinant or article (“the”, “a”) is occasionally incorrectly dropped, e.g. “in non-robust dataset”, “for normalization approach”, “find potentially better classifier”, etc.",19-150.txt
42,https://openreview.net/forum?id=ALHafSyaLs,0,6,3,"Problem Statement: The authors provided clear problem statements and introduction to the original paper. The authors reproduce a Residual Neural Network (ResNet) classifier baseline on CIFAR-10 dataset and some ablation study.

 
Hyperparameter Search & Ablation Study:  The authors conducted hyperparameter search with model hyperparameters, such as learning rate, value of adversarial perturbation and normalization approaches as well as different architectures. It would be better to have more various kinds of ablation study to further strengthen the paper.
 

Discussion on results & Recommendations for reproducibility: The authors provided discussions on the reproduced results and gave recommendations for reproducibility.


Overall organization and clarity: The paper is in general well-written and easy to follow.",19-151.txt
42,https://openreview.net/forum?id=ALHafSyaLs,0,5,4,"Summary: the authors reproduced on CIFAR10. However, it authors only experimented on CIFAR10, not ImageNet. As CIFAR10 is much smaller scale compared to ImageNet, I am concern this work is not enough to confirm the reproducibility of the original paper.

Problem statement: authors did a good job in problem statement in the introduction and related work section.

Code: authors ""designed, implemented and evaluated "" several extensions  (e.g. VGG, densenet, ...) of the original paper. It is not super clear to me if authors rewrote the code from scratch, or used the code from other source.

Communication with original authors: there is no communication results with the original authors.

Hyperparameter Search:
1. Authors experimented with different epsilon (a threshold in robust dataset construction) in 0, 0.25, 0.5, and 1.0 respectively.
2. Also authors experimented with different learning rate (0.01, 0.05, 0.1, and 0.5).
3. Have authors tried even larger learning rate, given among all LRs the largest one achieved best performance across all dataset?

Ablation Study:
1. Authors experimented some model hyperparameters, such as learning rate, value of adversarial perturbation and normalization approaches.
2. Authors also designed new models (VGG, Densnet, inception V3) as the extensions of reproduced paper.
3. Authors provided new robust dataset including adversarial examples (for a ResNet-50 through generating untargeted adversarial examples on the original CIFAR-10 dataset).

Discussion on results: 
1. ""Due to the challenges of computational resources, our ResNet-50 classifier could only be trained within the limited iterations and reported 75.84% accuracy in the standard classification."" so my understanding is the original results are not fully reproduced?

Recommendations for reproducibility: I didn't find clear recommendations for original authors for reproducibility.

Overall organization and clarity: 
1. Author may cite the performance metrics in the original paper for easier comparison.
2. Also when compared different networks, authors may consider presenting with the plot like figure 3 in the original paper.",19-152.txt
43,https://openreview.net/forum?id=KnSLcAe_qC,0,3,4,"In this report, the authors reproduce the experimental results of ""Solving interpretable kernel dimension reduction"" by Wu, Miller, Chang, Sznaier, and Dy (WMCSD). The key contribution of WMCSD is theoretical, so it is unclear whether this was a good paper to choose for the reproducibility challenge.

The experiments conducted by the authors are rather small experiments that took minutes to run. The original authors WMCSD released well-documented code, but the authors neither acknowledge this fact nor do they release code of their own. Without any explanation, I am assuming the authors used the code of WMCSD and repeated the experiments on similar but different datasets.

The writing quality is not good. There are many grammatical errors and (despite the short length of the manuscript) the paragraphs are wordy. Furthermore, much of the introduction's content overlaps (paraphrased) with the introduction of WMCSD, although I understand that this should be necessary to some extent. In the end, there is very little novel content or discussion offered by this manuscript.

I do believe that the authors will have learned about the iterative spectral method through this project. However, I do not think this report provides much value, if any, to the ML community. I do not think the work reaches the standard outlined in the reviewer guideline. For this reason, I recommend rejection.",19-153.txt
43,https://openreview.net/forum?id=KnSLcAe_qC,0,7,4,"Kernel methods for support vector machines are one of the most popular method in reducing the data dimension. They map the data to a higher dimensional space to capture non-linear relationship among features. The problem, however is that features are not interpretive in higher dimensional space. “Interpretive Kernel Dimension Reduction” method was proposed to resolve the issue, which requires computational intensive non-convex optimization. Iterative Spectral Method (ISM) was introduced to resolve the issue, but it provides theoretical guarantee only for the Gaussian kernel in alternative clustering. The paper at NeurIPS 2019 by Wu et al. extended ISM to include an entire family of kernels, while it can be applied to supervised, semi-supervised, and unsupervised learning paradigms. In their experiments, they showed that their method is faster with lower cost compared to baselines.

This paper reproduced their work and performed some experiments using various kernels  and learning paradigms on different datasets (no in the original paper). They run the experiments on four datasets, two from UCI, two subreddits, and two others from original paper. The kernels they used were: Linear, Polynomial, Squared, and Gaussian. They confirmed the accuracy and correctness of claims of original paper.

They also noted one limitation of the original paper, which is the reduction in test accuracy for reddit data since kernel tricks significantly reduced the features dimensions of the Reddit, and consequently the reduction in accuracy.

- Problem statement: the paper was well-written and they state and understands the original paper.

- Code: It seems to that they wrote the code from scratch, but it was not mentioned specifically in the paper.

- Communication with original authors: nothing in the paper was mentioned regarding the communication.

- Discussion on results: they explained very well the reproduction process. 

- Overall organization and clarity
Grammatical issues:
* These two sentences are connected: “After a series of cautious experiments and reproducing. We have confirmed the correctness of their work, hence validate the newly established baseline.”

* Change “break though” to “breakthrough” in abstract

* Put space between word and parenthesis: “Iterative Spectral Method(ISM)”

* Change ’lose’ to ‘loss’ in sentence: “We supposed that this lose was due to the low mutual information nature of the dataset used.”",19-154.txt
43,https://openreview.net/forum?id=KnSLcAe_qC,0,1,5,"The authors performed the task in the _Replication Task_ not the _Baseline Track_ as indicated in their submission, they implement the new technique proposed by the authors. The author's only vague adhered to the official task description: ""here, your role is to perform exact replication of the presented main results of the paper.""

In the NeurIPS paper the authors perform experiments on the Wine, Cancer, Face, and MNIST datasets. In the reproducibility report the authors state that they perform experiments on Abalone, Iris, Reddit, Wine, and Cancer. So the only datasets the original paper and this report share are the Wine and Cancer datasets.  The report authors give no numbers regarding their experiments with the Cancer dataset instead simply stating ___""[T]he result shows that their method pretty much fulfilled their claims in terms of dimension reduction and time efficiency, as most of the result we get from the wine and cancer data set shows similar NMI rate as they claimed.""___ The report contains no numerical values for the experiments on the cancer dataset. 

In the original NeurIPS submission the authors considered a supervised and unsupervised setting with 5 different kernels and report objective costs, runtimes and classification accuracy or Normalized Mutual Information (depending on supervised or unsupervised setting). In the reproduction report the authors only include the unsupervised setting for Wine and only report the training time and the NMI. The training time has no units, so it is unclear what these values mean. This leaves only a tiny portion of the report results for one dataset in common with the NeurIPS paper: the NMI values for the Wine dataset.  The NMI results in the reproducibility report match the paper results EXACTLY.  Interestingly the reproducibility report contains no NMI values for the other datasets where the authors instead report HSIC values (no HSIC values were included in the original paper). I find this exceedingly suspicious. I'm wondering if the report authors had a hard time estimating NMI since entropy estimation is nontrivial and they simply copied the value.

There are many other issues with this report, but the above reasons are enough to reject.",19-155.txt
43,https://openreview.net/forum?id=KnSLcAe_qC,0,6,4,"I am glad the authors of this report were able to reproduce and verify the results of the original paper. I have a few comments on the type of other experiments and information that could have improved the reproduction process and in the end I talk of the exposition of the report.

- Regarding the paragraph below Table 1:  From 5 datasets, why 3 of them are introduced but Wine and Cancer left as a column in Table1 without mentioning in the paragraph
.
- Tables 2 and 3: 
     - The first two rows look redundant, maybe the dimensions could have been moved to the caption.
     - It is not clear from the report why we only need such information only for two datasets (out of 5).
     - I am assuming the row ""Train time"" refers to the dimension reduction step. It would have been great to report a comparison of train time with the full space vs dimension-reduced space. So we see the advantage or dimension reduction in terms of time too.
    - Two rows of accuracy are included in these tables, and it is mentioned that these accuracies are low. I wondering if the accuracies were low even using a regular SVM without dimension reduction, or this phenomenon happens exactly due to the dimension reduction.  It would have been great to include (train and test) accuracies without dimension reduction for comparison.
- Regarding the sentence:  ""we suppose the bias may due to the difference in experiment devices."": Did you try to figure out if the differences in all times are a constant factor? or the relation in the difference of time measurements can be associated with something other than device difference: 1) too few runs for the experiments 2) Difference in the implementation of code (Especially the bottlenecks).
Most importantly: You and the original paper need to mention the platform you did the experiment: processors, memory, OS, programming language and package versions, etc.

Comments on the write up: There are lots of typos and grammatical errors in the text. Even though these issues make it harder to read the paper, I am confident I was able to follow the paper and understand the process. Also, this paper's writing was better than another paper I reviewed for this challenge.
Overall, I appreciate the authors of this report for participating in this challenge and working hard in reproducing the results of the original paper. I am leaning towards the acceptance of the paper.
 ",19-156.txt
44,https://openreview.net/forum?id=KfxtCg8sRr,0,5,4,"(Apologies for the delay in submitting the review)

Firstly, I would like to thank the authors of the report for participating in the challenge and making the effort to reproduce the results from the original paper.

The report tries to replicate the results of the original paper where the authors try to tackle image synthesis tasks using a simple classification framework trained in a perturbation-robust manner. The original paper operates on the empirical evidence that robustness to adversarial perturbations contributes to human-aligned saliency to some extent. The authors of the report try to reproduce two specific set of experiments from the original paper -- (1) image-synthesis and (2) image-inpainting.

- The report is not very well-written which makes it harder at times for the reader to understand the exact experimental parameters used. For instance, section III. B, mentions -- “The model we constructed with 100 epochs ...on the test set. It reaches 89.6 % and 95.7 % accuracy … with 0.8 budget.” It is unclear here what perturbation budgets on the test-set do the two accuracies correspond to. 

- Please refer to the original work as “X et. al’s paper/work…” instead of “X’s paper/work”

- Compared to the results in the original paper (Table. 1, page 5), it seems the authors observe lower inception scores (Table II) using ResNet on the CIFAR-10 dataset. Coupled with this is the observation that VGG-13 inception scores are higher compared to ResNet and qualitative samples from the former appear to be much sharper compared to the latter. I’m slightly confused about the takeaway from the report -- would the authors claim that they were unable to reproduce the results for the class-conditioned image-synthesis task from the original paper?

- I like the fact that the authors provide some evidence regarding how lesser iterations combined with smaller perturbation budgets can lead to better reconstruction for smaller images for the image-inpainting task and further discuss the results of the reconstruction.

In summary, while the paper provides a few interesting insights -- especially w.r.t. artifacts in the reconstructions for the image-inpainting task -- there are a few flaws associated with the report. Firstly, it’s unclear whether the authors were able to reproduce the results for the image-synthesis task. Secondly, the report only considers 2 of the 5 major image synthesis tasks explored in the original paper (ignoring the image-translation task as the authors claimed they did not have access to the dataset and were limited by compute resources). Finally, the report could have been written better by clearly highlighting the take-aways from each of the reproducibility experiments. The above discussed points form the basis of my rating.",19-157.txt
44,https://openreview.net/forum?id=KfxtCg8sRr,0,5,3,"I would like to congratulate the authors upon successfully participating in the reproducibility challenge and preparing a comprehensive report detailing their approach and findings. Although the report aims to replicate part of the experiments from the original paper, the authors described the idea, intuitions and methodology in detail. However, there are certain shortcomings of the reproducibility report that need to be addressed in order to improve the overall quality of the work. 
Minor comment: I feel that the report better fits into the ""Replication track"" instead of the ""Baselines track""

Below are listed the various parameters on which the report was judged:
1. Problem statement - The authors exhibit a clear understanding of the problem and provide a detailed explanation of the same. I believe this report adds to the problem description of the original paper, ultimately improving the reader's understanding of the paper.
2. Code - From my understanding, the authors replicated the image generation task from scratch (using VGG-13 model) and reused the available code for the image inpainting task. However, the code base used by the authors is not provided and it is difficult to rebuild or extend their work. In addition, the numbers in Table 2 do not match exactly with the numbers provided in the original paper Table 1 (4.244 vs 7.5 for Resnet). The authors briefly mentioned that it was because of the hyperparameter settings, but do not provide results of any hyperparameter search. 
3. Communication with original authors - There is no mention of communication with the original authors in the report. I feel that the discrepancy in the Inception score for Resnet image generation could be taken up with the authors to clarify and comprehend the underlying reason. In addition, the authors mentioned that training the robust classifier from scratch took significantly longer than using the robustness package. I feel the authors could have contacted the original authors regarding this issue to ensure that their PGD implementation was not missing any tricks that were required to optimize the training. 
4. Hyperparameter search - As already mentioned, the results of a hyperparameter sweep would add to the quality of the reproducibility report. The authors could have also tried varying the seed selection (as hinted in the original paper), but mostly I would have liked to see the variability in results with changing hyperparameters. A plot or table summarizing these results would be ideal. 
5. Ablation study - Although there is no significant scope for an ablation study in this paper, I was impressed by the authors' explanation of the 2 cases in image inpainting (Fig 9). Maybe the authors could think of further investigating this issue in future work, possibly choosing datasets where the background is also labeled (eg. segmentation tasks).
6. Discussion on results - The authors do a commendable job in explaining the results and error cases in each task. However, the arguments in their present form are still loose and need to be further grounded. I understand that the timeline of the challenge may not have been ideal to investigate these issues. Furthermore, the report currently misses a detailed explanation of the results that were reproducible from the original paper. A table indicating the same would improve the usability of the report.
7. Recommendations on reproducibility - Currently, the authors do not add any recommendations for improving reproducibility. I feel this stems from the fact that the authors haven't explicitly mentioned which aspects were reproducible. Adding these recommendations, based on their personal experience, would hugely increase the impact of their report. 
8. Overall organization and clarity - I thoroughly enjoyed reading the report and it added to my understanding of the problem being addressed. However, I would request the authors to revise the grammar and writing of the manuscript to aid readability. They may consider using grammar check software in order to do the same.

Overall, I feel the authors did a commendable job in reproducing part of the original paper. However, there are significant shortcomings that I have highlighted in my comments above. I would be happy to update my ratings if the authors can address some of the concerns highlighted. ",19-158.txt
44,https://openreview.net/forum?id=KfxtCg8sRr,0,5,4,"This submission successfully reproduces the results from ""Image Synthesis with a Single (Robust) Classifier” (Santurkar et al.), which demonstrates that an off-the-shelf adversarially trained model can be used to perform a variety of image synthesis tasks. The authors perform PGD adversarial training of a VGG-13 classifier and use it for image synthesis and inpainting. The synthesis results are plausible but oversaturated, but arguably no worse than those shown by Santurkar et al. The inpainting results show visible artifacts, but this is also true of the inpainting results in Santurkar et al.

There are a few issues that make me hesitant to accept this submission, although for the most part they can be easily resolved.

1. The reviewer guidelines indicate that code should be provided, but the authors have not supplied any code. Since the work in one place refers to “notebooks,” it seems like this may simply be an omission of a link, but I cannot accept the submission unless code is provided.

2. The hyperparameters used to train the CIFAR-10 network are not specified. Moreover, the classifier in Fig. 1 (really a table and not a figure) is said to take a 224x224 RGB image as input, but the dataset is 32x32. Ordinarily one would adjust the number of downsampling operations in an ImageNet network before applying it to CIFAR-10; it’s not clear if this was done.

3. The authors claim that their VGG-13 model achieves a higher Inception score than the ResNet from Santurkar et al. However, Santurkar et al. report an IS of 7.5 for their model, which is higher than the score of 6.7 reported for the VGG-13 model presented here. This submission reports an IS of 4.2 for Santurkar et al.'s model and does not note or discuss the discrepancy.

4. In some places, the report seems to present speculation as fact. The authors suggest that the failure of inpainting when the corrupted region is not part of the object is related to the fact that the inpainting objective only maximizes the conditional class likelihood and the background doesn’t matter. This hypothesis seems plausible and is interesting speculation, but the claim that inpainting performs worse on the background is demonstrated only through examples, not quantitatively. Assuming the effect is real, further work is still necessary to demonstrate that the problems arise from the loss used for inpainting. Additionally, when the authors compare VGG and ResNet in the conclusion, they suggest that visual differences between ResNet and VGG are related to properties of residual connections. Again, this is possible, but not at all proven in the current work. The authors do not need to remove this speculation, but should clearly delineate it from what is rigorously demonstrated.

5. The writing contains many grammatical errors and could benefit from extensive proofreading. Some examples:
 - “the field of image processing changed revolutionary”
 - “They implemented adversarially robust classifier”
 - “the steps is”
 - “Projected gradient descent (PGD) approach,a” (missing article, missing space after comma)
 - ““We used the default hyperparameters and a 0.3 for PGD”
 - ""we decided to illustrate our scratching based on MNIST database” (what is “scratching” here?)
 - ""The key for reconstructing smaller images is to apply fewer iterations and smaller perturbation budget while optimization using PGD”
 - “Clear images see in notebooks”
 -  “In another word,”

Minor:
- This paper appears to have been submitted to the wrong track. It is listed as part of the baseline track, but it appears that it should have been submitted to the replication track.
- The author list on OpenReview is ordered differently than the author list in the PDF.
- Authors of the work being reproduced should be referred to as ""Santurkar et al.,"" not ""Shibani,"" ""Shibani Santurkar,"" or ""Shibani's team.""
- Eq. 2 and Eq. 3 are the same. The submission should just reference Eq. 2 instead of repeating it.
- After Eq. 3, the authors discuss optimization of 
θ
 without explaining their notation. Presumably 
θ
 refers to the parameters of a neural net, but this should be stated explicitly.
- argmax should be written as 
argmax
 not 
argmax
. It's also not clear what argument the argmax is taken over.
- The results in Table I refer to an MNIST classifier that was not used for other experiments in the paper, which seems superfluous. Additionally, the experimental setup is not fully specified. The training hyperparameters are not specified at all, and the difference in time between training on CIFAR-10 and MNIST suggests that different architectures were used for these datasets.
- The discussion of Danskin’s theorem does not seem relevant to the results.
- The authors should specify somewhere that the are measuring the perturbation with respect to the 
ℓ2
 norm, which I assume is what they did given the perturbation sizes.",19-159.txt
44,https://openreview.net/forum?id=KfxtCg8sRr,0,7,2,"This paper reproduced the image synthesis model proposed by the ""Image synthesis with a Single (Robust) Classifier"" paper. The original paper introduced the robust classification objective function and its solver for image classification, and used the representations learned from this model for image synthesis.

This paper validated that the authors successfully reproduce the experimental results and trained the image synthesis model, even when the authors uses VGG, which is a simpler model than ResNet, which is used in the original paper. 

Minor comments:
This paper contained many typos and errors. The authors should check the whole paper again and revise the mistake before publication. For example,
- Citation in the reference section should contain the journal/conference names if exists. Pdf link is not sufficient.
- In Section III. A. 2), some sentences lack necessary empty characters.
- In formula (5), the first * is not necessary.
- In formula (5) and (6), the argument of the argmax should be explicitly denoted.",19-160.txt
45,https://openreview.net/forum?id=0LtI-ryeW8,0,4,5,"The authors of this report reproduce a few figures from Zhou et al,
and find the their results correspond with those of the original
paper.

The authors show that they have some understanding of the original
paper, and reuse the originally provided code, rerunning experiments
as-is, with new random seeds. No other hyperparameter search or
ablation is done. While the report confirms that the original
experiments can be rerun and the results reproduced, there is little
discussion on the impact of the results of the original paper, nor any
recommendations or additional insights about the original method.

An ablation study is typically understood as removing some parts of a
system (that has many parts) to see how the removal affects it. No
such procedure is performed in this report (in the ablations track).

Just to be sure, googling 'ablation study' yields similar definitions
of ablation, such as in:
https://stats.stackexchange.com/questions/380040/what-is-an-ablation-study-and-is-there-a-systematic-way-to-perform-it


Additional comments:
- ""The most straightforward one is probably the one saying that
  weights with small magnitudes should be pruned"" This isn't as
  obvious as may first appear, and not always true. A large number of
  papers have investigated this question and could have been cited
  here. Perhaps look at the references within the papers you already
  cite.
- Table 1: ""the larger this value is, the less likely it is to be
  pruned"", this isn't quite accurate, as the pruning is deterministic,
  except for tie breaking, from Zhou et al: ""Rank the weights in each
  layer by their scores, set the mask value for the top p% to 1, the
  bottom (100−p)% to 0, breaking ties randomly"". It should be restated
  in your analysis exactly how masks are generated, since it is a
  fundamental mechanism at play here.
- 1.3 ""In the past [..]"", citations here would be nice. Again, looking
  at references within the lottery ticket papers should provide you
  with plenty.
- Figure 3: ideally you should reuse the same colors as in Figure
  2. Why do only some lines have a standard deviation? If the variance
  is very small and not visible on the figure it should be mentioned.
- 2.3 It's nice to go dig into the paper's code, but why do you think
  they omitted those results? Some speculation could be
  informative. What is different between magnitude increase and simply
  having a large final value?
- It's not clear whether you simply reran the original code with new
  initial weight seeds, or if you also changed the
  hyperparameters. This should be very clear, and could be for example
  reported in the form of a table.
- writing could be improved: in long sentences where many things are
  being referred to, it isn't always clear what ""it"" or ""they"" refers
  to. It's ok to reuse technical words in a scientific essay, since it
  then leaves no doubts as to what is being said. There are also some
  grammatical errors and typos that make reading a bit harder.",19-161.txt
45,https://openreview.net/forum?id=0LtI-ryeW8,0,3,4,"# Pros

The proposed ablations studies are reasonable, e.g. the effect of the sign with magnitude increase mask criterion. 

# Cons

## Writing

The paper's description is confusing. For example, when ""these"" authors refer to ""this paper"", I don't know what is ""this paper"" -- the original NeurIPS paper or this ablation study paper. 
Also, it seems that this ablation study paper does not use CIFAR dataset at all, then why would they include the CIFAR sample images in Figure 1?

When this ablation paper demonstrates Figure 2 (blurred) from the original paper, I don't know why they do not use the original PDF file provided in the arXiv link. 

## Only MNIST and MLPs?

The ablation studies are only done on MNIST with MLPs, while the original paper with code is dealing with more realistic tasks. This makes this whole ablation study report not very useful. 

When they claim ""....would require 4 weeks to complete"", they did not describe the hardware they are using for this ablation study. ",19-162.txt
45,https://openreview.net/forum?id=0LtI-ryeW8,0,7,5,"This report reproduces part of the experiments in a previously presented paper “Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask”. The report extends the original experiments by studying the performance of including sign effect in masking criteria with good performance. The report also evaluate the statement “masking “is training” under fully converged and not fully converged models. From the experimental results in this report, it seems the results reported by the original “Deconstructing Lottery Tickets” can be successfully reproduced while introducing more interesting findings. 

Pros:
1. This report is a complete reproducible study on a recently proposed model compression study, though results on large datasets and models aren’t reproduced due to limited computation resources.
2. The methodology and detailed experimental setup used in the reproducing process are clearly discussed in this report. Experiments are conducted for multiple trials and averaged results and error bars are included.
3. The report is well-written. 

Cons:
1. It seems the results in the original paper can be reproduced. But how large are the deviations from the results in this paper to the original paper? It would be valuable to include the deviations.
2. More discussions on the results of Figure 4 can be added. In the Section 2.2 of the current manuscript, the discussion is more like summarizing and reporting the observed results. The authors are expect to discuss more on what effect is introduced when combining “diff/same_sign” with “large_final” and to provide the potential explanation.

Minor Comments:
1. The legend for the dotted line (using star marks) seems to be missing.",19-163.txt
45,https://openreview.net/forum?id=0LtI-ryeW8,0,7,3,"The authors have well understood the paper [1] on which they are presenting their reproducibility report. In the introduction, they clearly state the main concepts from [1] and highlight how they differ from the original work on LTH [2]. There are four components which are explicitly enumerated making it easy for the reader to understand what is the target of this reproducibility report. In addition, the authors discuss the difference between pruning and masking in detail.

The reproduced results presented have been generated by re-running the original code from [1] and sweeping various parameters. First, the different masking criteria from [1] are explored and experiments with varying seeds are performed (5 runs per criterion/experiment). The obtained results concur in general with the findings from [1]. Finally, a statement made in [1] that under some circumstances (supermasks) 'masking is equivalent to training' was considered and the authors attempted to verify if that is the case. It is found that this is indeed the case provided the weights have converged to a reasonable accuracy.

Overall, this is a very good report where the authors re-produced several results from [1] and provided a satisfactory discussion on those. One request I would ask to the authors is to include the plots in vectorized format. Since a lot of very important information is contained in those plots, it is important that they do not be pixelated.",19-164.txt
46,https://openreview.net/forum?id=6C1s-vGlAC,0,5,3,"The report validates the results published in ""Making AI Forget You: Data Deletion in Machine Learning"". The code to reproduce the algorithm introduced in the original paper is available online. The report is in the ablation track. There are two ablations studies (number of iterations, tree depth) and one algorithmic modification covered in the report (weighted version of DC-k-means).

Problem statement  :
The paper clearly states which ablations studies it is going to run: number of iterations and tree depth. 
Code  I could not find the code to reproduce the results. However, given that the paper is in the ablation track, I find this less important. 

Communication with original authors:
  There is no mention about communication with the authors. 

Hyperparameter Search:
  The report uses the hyperparameters reported in the original paper.  

Ablation Study:
  The report provides two ablation studies by changing either the number of iterations of the depth of the tree. The motivation behind the ablation studies is a bit unclear, adding it to the report would make it stronger. The report also adds an algorithmic modification (weighted version of DC-k-means). The results of ablation study and algorithmic modification would be stronger with some form of hyperparameter search to ensure that the selected hyperaparameters are optimal for new model configurations. 

Discussion on results  :
The paper provides large number of experiments that include reproduction of the originally published results, ablation study and algorithmic modification. However, the abundance of plots and tables makes the report hard to follow. Moreover, the analysis and discussions of the plots is rather limited. I would encourage the authors to strengthen the discussion of the results.  

Recommendations for reproducibility  :
The authors reproduce the results of the original paper using the original code. Although the differences looks small, it is hard to say if they are statistically significant. Running statistical significance tests might make this part of the experimental section stronger.  

Overall organization and clarity:
The report is well structured. However, the writing could be improved.",19-165.txt
46,https://openreview.net/forum?id=6C1s-vGlAC,0,3,5,"# Problem statement
The problem was explained well enough, but it was difficult to understand how the reproduced models were addressing it.

# Code
Based on Section 7, the models were re-implemented from scratch. There is no link however to any code.

# Communication with original authors
There was no mention of contact with original authors, and no comments on OpenReview.

# Hyperparameter Search
The hyperparameter exploration was very limited. Only 3 different values of the number of iterations were tested and 3 different depths for the model. In original paper, they always set the number of iterations to 10, while reproducibility attempt was on 10, 50 and 100. They did not investigate the hyperparameters \epsilon, d and \gamma for Q-k-means, as well as w for DC-k-means. 

# Ablation Study
There was unfortunately no ablation studies in the report. An improvement instead is proposed, which is detailed in section 4.4. My understanding is that this proposed improvement is out of the scope of the reproducibility challenge.

# Discussion on results
The results for the reproduction of original paper are clear, however I needed to access the original paper to be able to compare. This should all be included in the report.

It is difficult however to interpret the plots for the results on the hyperparameter optimization. I could hardly identify any trend with 3 points considering the possible level of noise in the estimation. Also, bar plots are difficult to identify these trends, A scatter plot with regression curves would help better. The use of ratios was also confusing, although I agree it can be an efficient way to make the comparison. 

In the text, references to figures should be given to help better understand the explanations.

# Recommendations for reproducibility
There is no recommendations for reproducibility.

# Overall organization and clarity
The organization of the paper is correct. There is many typos, a few of which I report at the end of the review. The plots are difficult to read as explained above. The layout is clear, but the type of plot makes it difficult to interpret them. 

# Misc
Not sure the Related work section was necessary.

Section 3.1 not clear that these are datasets of original papers. I needed to verify to confirm.  It should be clearly stated.

Section 3.2, there is not mention of pre-processing in original paper. 

Figure 13-15: x-label -> iterations

Captions of Figure 16-18, all mentions MNIST but the titles give the right dataset.

Section 5.4
Wieghted -> Weighted
metircs -> metrics

Section 6.1
datapoins -> data-points

Section 6.2
mount -> amount (should be 'many attempts')",19-166.txt
46,https://openreview.net/forum?id=6C1s-vGlAC,0,7,4,"This paper aims to reproduce the results of the paper: ""Making AI Forget You: Data Deletion in Machine Learning"", which aims to transform the model performing k-means such that it mimics the effect of removing an entry from the dataset without removing the entry and retraining the whole model again. The authors experimented with the techniques and datasets presented in the original paper and performed hyperparameter tuning in terms of the number of iterations and depth of the hierarchy chosen for the proposed algorithm. The authors also made a minor extension to the originally proposed ""DC k-means"" which they call ""weighted DC k-means"". Overall, the authors could replicate the trends and the deviations from the absolute reported numbers in the original paper and the runtime analysis in the original paper can be attributed to non-convex modelling noise and differences in hardware.

The replication and algorithms themselves are straightforward and successful reproduction of the results is expected. Analysis of the results is lacking in insights. For example, the effect of changing iterations doesn't really seem to result in consistent changes and could have been explored better.",19-167.txt
47,https://openreview.net/forum?id=O1IJIoWN6p,0,3,4,"Overall Summary
==============
The report needs to be largely improved both in terms of correctness of the statements and the clarity.
There are many technically inaccurate or wrong statements. The manuscript also seems to have many relatively irrelevant contents. The claims do not seem to be fully in line with the observations of the reproduced experiments.
The difference in the classification accuracy between this report and the original paper (approx. 1% for MNIST and 8% for CIFAR10) deserves further discussion. Partly as a consequence, the keen difference in the classification performances of NODE and ANODE reported in the original paper is not observed in this reproduction effort. Since this is a reproducibility challenge, an in-depth assessment of the reason why this discrepancy occurred could have been a key contribution of the project (which is, unfortunately, not present in the report).

Details
========
[Questions]
- (For clarification) How much of the original code is used in the reproduction?
- (Discussion) Where do the differences in the accuracy between this report and the original paper likely come from?
- In the following, please correct me if I have some misunderstanding.

[Concise Problem Statement]
The goal of the report was to test the robustness of the finding of the paper for the choice of hyper-parameters. I think it is nicely made clear in the introduction.

[Reproducibility code]
I could not determine whether the code for reproduction is based on the released code-base of the original authors.
According to the website of the reproducibility challenge (https://reproducibility-challenge.github.io/neurips2019/task/),
As a publication in the ablation track, it is expected that the participants use the released code-base of the authors and perform ablation studies.
Although I do not think the instruction has to be applied strictly, it is worth reporting why the authors did not reuse the code from the original paper if there is a reason.

The authors may as well update the Jupyter notebooks with the results of running the code cells (currently, only the codes are shown).
For example, the notebook of the original ANODE paper has the evaluation results such as plots:
https://github.com/EmilienDupont/augmented-neural-odes/blob/5d3fdefc5eebdb4b6bdfaf74640a5cd50d5484c1/augmented-neural-ode-example.ipynb
and it is very convenient for interested researchers.

[Communication with authors]
Not applicable.

[Hyperparameter Search / Ablation Study]
The authors performed a hyperparameter search for the MNIST dataset.
However, the intention behind the choice of the hyperparameters has not been explained in the report.
I recommend the authors to clarify how the hyperparameter ranges were chosen because I think that helps the readers understand the purpose and the focus of this ablation study.

[Discussion on the results]
It seems that there is some non-negligible discrepancy between the originally reported accuracy and the reproduced accuracy.
However, the reason for the discrepancy does not seem to have been discussed in detail in the report.
I recommend the authors provide hypotheses for where the difference comes from.

Comments on other discussions:
- The caption on Table 1 on page 8 says: ""as the tolerance decreases, the accuracy deteriorates."" I could not follow how this conclusion is derived from the table. It seems that the learning rate is also changed towards the bottom of the table, which also seems to have a certain effect on accuracy. Please clarify why the effect of the changing the tolerance is specifically picked up, and why not that of the learning rate.
- (Section 5.2, paragraph 2) ""Moreover, a higher tolerance did not impact the performance of the model whatsoever"": this contradicts the claim of the caption of Table 1, which claims that the tolerance has an impact. Please be consistent.
- (Section 6, line 1) ""does indeed alleviate restricted flows"": this does not seem to be tested in this report, which focused on the performance improvement attributed to the device of augmenting dimensions.
- (Section 5.1.2) It is stated that the difference between the two classification performances is not significant. However, in the abstract, it is stated that ""[ANODE] is robust and is a significant improvement."" Where does this conclusion come from? I believe the difference in the classification performances is worth further discussion because it means that the results in the original paper could not be reproduced in this report. I also recommend the authors to discuss the computational performance improvement and the classification performance improvement separately.

[Recommendations for reproducibility]
Not applicable.

[Technical correctness of writing]
Many statements are technically wrong or inaccurate (which I see as a critical shortcoming of this report).
- (Section 2.1, line 6) ""until the adjacent layer learns its weights"": this sentence (especially ""until"") is ambiguous and does not seem to be correct. After the training of weights, the skip connection is used for inference, therefore the ""until"" does not make sense.
- (Section 2.1, line 10) ""that it detects as non-essential during training"": we do not know if this is true, and it is a sentence that needs validation using theory or experiments (along with defining ""essential"").
- (Equation (1)) The function f is not defined anywhere in the main body.
- (Section 2.3, line 2) ""The ODE flow will project points onto these additional dimensions"": This explanation is problematic because ""projection onto the additional dimensions"" implies (usually orthogonal) projections on the subspace spanned by the additional dimensions. This is not the case for ANODE, which uses all dimensions for computation, not only the additional dimensions.
- (Section 5) ""The difference between the two is significant."": by what test, and at which significance level?
- (p.8) ""tolerance decreases"": this should be ""increases"" because the change is from 10^{-3} to 10^{-2}.

[Clarity of writing]
The writing has much room for improvement. Please have your submission proof-read for English style and grammar issues.

The following is a suggestion for improvement:
- (p.8) Experiment number may be an unnecessary distraction. It also helps the reader if you sort the table according to the message the authors want to convey (e.g., moving the tolerance column to the left).

The following parts of the text seem to be irrelevant to the goal of the report.
- (Section 2.1) The explanation of ResNet is in the original paper and is not directly relevant to the experiments conducted in this report.
- (Section 2.2, last paragraph) This paragraph is irrelevant to the report unless the time-series data is used in the experiments of the report. The report does not have to recall all contributions of the original paper.
- (Section 3.1, the 2nd, 3rd, and 4th sentences) The history of MNIST does not seem to be relevant.

Following is an incomplete list of (insignificant) potential issues I noticed:
- (Abstract, line 2) The meaning of ""continuous spaces"" is ambiguous. Quite many machine learning models work in continuous spaces such as Euclidean spaces.
- (Abstract, line 5) ""current"" depends on when this manuscript is read. It may well be replaced with other time-independent expressions such as ""original"".
- (Abstract, line 7 and line 13) ""robustness"" can be used in various contexts and hence, in my humble opinion, requires disambiguation. I read it as ""robustness to the choice of hyper-parameters.""
- (Abstract, line 13) ""the model proposed [...] is a significant improvement"" is
- (Abstract, line 13) ""previous models"" is just NODE here if I understand correctly because the comparison is made only against NODE.
- (Section 4.1, 3rd paragraph) ""the task is categorized based on an image database"": I could not understand what is meant by this sentence.
- (Section 4.1, 3rd paragraph) ""our initial value is the dataset"": dataset refers to the set of data in this paper. This should be ""input data"", etc.
- The convention of denoting 10^{-2} by ""1e-2"" (the E-notation) seems to be relatively uncommon and it is better if the notation is explained in the report.
- (Section 4.1, 4th paragraph) It suffices to just state ""the input for ANODE is padded by zeros to match the model's input dimension"" (say) instead of using the whole paragraph.

There are also many typing errors. To pick up a few:
- (Section 5.2, 2nd paragraph, line 1) ""that re tuned"" -> ""that we tuned""
- (Section 5.1.1, 2nd paragraph, line 1) ""and is NODE is"" -> ""and for NODE is""
- (Section 2.2, 3rd paragraph, line 3) A period is missing.
Please have your submission proof-read for English style and grammar issues.",19-168.txt
47,https://openreview.net/forum?id=O1IJIoWN6p,0,6,4,"The report reports clearly states the original problem with a description of some theoretical results presented in the paper.

It appears that the authors reproduced the code from scratch. The documentation is unfortunately rather limited. A more extended guide/documentation would have been very useful in order to understand how to setup the reproducibility study. The code of the reproducibility study does not contain comments.

It seems that there was no communication with the original authors through OpenReview.

The authors present a fairly thorough analysis of the hyper parameters space. The observations from the hypers-space

The discussion of the results is quite limited. The recommendations for reproducibility are quite limited.

The report is fairly well-organised and readable overall, even if there are some parts are too concise. Some sentences that are not completely clear. In my opinion the authors devote too much space to the presentation of the original paper itself. This might also due to the fact that it seems that this report was originally the submission of a coursework of a module.

Overall, the authors shows that the paper is reproducible, but the discussion that is provided with the analysis is very limited in scope. The authors do not provide insights for improving the reproducibility of the results.",19-169.txt
47,https://openreview.net/forum?id=O1IJIoWN6p,0,6,4,"Ablation study of ANODE presents us with baseline verifications and small scale hyper-parameter stability and limited ablation. Here are specific points and positive & negative associated comments:

Minor comments:  

The paper presents the material relatively clearly. A summary of NODE is presented and ANODE is built upon that. Overall the structure of section 1 and 2 are appropriate however it could have been better with some improvements:
- The material is presented with figures taken from original papers. This is fine, however, it causes inconsistencies in notation. In some cases, notation is not introduced at all. The paper would be much more clear if the authors tie different subsections together. For instance, in Figure 1, one could explain what F and x are for the consistency within the figure itself. In Eqn 1, f is not introduces (as well as h), true,  we deduce them from the context, however, adding a few words would make it more streamlined and coherent.
- Section 3 explains the datasets that are used in experiments. Even though the datasets are relatively standard, they are nicely introduced in detail.
- Section 4 clearly lays out the experimental procedure and the comparison. This section also contains a small remark on the results where the authors observe an improvement in performance when they don't use adj. sensitivity. Furthermore, for better comparison, preprocessing, weight decay etc are turned off.
- Code: The codebase is a simplified version of the original ANODE code. Added hyperparameter search is necessary for the study of stability. This is completely fine however the paper lacks detailed hyperparameter search which will be addressed further below.

Major comments:
Section 5 presents the results. It is composed of several parts: (1) NODE VS ANODE comparison on CIFAR10 and MNIST, (2) hyperparam tuning, and (3) modification scheme where the filter sizes are modified. This section is followed by a very short discussion conclusion part:
- (1) is discussed clearly and the difference in the way validation loss changes depending on the dataset is striking. The authors point this difference out at the end of 5.1.2, perhaps this data dependence is something that could be discussed in more detail in another paper, but it is still worth emphasising in the discussion/conclusion.
- (2) Hyperparameter tuning has been carried out in a rather narrow band. The range of exploration for each one remains in the range of a factor of 3 to 10. On one hand, it is good to see the robustness of results doesn't depend on small fluctuations. However, in many cases of deep learning problems, one runs huge sweeps to find the sweet spot where models are relatively stable. In a study where stability of a model is tested, I think it would be great to see the limit of values where the model breaks. This is the first major shortcoming of this paper, although I understand that such tests may be computationally challenging.
- (3) Modified baselines: Similar to part (2), the modification is very limited in its nature. 

Overall, I see several ways the paper could improve:
- Streamlining the writing: consistent notation, consistent figures/captions, clear presentation of each result of section 5
- If feasible, finding the range of hyperparameters where the model breaks
- Implementing and/or suggesting engaging modifications (with motivations)
- Expansion of discussion and conclusion sections (e.g. to discuss wider range of hyperparameters, and discuss data dependancy)

Overall, I lean towards recommending the paper for acceptance.",19-170.txt
48,https://openreview.net/forum?id=hR_leftqaM,0,7,3,"Pros:

1. Clearly great care taken to detect and investigate experiments in detail, evidenced by tracing discrepancies in results to a hyperparameter difference between code and paper as a root cause.

2. Experimental results for many intermediate conclusions provide support for authors' explanations and narrative.

3. Cost/accuracy curves in Fig 2, 3 partially correct a major omission in reported results of original paper.

Cons:

1. Constraints on time and computation prevented authors from reproducing some of the most important results from the original paper.

2. Conclusions drawn from smaller-scale experiments cannot show much more than a proof-of-concept, and should not be overstated.

Details:

The submission clearly takes great care in reproducing the results in the original paper and ensuring that they were reported correctly. One notable example is the discovery described in the submission that, while the original paper reports a weight decay of 1e-4, the code released by the authors uses 5e-4 (for instance in the VGG-16 CIFAR-10 experiments at https://github.com/youzhonghui/gate-decorator-pruning/blob/b95f86a7d921432e2a149151137addd005e8e836/run/vgg16/prune.json#L26). The authors of the reproduction report results for each value in order to resolve this discrepancy. This already represents a nonzero contributions to any future researchers building on the Gate Decorator work whose results may have otherwise been affected by having this hyperparameter set to an unexpected value by default.

Providing additional FLOPs/accuracy curves is also valuable to any other researchers looking to evaluate the effectiveness of Gate Decorator. Methods that reduce the computational cost of neural networks also typically reduce their accuracy, with tradeoffs varying between tasks and methods. Not providing examples of multiple possible costs and accuracies makes it harder to draw more general conclusions comparing methods, and creates the potential to cherry-pick particular networks that do apparently well on one measure. The submission extends the higher-quality FLOPs/accuracy experiments that are only done for CUB-200 in the original paper (Fig 4) to also be done for CIFAR-100 (section 4.5 in submission). The truncation of results at 80% reduction and a few % of accuracy loss means that these may not fully represent the whole tradeoff curve, though.

The submission does not report an attempt to reproduce the results on ILSVRC or PASCAL VOC. This is important as these challenges are more difficult and more closely approach the complexity of the kinds of Vision problems that require cutting-edge approaches. More importantly for a pruning paper, though, the difficulty of these tasks requires higher-capacity models and thus more clearly demonstrate the effect of a pruning approach and its relative merits. The authors of the submission, for instance, note that some of their results are difficult to interpret because ""ResNet-56 may be too complicated a model for such a relatively simple classification task [namely, CIFAR-10]."" This is arguably true of VGG-16-M as well, the other model tested on this task. Of course, not running very expensive experiments is a reasonable choice, but it still makes the reproduction less valuable.

As one additional example of why very small-scale experiments, on which a small model can achieve a very good fit, can be hard to interpret, one must compare how tightly an accuracy drop from pruning has been reproduced versus the overall magnitude of the accuracy reduction. Note for instance that the ""deviation smaller than 0.1[%]"" in Table 1 is being measured in a case where the original paper is reporting effect sizes of 0.33% or 0.03% in its Table 1. This makes it not obvious that the reproduction is entirely confirming these results, since the small delta in results between original and reproduction could just reflect the overall small scale of accuracy changes in this domain. Multiple runs would show how much the variation in accuracy can be attributed to random factors such as the shuffling of examples between minibatches and how large of a change demonstrates a real effect. Both the original papers' conclusions and confidence of the reproduction would have benefited greatly from this kind of analysis.",19-171.txt
48,https://openreview.net/forum?id=hR_leftqaM,0,4,4,"I am quite puzzled about this submission, and cannot give any score except rejection at this status.

The main reason is that the report is submitted to a ""baseline track,"" rather than an ablation track or replication track. According to the reviewer guidelines, participants should ""reproduce and inspect the performance of the baselines reported in the paper."" If I understood the original paper correctly, the main baseline pruning methods should be the ones in table 1& 2 of the original paper (e.g. NISP, Thinet ...), while in the submitted report the participants reported the results for the ""GFIR baselines"" of Slim and PCNN.

On the GFIR baselines, the authors report that the baseline methods _underperformed_ compared to the figures reported in the original paper, where the authors account the failure to a possibly discrepancy in image resolution.

A possibly interesting result reported by the participants is the robustness of GBN method with respect to the varying values of hyperparameters lambda and eta, which renders that the report to be a more attractive candidate to be accepted at  an ablation track.",19-172.txt
48,https://openreview.net/forum?id=hR_leftqaM,0,7,5,"Problem statement: The report clearly states and understands the problem statement of the original paper.

Code: The authors re-used the official repository of the paper, except for the CUB-200 dataset. The paper belongs to the Track 2 - Ablation Track, focusing on hyper-parameter search. However, the modified codebase seems to be not provided.

Communication with original authors: The authors do not seem to communicate with the original authors, which could be further improved especially for the bad results on CUB-200.

Hyperparameter Search: The authors make many efforts on the hyper-parameter search. It goes beyond the paper settings and explores the effect of weight decay, batch size (Table 1), sparsity, lambda, etc. using novel values other than the original paper's. The exploration helps us understand the robustness of the method.

Ablation Study: The author provides an ablation study on fine-tuning (Table 3), showing the necessity of the tick-tock process.


Discussion on results: The authors make many discussions on how to reproduce the results and the potential reasons why it cannot reproduce some of the results. For most of the cases, the reproduction is successful.

Recommendations for reproducibility: Yes. For example, weight decay is not the same in the paper and the repo. The authors make a comprehensive comparison, and also provide suggestions on the batch size.

Overall organization and clarity: The paper is well organized. The figures and tables are presented in a clear way.",19-173.txt
48,https://openreview.net/forum?id=hR_leftqaM,0,6,5,"This report reproduces a subset of the experiments in a recently proposed CNN pruning method: Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks. The authors try to reproduce the results introduced in the original paper using the same hyper-parameters and experimental setups. Moreover, the authors also study how varying some hyper-parameters can affect the results of the proposed model pruning method. This report successfully reproduces the results reported in the original paper of ResNet-56 trained on CIFAR-10 dataset and VGG-16-M trained on CIFAR-100 dataset. However, for VGG-16-M trained on CUB-200 dataset, the authors failed to reproduce the reported results and some potential reasons are provided.

Pros:
1. This report is a complete reproducible study on a recently proposed model pruning method, though some results on large dataset e.g. ImageNet are not reproduced due to time and computation resource limit.
2. The methodology and detailed experimental setup used in the reproducing process are clearly discussed in this report.
3. The sensitivity of the proposed method under different hyper-parameter values is studied.

Cons:
1. It seems the reason of failing to reproduce the CUB-200 result is because the pre-trained model is fetched directly from torchvision.model, but in the original Gate Decorator paper it was trained from scratch. The authors are encouraged to use the identical experimental setup and report the reproducing result.
2. The authors tried to vary the “weight decay” and “batch size” hyper-parameters for fine tuning and report the results on the ResNet-56 model. It would be interesting to extend the study to all other considered datasets and model e.g. VGG on CIFAR-100 and CUB-200.

Minor Comments:
1. There are several typos in the manuscript.",19-174.txt
49,https://openreview.net/forum?id=3-K_F_VAdO,0,4,4,"Problem statement: 
The report does not clearly state the problem statement of the original paper. A summary of the original paper is provided without clear statement of what the report is meant to accomplish. cf ""Overall organization and clarity"" below.

Code:
The authors of the original paper have publicly shared code for their paper. However, there is no mention in the report whether the authors re-used that code or have re-implemented it.

Communication with original authors: 
There is no statement regarding communication with the original authors and no public discussion in the openreview page.

Hyper-parameter Search:
The report show an experiment doing hyper parameter sweeps on the learning rate and penalty value.

Ablation study:
The report does not show an ablation study but mostly hyperparameter explorations.

Discussion on results:
The report mainly discusses the qualitative and quantitative results of the methods and not the reproducibility results.  

Recommendations for reproducibility:
There is no stated recommendations for the authors of the original paper.

Overall organization and clarity: 
- The paper was confusing because there is no clear distinction between the contributions of the original paper and the contribution of the report. The report is presented as an extension of the original paper. More focus was put on extending the original paper than trying to reproduce and discuss the reproducibility challenges of the original work. 
- Additionally, the abstract and introduction (1.1, 1.2) are confusing because the authors of the report summarize the original paper without specifying that they are reproducing that paper and the use of ""we"" to paraphrase the original paper leads to confusion (we show, we propose, etc). 
- There is no clear statement of what the experiments are meant to accomplish vis-à-vis original paper. 
- Section 6.1 mentions new set of hyperparameters but it appears to be a different exploration (new values) of the original hyperparameters (learning rate and penalty values). Can the authors explain this? ",19-175.txt
49,https://openreview.net/forum?id=3-K_F_VAdO,0,5,4,"For the two interpretation methods, LRP and Grad-CAM, adversarial fooling is studied. The main idea for fooling is based on introducing a penalty term to penalize the deviations in the interpretation map. Three different forms for the penalty term are considered that can manipulate the location and significance of fooled interpretation maps. Overall, the Authors study an important direction and provide extensions on a notable paper. Yet, I don't find the contributions and the analyses sufficiently valuable to warrant a workshop paper. 

Below are my concerns/suggestions:

- Beyond a few cherry-picked examples in the figures, some user studies on the efficacy of fooling would add value. FSR itself is not completely convincing, because it is limited by the accuracy of the original interpretation map.

- The impact of learning rate seems subtle - it is hard to conclude any systematic effect of it. Merely trying few different learning rates and reporting results and then concluding ""A higher learning rate can result in a more incorrect interpretation."" is not convincing. 

- The impact of penalty value is intuitive but expected in a straightforward way. More analyses on it would be helpful, such as methods to determine the value of it in an automated way given the input and the model. 

- Standard deviations of the results are not presented. 

- I think Section 7 presents the right directions to push this work above the acceptance threshold - I suggest integrating them.",19-176.txt
49,https://openreview.net/forum?id=3-K_F_VAdO,0,6,2,"This paper reproduced the experimental results of ""Fooling neural network interpretations via adversarial model manipulation"". The original paper proposed adversarial methods to fool the interpretation methods of Deep NNs while maintaining the classification accuracy. The original paper concluded that the interpretation of the model can be easily fooled.

This paper reproduced experimental results shown in the original paper, and validated that the authors could reproduce the very similar results, although the paper only focus on the passive fooling method due to the lack of the computational resource. 

This paper contains several typos and need to be refined before publication. For example,
- Right after the formula (1), I could not understand the meaning of d_i and d_I. I guess that d_i is the typo of d_I, but I do not know which is correct. Moreover, x_i is also strange because h^I_c,j(w) does not have the index i.",19-177.txt
49,https://openreview.net/forum?id=3-K_F_VAdO,0,5,4,"Heo et al. original paper: 
Heo at al. finds that altering the penalty term of the objective function during a finetuning stage can purposefully manipulate explanations with minimal decrease to accuracy. The original paper demonstrated results for:
- Single dataset - ImageNet, 3 different “passive” manipulation technique (location, top-k, center-mass) and 1 “active” fooling technique
- Across 3 networks (VGG-19, ResNet-50, DenseNet 121) and 3 interpretability methods (Grad-CAM, SimpleGrad, LRP)
- The penalty term is computed on a far smaller subset of the training set (1,100 training 200 validation)

Replication report:
The report author’s evidence a clear understanding of the problem statement of the original Heo et al. paper. For certain hyperparameters such as learning rate and the penalty term, the authors go beyond the initial value specified and try a wider range of values. They evidence sensitivity of results to increasing both learning weight and the strength of the penalty term (although this incurs test-set accuracy trade-off which is no longer negligable).

A key limitation is that there is no reference or link to the code used by report authors (it is unclear whether the authors used Heo et al. open source code or implemented their own from scratch)? The code url in openreview appears to be the open source code of the original Heo et al. paper? 

The work would greatly benefit from the addition of a detailed discussion on how the baseline hyperparameter results compare with the results reported by Heo et al. In addition, The writing is difficult for a reader to parse -- many sections appear to be a close replication of sections in Heo et al. (such as section 3.3.1). 

Comments on report: 
1) Implements the “passive” manipulation techniques on 2 interpretability methods (Grad-Cam and LRP) and the 3 networks specified in the original paper.
2) The authors empirically find that increasing the learning rate and the penalty term increases FSR. It is interesting to observe the rate of change, but the direction of the finding does not feel that surprising [?] Given that the premise of the original paper is to “radically alter the explanations without hurting the accuracy of the original models”, it is unclear whether these lower accuracy models still fulfill the constraints of the original paper. With a 2-5% accuracy decrease, does the statement “interpretation methods can be fooled without significantly changing accuracy” still apply?
3) How was the subset of D_fool selected? Did the authors try and vary the composition of this subset, it would be interesting to see how much variance that introduces to the final results (given how small the subset is ~1100 in train).
4) At the original hyperparameter settings, do the results of the original paper hold? Was not able to find this discussion. From a visual check, it does seem that at least for vgg19, LPT_t results are very close (worth commenting on this as well as where there is greater divergence).
5) This submission could be improved by including recommendations to the original authors on how to further improve reproducibility. Unable to locate a link to the code used for reproducing (is it the original code or re-produced from scratch?).
 
Overall organization and clarity:
1) The reader would benefit from some improvements to the paper organization. Section 3.3.1 is largely a replication of section 3.2.1 in the original paper. It is probably sufficient to simply reference the original section. 
2) It is  unclear what in section 1 pertains to discussion of the original paper vs. is the contribution of this paper. The reproducibility track this paper contributes to is not clearly stated in the introduction.
3) The related work section 2.1 could benefit from being consolidated with section 3.1.
4) With respect to related work on evaluating the reliability of interpretability methods, it is worth being aware of other approaches to measuring the fidelity of interpretability method such as Kindersman et al. 2019, Hookers et al. 2019, Yeh et al. 2019, Tomsett et al. 2019 which each propose formal measure to evaluate the reliability of saliency methods (tomsett et al review existing methods).
5) Small nit pik: there are several sentences throughout the text that could be strengthened by adding relevant citations. One such example:
 “Although there have been many proposed defense algorithms, people are smart enough to attach the models by modifying the attack algorithms”",19-178.txt
50,https://openreview.net/forum?id=nl5mzotQhm,0,8,3,"The authors have successfully used the code provided by the original paper in their reproducibility challenge. They have extended the experiments by adding one additional dataset for experiments to study the robustness of the proposed method. Moreover, authors have also conducted experiments on a new CNN-LSTM based baseline architecture for further comparisons. Finally, the authors do a study to understand the effects of hyperparameters 
γ
 and 
α
 on the proposed model. 

The reviewer could not find any communication between current authors and that of the original paper on OpenReview and authors do mention in the paper that original authors did not respond to the queries about the specifics of the supplementary material.

The authors have done a great job of reproducing results of the original paper and providing insights on the various comparisons on the different type of loss functions used in the time-series model forecasting. Authors have shown that although the proposed DILATE loss is always better than DTW there are instances of DILATE being only as good as MSE. Hence, for a better understanding, more experiments should be conducted to establish the cases when DILATE is clearly superior to utilize. 

The authors used a new dataset WAFER to show the robustness of the proposed loss function. They obtain the results similar in trend when compared to results reported on datasets used in the original paper.  However, the authors do highlight that results for the DTW loss function are not close to the ones reported in the original paper. The authors could do a systematic study of the differences and include in the manuscript.",19-179.txt
50,https://openreview.net/forum?id=nl5mzotQhm,0,3,3,"In this reproducibility study, the authors aim to reproduce the results of the paper introducing DILATE (DIstortion Loss including shApe and TimE). DILATE is an objective function designed to perform accurate time series forecasting for nonstationary signals and multiple future steps prediction. This study presents results for reproducing some of the main results from the original paper and also uses a Convolutional-LSTM model (on top of the Seq2Seq model used in the paper) to show DILATE can be used with other Neural Network Architectures. The authors also study the impact of 
α
 and 
γ
 on the performance of the model. They find that some of their results do not match the original paper.  

Overall, the paper relies heavily on the original paper for details like hyperparameters, method, experiments etc. The authors do not provide reasoning for some of their choices, and the results have not been described adequately. 

Problem statement 
The report clearly states and understands the problem statement of the original paper. However, adding a few examples of the problem (as done in the original paper) will be useful for the reader. 

Code 
The code has been re-used from the author's repository.

Communication with original authors 
The original authors did not respond to the authors of the report

Hyperparameter Search 
The hyperparameter tuning is not very extensive or well described. 

Ablation Study 
The authors implement DILATE on Convolutional-LSTM model, which is different from the Seq2Seq model used in the original paper. They claim that they want to test if DILATE works on other architectures; however, they do not elaborate on why this would be useful for the field. 

Discussion on results 
Some of the choices, like ""The results were averaged over 5 runs as opposed to 10 runs"", are not supported by reasoning. 

The authors make multiple observations but do not provide detailed hypotheses that could be useful for the reader. For example, they suggest that the difference in results between their report and the original paper is ""due to the difference in ways of processing the data"". However, they do not elaborate on what those differences are and how they could potentially lead to different results. 

The authors perform an impact study of 
α
 and 
γ
 but do not describe the importance of these terms. They also do not motivate the different values assigned to them in the study.

Recommendations for reproducibility
The authors do not provide any recommendations to the original authors for improving reproducibility.

Overall organization and clarity
The paper requires more thorough experiments and detailed descriptions of the motivation and the results. There are also grammatical errors and typos in the paper e.g., ""The same analysis (be) can carried out...""",19-180.txt
51,https://openreview.net/forum?id=Bkx4d69zTr,0,6,4,"Problem statement: 3.5/5
The authors do a good job of summarizing the problem formulation studied in the original paper, but I think they could do a better job describing the algorithm they are exploring. At the moment, the ablation study can not be read without heavily referencing the original paper.

Code: Used Existing Repository

Communication with original authors: I did not see evidence of communication in the paper

Hyperparameter Search: The authors replicate the original results and conduct 3 additional experiments that vary the parameters of the MAB problem. 

Ablation Study: 4/5
Overall, I think the problem chosen are interesting and build on the results in the original paper. On the surface, it seems like there could be more variation done in picking the problems settings to study.

Discussion of Results: 3/5
The authors discuss the results, and provide some context but I think that more could be done to discuss the significance of the new results, how they complement and connect to the original paper. 

Overall Organization and Clarity: 3.5/5
The paper is reasonably clear and includes good figures to describe the results. There are some improvements to be made with respect to clarity that I have included below.

Other Comments
———————————————
What are the three different grids being used? This may be obvious for people more familiar with the previous paper, but it would be helpful to provide a brief description at the top of section 4.

“In Figure 1….” — I found this phrasing confusing, as it is interesting that some of the results did not replicate. Consider relaxing the language to saying that the results replicate with the exception of 1.D and then describing the difference. It would also be helpful to discuss the significance of this difference. 

Section 4.2: unclear language, consider replacing the use of “former” and “later.” 

Section 4.3: “Exploration requires more batches 
Δ
 decreases” — missing word

“In the first” — missing word

Why does the arithmetic grid outperform UCB1 in Fig3.A?

“Regardless of the level of the reward” — unclear",19-181.txt
51,https://openreview.net/forum?id=Bkx4d69zTr,0,5,4,"This report does an ablation study of an oral-presentation paper at NeurIPS 2019 called ""Batched Multi-armed Bandits Problem"", which is a theoretical paper. Thanks the author for reproducing the original experiments and for designing and conducting more experiments, while I think this report is not good enough.

First of all, I think the report clearly states the problem, but the presentation of the proposed algorithm is too short. I think it is better to provide more details of the algorithm.

The report reproduces the experiments in the original paper using the code provided by the original paper, and shows that some result negates the corresponding result in the original paper, which opens the room for further investigation and communications with the original authors. However, based on the report, it seems that the author of the report did not do it. In addition, I have a question related of graphs (a) and (d) in Figure 1. Both experiments include BaSE with minimax grid and BaSE with geometric grid, but why the corresponding two curves in graph (a) are not that similar to those in graph (d) in terms of numerical values. BTW, this phenomenon also appears in the original paper. I could not figure why. In addition, I could not understand the dotted blue curve (the one associated with ETC with geometric grid) is increasing when M >= 4. In my opinion, as M goes large, the curve should be non-increasing.

In addition to the experiments conducted in the original paper, this report also includes the experiments associated with a large-delta case and two small-delta cases, respectively, where delta is the mean gap between the optimal arm and suboptimal arms. I am not sure why the ETC algorithm is not included. In addition, although the author provides some intuition and insight of conducting these experiments, I want to see more related to the motivation of doing these experiments and takeaways of the results.

In the proposed algorithm in the original paper, there is a tuning parameter denoted by gamma. Theorem 4 of the original paper requires gamma >= 12, however, in the experiments of the original paper, gamma is set to 1. I really want to see some experiments that set some gamma that is consistent with the theorem, however, I did not find them in this ablation-track report. It would be great that the author could include these experiments.

There are several typos in this report.
On page 1, you write 
we=1,2,...,K
. Is 
we
 a typo?
In the second line of Section 4.2 on page 4, the upper bound of 
Δi
 should be replaced by 
K
.
In the second line of Section 4.2 on page 4, this is an redundant ""="".",19-182.txt
51,https://openreview.net/forum?id=Bkx4d69zTr,0,6,3,"The authors have a good understanding of the problem. The code is provided by the authors of NeurIPS paper.

The authors do not communicate with the original authors.

The new hyper-parameter, \delta that represents the gap between the mean reward of the optimal arm and  other arms are tested. The results show that if the gap is larger, the best arm can be identified with few number of pulls. The gap in the original paper is not specified. I wonder how the gaps and the mean rewards are chosen and tuned. For large delta, the authors only try mean reward for optimal arm as 1.5 and the mean reward for other arms as 0.5. As shown the experiments in the small gap, the convergence rate of average regret vs. M, K and T are similar for the compared algorithms with the same gap 0.2 . I wonder if the algorithms have similar performance with large gap. 

The experimental results are implemented on toy data. It will be interesting to see results on real-world data sets. 

It is interesting to analyze the performance of algorithms in different setup, not only limited to the same gap (large, small). ",19-183.txt
51,https://openreview.net/forum?id=Bkx4d69zTr,0,4,4,"I would like to start by thanking the author for dedicating time and effort to verifying and extending the experimental results of the original paper. In this study, the author examines the effect on the BaSE algorithm of different problem settings, with particular interest shown to variations of M (the number of batches - along with 3 different types of grids), K (the number of arms), T (the time horizon) and 
Δ
 (the gap between the optimal and suboptimal arms). I found the problem statement and algorithm description to correctly reflect the descriptions in the original submission.

I do however find the ablation study to be missing a few evaluations that I consider important. I am referring to the study of the effect of the tuning parameter 
γ
 on algorithm performance. I find this to be particularly important since that is the main knob a user of this algorithm would use to tune the algorithm's performance. I would have liked to see more granular data points in the plots and possibly larger horizons (particularly those in Figure 2 - looks like a higher horizon for K would have made the plot more informative) along with more detailed explanations regarding the experimental setup: number of runs over which the averages are taken, explanation as to what are the units of measurement on the Y-axis of the plots. I would have also liked to see insights as to why the curves behave in certain ways during ablation - for example: Figure 4 (a) - the regret of the minimax grid produces a dip for M=6 - why?

Overall I find the ablation study OK but there is definitely plenty of space for improvements.

Minor comments:
Typo: ""...under different batching frameworks, or grids.""
Typo in capitalization: ""We faithfully reproduce...""
Typo: there is a dot right before this paragraph, can that be removed?
Clarification: \mu^* is the maximum expected reward - this needs to be clarified otherwise it spawns confusion as to whether or not the rewards are bounded
Typo maybe: ""Exploration requires more batches _as_ \delta decreases""?
\Delta = 2 instead of ==, the '==' notation is not used in mathematical expressions",19-184.txt
52,https://openreview.net/forum?id=S1lXO6cf6S,0,6,4,"Overall, the report seems to be a faithful reproduction of the results presented in the original paper. There are some problems with readability and grammatical issues discussed at the end of this review, as well as some additions I would appreciate in the final draft. I've made comments on each of the criteria listed in the reviewing guidelines. I've made some concrete suggestions for future drafts before minor edits and fixes.

I think the contributions this report provides (i.e. the implementation, filling in some missing details, some reproduction of results) is worth accepting to the workshop given the authors make some of the changes provided below. As this is my first time reviewing for a reproducibility challenge, I do not have any calibration on the rating. As you improve the draft and we discuss in the rebuttal phase, I would be happy to revisit this review and my score. 

Problem statement:
=============================

This was well done, and the author had a grasp on the original problem addressed by the authors.


=============================
Code: 

- The authors provided an all new codebase for their reproduction, and the codebase seems relatively well documented and easy to use (I did not test this, mearly looked through the code base). I would have preferred more comments when going through the actual algorithm code, but it is currently sufficient.
- For the code used for data generation, it would have been much better to use a different implementation or to provide a novel implementation of these algorithms. This would have strengthened the results you provide and the experiment would have a double check.

=============================
Communication with original authors:

  The author mentions some communication with the original authors, but there does not seem to be any communication through OpenReview. They discussed with the authors to decide on hyperparameters, and to learn about critical aspects of the architecture (i.e. VAE). I believe there was a sufficient amount of work done to work with the authors to make a good faith reproduction.

=============================
Hyperparameter Search:

  The hyperparameters reported by the original authors seem to be used, and in the original paper the baselines seem to be lacking a hyperparameter search where default parameters are chosen. Given the original authors did not provide any semblance of hyperparameter study, I think in reproducing their work this is an important addition.

  Given you have provided actual novel code, this is less critical but would have made for a much stronger reproducibility report.

=============================
Ablation Study

- No ablation study.

=============================
Discussion on results

The authors did a good job reproducing results, and discussing how their results compare to the original paper's. There are several issues in this section that, once addressed, could make the report much better.

Results/Experiment:

1. More runs! It seems you are only doing a single run to reproduce the paper. The original only used 5 as well. This report could have provided a nice contribution if a better statistical study was done.
2. Data collection: I would have liked if all aspects of the original experiment was different. This includes the soft actor-critic implementation used for data collection. While it is clear that the data collected is minorly different, the report would have been stronger if another implementation was used.


=============================
Recommendations for reproducibility

- There does not seem to be any recommendations for reproducibility for the authors. Although there could have been several good points (already briefly mentioned by this paper) which could have been more clearly stated. Such as:
  - Missing details in the algorithm/architecture
  - missing details on hyperparameters
  - lacking details on any hyperparameter tuning


=============================
Concrete suggestions for improvement:

1. I would like the authors to provide more runs, at least for the novel algorithm, in their replication of the original results. This would go a long way of making this report better.
2. I would like there to be some clear discussion on what the paper could have improved on in terms of reproducibility.
3. Several of the grammar/inaccuracy issues to be addressed (see below).
4. Lists of all the hyperparameters used in the other baselines and data collection algorithms. I believe this is necessary even if you are using another repository. There is no guarantee that code on github or anywhere will remain forever unchanged, so it would be better for us to keep track of these used parameters disconnected from the specific implementations. 


=============================
Overall organization and clarity


There are several odd/inaccurate phrases throughout:

  - ""with the comparable performance with humans or sometimes surpassing human ability""
  - ""In reinforcement learning, Markov Decision Process (MDP) can be expressed..."" -> An MDP is not unique to RL. Typically we say something along the lines of ""We model the dynamics of our problem using an MDP...""
  - ""The goal of reinforcement learning is to maximize..."" -> This is technically true, but maybe a little bit inaccurate. We usually talk about the agent maximizing expected discounted reward or specific algorithms doing this. In general, we want agents which achieve goals and maximizing discounted return is one way of doing this (average reward case is another, etc...).
  - ""Q-learning is off-policy RL algorithm"" -> ""Q-learning is an off-policy...""
  - ""that allows target policy... Q function"" -> ""which enables an agent to learn a policy different from the current behavior policy, typically called the target policy.""
  - ""equation (2) is only valid when the action space is discrete."" -> We can still define the max operator over continuous variables, but this is intractable. So the equation is still valid, just intractable.
  - Section 4.1 needs to be rewritten/reorganized. Currently it is very difficult to read due to how dense it is.

  There are other issues throughout, so I would recommend a few more editing iterations.",19-185.txt
52,https://openreview.net/forum?id=S1lXO6cf6S,0,7,3,"In this report, the author replicates the code for BEAR from scratch. The replicated version yields similar performance in most of the major experiments in the original paper. The author also provides additional details that are missing from the original paper (e.g. the use of VAE to approximate behavior policy, hyperparameters settings). Overall, the paper is easy to follow, and I believe this report is a useful reference for practitioners who want to reproduce the BEAR algorithm.

A few minor comments:
1 ) How difficult do you think it is to reproduce the results? It would be nice to briefly address the difficulties you encounter (if any) when reproducing the algorithms.

2 ) How sensitive is BEAR to those missing hyperparameters? Have you tweaked those numbers a bit and see what would happen?

3 ) What information should the authors provide in the paper to help improve reproducibility?

4 ) Since the official implementation is released, can you find the differences that cause the discrepancies in your experiments?",19-186.txt
52,https://openreview.net/forum?id=S1lXO6cf6S,0,4,5,"This paper concerns a replication of the BEAR-QL algorithm for batch-RL. Code written from scratch is released. 

Strong aspects of the report:

- Code is released in a nice, well-organized repo. 

- There is a reproduction of all the main results of the original paper, and nice comparison figures.

- Section 4.1 shows a very nice effort on the part of the author to reproduce the datasets generated by the original paper, and I think the system used to do was fair.

Comments

- Some implementation details are missing. How is MMD computed? What kernel function is used?

- It seems most of hyper-parameters are taken from the BCQ paper rather than the BEAR-QL paper, if you communicated with the authors, why not use the actual hyper-parameters rather than the hyper-parameters from BCQ? 

- I would like to see a lot more discussion of both the implementation details and the results. I found some of the results in your paper to be quite interesting, but these are my -own- interpretation. Some things that stood out to me:
(1) The replication of BEAR-QL performs worse than the results in the paper in 3 of the 4 environments in both Figure 1 and 2. To me this suggests that the claims in BEAR-QL are overstated OR the algorithm is missing key results for reproducibility. 
(2) The BEAR-QL authors make no mention of the VAE in the main paper but is seemingly a major part of their algorithm. 
(3) The GitHub mentions environment-specific hyper-parameters, do you feel like this is a fair comparison to other methods? Were these environment-specific hyper-parameters important? 

- To add on, there is a missed opportunity for recommendations on how to improve the reproducibility.

- The background section feels a little disjointed from the focus on the paper, BEAR-QL. I believe some of it is relevant for the theory of BEAR-QL, but not the algorithm itself. 

- The organization is a little poor in places. For example, equation (4) should be grouped with (3) for better clarity.

- Section 3, Analysis of BEAR Q-Learning includes a lot of claims made by the original authors. These are what this report is meant to test, not re-state. For example: ""It is less strict than BCQ"" (referring to the policy). This is a claim made by the original authors and is not tested in this report.

- The hyper-parameters include mention of clipping of the standard deviation of Q functions. I couldn't find mention of this in the original paper (although, I could have missed it). Is this a new hyper-parameter that you found necessary? What happens if this is removed?

I do want to thank the author for taking the time to reproduce the code and write this report. I think the code and results in the paper would be of interest to the reader and RL community, but unfortunately, the discussion and overall quality is not up to the standard of publication at this time. ",19-187.txt
52,https://openreview.net/forum?id=S1lXO6cf6S,0,5,2,"This contribution will certainly be helpful for others attempting to reproduce or build upon results of the original paper. That said, it is lacking critical discussion of many of its observations and needs to thoroughly be proofread.

Disclaimer: I haven't been given sufficient information by the organizers to be properly calibrated regarding the overall score metric. I'm therefore open to adjusting my rating.

Detailed comments below:

_Problem statement_
The problem is described in a clear and concise manner.

_Code_
The codebase is made available and reasonably well documented.

_Communication with original authors_
- The author should have reached out to the authors regarding hyperparameter settings.
- Contact with the original authors has been made to obtain clarity on the use of VAEs. 
 
_Hyperparameter Search_
- Only one standard hyperparameter setting is used. Knowledge of the original setting is not available which limits the use of the presented results.
- There is a lack of information about the behavior policy that has generated the dataset. This may be one of the main reasons for discrepancies in the results. Did the author contact the authors of the original paper to obtain this information?

_Discussion on results_
- There are large discrepancies form the original results that are not discussed in sufficient depth. For instance Fig. 1, columns 1, 3, 4; Fig 2, columns 2, 3, 4.
- The author does not provide any explicit recommendations to the original authors for improving reproducibility.
- The author states that ""further hyper-parameter tuning or generating additional datasets with a different standard deviation of average return seems to [...] [result] in more similar results."" This seems to be a mere hypothesis and is not empirically backed. 

_Overall organization and clarity_
The paper is well organized and easy to follow.

Grammatical issues / organization / proper plots
- The paper needs thorough proofreading. It contains many orthographical and grammatical errors.
- Fonts in plots are extremely small, barely readable in a printed version.
- Some plot axis have missing labels (like in the original paper).",19-188.txt
53,https://openreview.net/forum?id=ryxarpcfTB,0,6,4,"In this work, the authors reproduced main results from DILATE (Distortion Loss including shape and time) and included additional studies on hyper-parameter, loss function and different neural network models.

The authors did reproduce the main results which include evaluation on three non-stationary time series datasets from different domains as in the original paper. In addition to that, they also carried out evaluation on different neural network models (a CNN-LSTM model) and impact of hyper-parameter and loss function.

I would have appreciated if the authors could have made a more one-to-one comparison to the results from the paper if this is possible. For example, did the authors do something different to reproduce the results? Were any details unclear? For instance, the authors stated the results they reproduced are ""similar"", but how are they similar, how do they differ? Are the reproduced results close to the one that have been reported? 

Further, they state  they could not reproduce the Hausdorff distance and Ramp score due to lack of supplementary materials. Could the author also elaborate what exactly is missing to reproduce this part?",19-189.txt
53,https://openreview.net/forum?id=ryxarpcfTB,0,6,4,"I found this replication to overall be of good quality. The authors do a nice job of clearly describing the published work, their replication attempts, and the results. Below are more substantial comments, organized by suggested criteria. 

-Problem statement 
The authors provide a good summary of the work being reproduced and give adequate background of the proposed method in the original work along with the motivation. 

- Authors use the code provided by the original paper to perform their analysis. 

- Communication with original authors
The authors say in the report that they contacted the authors for the supplement and data cleaning procedure but did not receive a response.
I was unable to find attempted communications between the authors and the original authors on the OpenReview page. 

- Hyperparameters
The authors do a nice job of considering the performance of DILATE over a range of the hyperparameters and provide a reasonable description of the sensitivities. 

- Discussion on results
The one area that I think can be improved in the writeup is a discussion of the results attained. It appears from the writeup that for at least the ECG and Wafer datasets, MSE outperforms the proposed methods. It would be helpful if the authors worked to provide some intuition around these results, if possible. For example, are there attributes of the time series that would provide intuition? 

- Recommendations for reproducibility
The authors do not give any direct suggestions for improving reproducibility, though they are implied. For example, it appears from the writeup that the lack of availability of the supplementary material hampered the replication.

- Overall organization and clarity
Overall I think the organization, presentation and writing of the report is good and does an adequate job of describing the replication effort. ",19-190.txt
54,https://openreview.net/forum?id=H1giraczTS,0,6,4,"+ The authors provided their own implementations of baseline models used in the original paper, which served as sanity check on the correctness of their continual learning proceduere.
+ The hyperparam tuning was elaborate however not too extensive, which is good as reproducing results with limited computing resources is necessary.
+ The comparison between their own implementations and the authors' didn't stop only on the reported numbers, but also included a couple of qualitative ones for analysing the failure.

- Although the report is thorough and includes several baselines, I found the discussion to be weak and too heuristic. I like that the authors pointed out several differences between the claims in the original paper and its earlier implementation by the authors of the paper, but not every detail matters in terms of explaining the significant performance gap observed in the reimplementation described in the report.
- I understand that there was a time constraint, but I would appreciate more if the authors of this report could point out several crucial detrimental factors in performance. ",19-191.txt
54,https://openreview.net/forum?id=H1giraczTS,0,4,4,"The submission starts by a reformulation of the original paper's goals and method. They redefine some notation and reformulate the method in their own words.
Some of the reformulations can be a bit misleading, though, such as using ""trajectories"" to denote both the sequences of data (denoted that way in the original article), and the series of values taken by the trained parameters during the optimization.
Denoting only the training part as D_traj, and the testing one as D_rand is also misleading, as the testing data is also a sequence or trajectory in the original article, as expressed in Algorithms 1 and 2.

Two paragraphs in Section 2 suggest that the authors did not fully understand the core of MAML and MAML-inspired algorithms:
- ""first making a copy of the initial weights of the TLN. Then,  an inner loop that only updates the weights of the copied TLN is executed. [...] After that the weights of both the RLN and the original TLN are updated using a concatenated sequence of the trajectory dataset and the random data [...]""
- ""In the context of OML, during the online updates, the 
θ
 parameter is fixed and only the 
W
 changes. The idea of this approach is that 
θ
 will be optimized for a better overall prediction accuracy but under the constraints of the online updates which is based on the representation of the meta-objective.""

The important part of MAML is that the optimization procedure of the inner loop (optimizing W) is itself differentiated through, in order to get the meta-update of the outer loop (meta-optimizing theta). MAML actually takes gradients through the application of the optimizing procedure for W.
In other words, learning W is not the important part. In fact W is randomly initialized for each training sequence. What happens is that theta is updated in a direction such that training W should be easier next time.

This leads to the main issue in the re-implementation: there is no gradient taken through the ""inner_update"" function (defined at https://github.com/sergiolib/reproduce_oml/blob/master/experiments/training.py#L33). Instead, the TLN's parameters are optimized from a training sequence, and then an additional gradient update on a train+test sequence.
In order to correctly implement the MAML-inspired algorithms, at least the following would have been necessary:
- not using ""v.assign(...)"" in the inner update, as it does not preserve the flow of gradients, but create new tensors at each inner step instead
- include the whole inner optimization loop inside the ""with tf.GradientTape(...) ...:"" block.

The authors clearly invested time and energy re-implementing the datasets and the baselines, in addition to the original method. The SR-NN re-implementation at least performs in a similar way on the various experiments.
The authors also uncovered various inconsistencies, missing details, and unclear settings in the original article, which the original authors acknowledged and updated in the camera ready version.

For these reasons, the work performed by the authors, and the documentation of that work, is valuable. I also appreciate that re-implementation of a MAML-like model, and subsequent testing and debugging, is tricky.
However, unfortunately, I cannot accept their conclusions that the original work is not reproducible ""because several details essential for its implementation are not explicitly stated in the paper or are misleading"", as there is a major issue in the core of the re-implemented algorithm.",19-192.txt
54,https://openreview.net/forum?id=H1giraczTS,0,9,4,"The authors of this report thoroughly understand the problem, methods and the way of reproducing the main algorithm in the original paper. However due to some missing details in the original paper or implementation differences, their results do not exactly match.  In general this report fell in track 3 to reproduce the codes but also has done many comparisons with other methods mentioned in original paper. Valuable insights are throughout the report.

Codes: The codes were reproduced from scratch since Tensorflow 2 were used here while original codes were written in Pytorch . Some hyperparameter settings were missing from the original paper but several of them were found by inspecting the original codes. Also Tensorflow 2 codes were released in Github.

Communications: No communications with the original authors was mentioned in the report but the NeurIPS paper authors have responded the report authors through OpenReview.

Hyperparameter Search: Due to the missing parameter 
β
 of SR-NN, the authors did their own hyperparameter search and fix it to be 0.01.

Discussion and Recommendation: The report contains detailed discussion on the state of reproducibility of the original paper and also gives enough recommendations. 
It confirmed that MRCL shows great resilience to forgetting the previous tasks but also pointed out many mismatches in the results..
Several of the recommendations were accepted by the original author for better improvement.

Overall the organisation is clear and the experiments are well motivated. I recommend accepting the report.",19-193.txt
54,https://openreview.net/forum?id=H1giraczTS,0,8,4,"The report is very clear and well written.  The problem statement of the original work seems well understood and the original approach is clearly restated with a detailed overview.

The authors implement the code from scratch in Tensorflow while the original code was in Pytorch and provide a link to the git of the project.

The authors perform a hyper-parameter search and mention that since it was not clear what the original hyper-parameters used were, it was necessary to do a new search and test different values.  Figure 4 shows the effect of different values of \beta.

The report does not mention fair communication with original authors, though it does mention a request for experiment code by a researcher in July.  It is not clear if this was one of the authors.  The original authors did respond through OpenReview with clarifications and points that they will update in their paper.  I would be interested to see a response in the rebuttal to the authors response in OpenReview and see if there are any updates to this.  The points from the authors of the MRCL paper are clear, but this was not completely clear in the original work or released code leading to the results in this report.

The authors provide a detailed discussion on their results and point out discrepancies between the original paper and it's provided code.  They do not make outright suggestions for how the authors can improve reproducibility but do point out the many issues that led to the original results not being reproduced.

Some small points:

OML vs MRCL:  The authors in the original work refer to their proposed results as OML it would be helpful if the same was done here to aid in comparisons, particularly in reference to Figure 3 in this report and Figure 6 in the original work.

Graphs in Figure 1, 2 and 4 should be larger to make them more clear.
Thank you for taking the time to undertake the reproducibility challenge for our paper. It's wonderful to see that papers are being scrutinized and being held to a higher standard. 

# Some clarifications 
## 4.4 Oracle: 
Oracle uses the RLN learned by MRCL (Now called OML) -- it is not different than OML at meta-training time. At the meta-testing time, it uses multiple epochs + IID data instead of continual learning to learn the TLN (Now called PLN). This allows us to test the quality of the representation without confounding factors -- such as catastrophic forgetting. In the camera-ready version, two new graphs present IID results for all methods (Figure 4 (c) and 4 (d) 

## Reproducing results from Section 5: 
We do link to the repository used to get those results in the readme file in the references (https://github.com/mattriemer/MER), but I agree that it's easy to miss that. I'll respond to the issue on GitHub to make it more explicit.

We simply loaded RLNs trained using different methods in the MER repository and used their existing code to learn PLNs. We also gave an equal budget to each of the methods for hyper-parameter tuning.  

## 6.4 Other issues
Thank you for pointing these out. I've fixed the EWC citation. The “single-pass through the data” is not named in Lopez-Paz and Ranzato [2017], but it is what they use. Nonetheless, we had removed this reference after the review period and clarified the protocol in the camera-ready submission. 

## 6.1 Dataset issues
We indeed used slightly imbalanced datasets in the review version of the paper -- it was a bug. We used 900*20 for the baselines and 963*20 for MRCL (OML). (The 900 * 15 is due to a change I made in the code after training RLN for SR-NN and Pretraining. I introduced a new ""test"" parameter to evaluating test accuracy on the meta-testing set but didn't realize it will break the baseline training methods. SR-NN and Pretraining were meta-trained on 900*20 images, contrary to what the repository does now.) 

Since then, we have fixed the issue and use 963*20 for all methods. We have also introduced an additional baseline -- MAML-rep -- which also uses 963*20. Fixing the bug did not significantly alter the results.

## Hypothesis as why MRCL (OML) doesn't converge
I've observed gradient-based meta-learning to be sensitive to small changes in implementations -- such as parameters used for the model initialization. This is not surprising as it involves flowing gradients back multiple steps. I suspect this is why OML doesn't perform well in the reproducibility report. As a sanity check, the authors could try reproducing MAML on Omniglot first using an RLN/TLN architecture. Even an RLN trained using the few-shot objective of MAML outperforms SR-NN in our experiments (See the Maml-rep baseline in the camera-ready version.) Once MAML results have been reproduced, the authors could change the meta-objective to OML. 

Moreover, Jeff Clune -- in a talk at NeurIPS 19 -- showed that their team not only reproduced OML, but also significantly improved the results. 
The talk is available at https://slideslive.com/38921888/biological-and-artificial-reinforcement-learning-3 (Staring at 48:00) 


I really appreciate the time and effort that authors put in for reproducing OML. It's evident from the report and the code that they were very thorough and meticulous. Since NeurIPS allows making small changes in the camera-ready version till January, I'll make sure I incorporate the feedback from the report, such as mentioning the missing hyper-parameters, in the paper. I'll also update the mrcl GitHub repository with the updated implementation of OML. 

Best, 
Khurram ",19-194.txt
55,https://openreview.net/forum?id=r1lDKP2fpr,0,6,3,"Good reproducibility effort, and proper followup from communicating with original authors. However, overall reviews are not quite good enough for the AC to recommend to the journal.",19-195.txt
55,https://openreview.net/forum?id=r1lDKP2fpr,0,6,2,"The authors of this report seem to have done a good job in trying to replicate the experiments of the original paper from scratch. This track can be challenging given that one is expected to replicate the experiments of the paper without referencing to the original code by the authors. Given the fact that sometimes papers miss details that might be important for a third person trying to reproduce the results, such as hyperparameter search as well as many other decision chosen in the course of writing the paper, the authors seem to have taken these decision seriously. They mention that they try to do educated guesses for many things which were not explicitly mention in the content of the paper. As they indicate, this can be easy sometimes, but also challenging other times. This becomes more important for the minor details of the architecture and such. The authors mention that they had to refer to the original papers for the details on the architecture and assume that the exact same architecture was chosen. Overall, here is the list of pros and cons that I see:

Pros:
1. The authors chose the replication track, which means replicating the code from scratch without referencing the code by the authors. This can be a rigorous check for if the paper is reproducible as is, making sure pays attention to the most minor details of hyperparameters and other design decisions. Authors mentioned that they tried to make most educated guess they could with the information at hand, which is a very positive attitude for reproducibility in the community.
2. The authors provide a link to their code, and mentioned that they employed peer-programming to spot bugs. This is a great pro-active approach from their side.
3. The authors mention very clearly on which parts of the paper they found hard to reproduce and which details they found were missing, but were crucial to the reproducibility of the experiments. This means they paid attention to the very minute details of the paper while doing their implementation.
4. The authors did mention that they reached out to the original authors of the paper for any doubts that they had. This communication with the original authors of the paper helped them realize that they were missing some of the finer details in their implementation which were present in the code and were not mentioned explicitly in the paper. But referencing that code was not allowed in the track chosen for this report.

Cons:
1. The main concern is that the authors were not able to reproduce some of the results due to computational issues. This means it is hard to say if we given those computational resources, were the results reproducible at all or not. However, I do believe that this might be hard to know in advance that the jobs will be computationally expensive, unless stated in the original paper, which it wasnt.
2. Due to missing information on the hyperparameters and other details, some of the parts were also not reproduced. This again, is not something that authors could be held responsible for as if the crucial details are missing in the original paper, getting things to work by just guessing can be challenging.
3. Since there were some details missing in the original paper, those go missing in the reproducibility report as well because the best that could be done in the report would be trying out things and making educated guesses. There were also issues with computation, making it hard to take a final call on how easy the reproduction of the full set of experiments would be if this were resolved. It is also hard to tell if the code published by the original authors works fine or not as this track did not allow looking at the author's code.",19-196.txt
55,https://openreview.net/forum?id=r1lDKP2fpr,0,6,4,"The report reproduces the results of the paper titled “Learning to Learn by Self-Critique”, which proposes a method for few-shot learning utilizing the target set along with the support set. They show that the original paper’s results can be reproduced even though many implementation details are missing in it.

The points in favor of the report are: 

(i) They have re-implemented the baselines as well as the work in the original paper.

(ii) The methodology used to reproduce the results seem sound.

(iii) The reproduced results are presented clearly and the comparison with the original results is easy to comprehend.

(iv) The reproducibility section is well written and provides insights into some of the missing details from the original paper.

(v) The Github code provided is well structured and comprehensive.

Notwithstanding the above points, there are some key areas where the report would benefit greatly from:

(i) They mention in the report that large amount of key information such as learning rates, weight initialization and choice of optimizers is missing. They mention some examples such as using PyTorch default initialization instead of Glorot, but much of the other information is still missing. As this is a reproducibility, the authors should make clear exactly what information was missing and what values they used, preferably in a tabular form so that the readers can further reproduce it easily.

(ii) The authors mention that they ran out of memory for 5-shot learning of MAML++ on both the data sets. They claim that the memory required is prohibitive. As the authors are using a state-of-the-art GPU and the original paper was able to get the results, the authors should interact with the original authors to understand how the model was trained. As a reader, this discrepancy is important to sort for a reproducibility paper.
We were contacted by the authors of the reproduced paper and received some valuable comments. We think in retrospect, that the final discussion is likely to be interpreted as overly harsh. What we intended to communicate is:

1) SCA can be reproduced for Low-End MAML++ without getting the details exactly correct, which we think is very positive.
2) Magic numbers obtained by extensive hyper parameter search, without explanation in a paper, is bad, although we have no reason to believe is the case for the High-End MAML++. The authors, however, pointed out that we are missing possibly ""crucial"" details in our implementation that were not mentioned in the paper.
3) We sincerely understand that it is hard to list every detail in a paper, and that it can make the paper hard to read. We are furthermore not arguing for whether the paper should be self-contained or not, but rather draw the conclusion that we were not able to reproduce everything based on solely the contents of the paper.

Overall, we were very happy to learn about this method and impressed that it is possible to guess many of the details and still produce the same results.

We will shortly update the manuscript on arxiv to reflect these changes, and will be happy to change this also in a final manuscript if accepted here.

Best regards,
Isac Arnekvist & Dmytro Kalpakchi",19-197.txt
56,https://openreview.net/forum?id=B1grPT9GTH,0,6,4,"First of all, I wanted to thank the authors of this reproducibility study for their significant effort in partially reproducing our results. I do believe that reproducibility is an important aspect of machine learning, and has not received appropriate attention in the recent years. I do therefore appreciate the authors' valuable contribution.

However, I have some concerns about the wording of this report. After reading it, one might get the wrong impression regarding the original contribution. I therefore wanted to clarify a few things:

1.) Only 1 out of 2 of our original algorithms have been reproduced by this replication study.
2.) Experiments from only 5 out of 8 environments have been replicated.

Details follow.

1.) The writing suggests that EAC is our main algorithm (as also picked up by one of the reviewers), which is not true. EAC is one out of two algorithms from the empirical section of our original paper---the other one is ACIE which has not been mentioned by this reproducibility study. On a side note, ACIE obtained clear performance boosts in our experiments on Hopper and Humanoid when compared to baselines.
 
2.) Concerning the replicated EAC experiments. In our paper, EAC was also unable to beat the baselines' performance on those five replicated environments---the only exception being Ant. While the reproduced EAC performance is good overall, we did obtain significantly better results on Ant in our paper. But this has not been mentioned in the replication study. Furthermore, in our experiments, both EAC and ACIE improved over baselines in three lower-dimensional pendulum environments that have been omitted in the replication (as also noted by one of the reviewers). This could have at least been mentioned.

Minor comment regarding model loss plots

I wanted to note that empowered actor-critic algorithms only require models for one-step-ahead predictions. So compounding errors, as in sequential planning algorithms, are not as much of an issue. Furthermore, high loss in terms of high cross entropy is not necessarily indicative for non-working models since log probabilities (and densities) are very sensitive to individual test point examples where models assign low probability (density) values to very probable subsequent states.

Conclusion

Let me state that the main contribution of our original paper is of theoretical and not of applied nature (as also recognized by the NeurIPS reviews). The experiments in Mujoco were aimed to demonstrate how to algorithmically apply our theoretical findings in a practical setting (using robotics domains as an example). This could have been stated more clearly in the report. While we could show improved initial performance, I do believe that a dynamic scheme for trading off intrinsic vs. extrinsic rewards is an interesting direction of future research (with the prospect to practically demonstrate better asymptotic performance). But the latter line of research was beyond the scope of our NeurIPS paper.",19-198.txt
56,https://openreview.net/forum?id=B1grPT9GTH,0,4,5,"The report aims the Replication Track: exact replication of the presented main results.

The report understands the formulation and the problem statement of the original paper. However, the report only reproduces one of the two algorithms provided in the origina NeurIPS paper. In particular, it only reproduces EAC, but not ACIE. This is crucial because ACIE shows significant improvement over EAC in 3 of the environments studied here (Humanoid, Walker and Hopper). It would have been desirable to evaluate this stronger algorithm in order to extract conclusions about the original paper.

The results reported for EAC are more or less similar to those in the original paper, achieving lower performance in Walker, but higher in HalfCheetah. However, it turns out that, in the original NeurIPS paper, EAC outperformed the baselines only in Ant -out of the 5 environments studied here-, so this improvement cannot be appreciated in the current report due to their lower score in this task.

In addition, the report studies only 5 out of the 8 environments studied in the original paper. The 3 additional environments are lower-dimensional pendulum variants, where the original NeurIPS paper showed that both EAC and ACIE learn faster than the baselines and achieve final performance similar to that of SAC. Since the current implementation is not able to achieve the performance of the original paper in Ant, it would have been useful to at least compare the behaviour of their EAC implementation in these pendulum environments too. 

The authors provide useful recommendations for reproducibility:
- They found challenging to setup the loss functions, so they include a brief discussion about how they computed such quantities, and suggest that providing the gradient expressions would have made this task much simpler. This is a good practical recommendation.
- They also highlight the difficulty of learning the transition and the inverse dynamics models, a note that should be useful for other researchers.
- They explain that since the original paper mentioned DDPG as baseline, but with parameters from the TD3 paper, it was not clear to them if the original paper actually used vanilla DDPG or TD3, so they chose TD3, which seems a more competitive baseline than DDPG. 
- In addition, the authors report that by using other implementation of the baselines, these achieve higher results than in the original paper. 

The authors submitted a codebase reproduced from the scratch. This is a significant amount of work that should be useful to and appreciated the community. As a minor comment, including more comments and docs would help future developments.

The authors didn't mention any communication with the NeurIPS paper authors. And there is no evidence of such communication through OpenReview.

In summary, the report does a good job reproducing one algorithm of the original paper, EAC, and provides useful recommendations on how to improve its reproducibility. However, the main results of the original paper can be appreciated more easily for the algorithm ACIE, which has not been reproduced. Alternatively, the boosting performance claimed by the original paper with EAC in 3 low dimensional environments has not been studied in the current report. 
My main concern is that the authors haven't included these limitations in their discussion, which may cause misunderstanding when evaluating the significance of the reported results with respect to those in the original paper. ",19-199.txt
56,https://openreview.net/forum?id=B1grPT9GTH,0,8,3,"I found the paper well written. In provides a clear introduction, explaining in detailed the method that is being reproduced. 
The results are also compelling. Not only did the authors manage to reproduce results but overpass them by a careful calibration of hyper-parameters and show that the carefully calibrated method underperform against well calibrated actor-critic baseline.

Over-all I found this work quite useful, and I was impressed by the amount of work put into this project. ",19-200.txt
56,https://openreview.net/forum?id=B1grPT9GTH,0,7,4,"Thank you for participating in this reproducibility challenge. This submission reproduces the ""A unified bellman optimality
principle combining reward maximization and empowerment,"" and tries to obtain the results reported therein. Authors have implemented the main algorithm of the original paper, and have provided the base-line algorithms' performance result. Their algorithmic implementation yields similar performance result compared with the original algorithm, however, they have shown that the base-line algorithms are performing significantly better than the ones reported in the original submission. In fact the authors have empirically shown that the proposed empowered actor-critic (EAC) in the original submission is performing poorly compared with base-lines. Furthermore, authors have given details about the challenges and shortcomings of implementing the EAC algorithm; e.g., the difficulty of learning a transition model in complex environments. ",19-201.txt
57,https://openreview.net/forum?id=Bye09vnGpB,0,6,4,"Good reproducibility effort, insightful discussion with the original authors. However, overall reviews are not quite good enough for the AC to recommend to the journal.",19-202.txt
57,https://openreview.net/forum?id=Bye09vnGpB,0,6,4,"Summary: This is a replication work where significant effort was taken to recreate the code from scratch for the work of  Chen et al. 2019. The report's organization is relatively OK. However, I would suggest to include more details both of what are the most challenging aspects of replicating the results and what are the insights obtained (and unresolved doubts) from interacting with the authors of the original paper. Below are a list of pros and cons of this report.

Positive aspects.  
The problem is clearly stated. The replication authors (evaluators) described in succinct, albeit informal, manner -- some mathematical formality in the descriptions could have improved the presentation. 
The evaluators communicated with the original authors through OpenReview to do their replication work.
The strongest aspect of this work is that the evaluators reported 3 main variations of the original architecture described in the original work of Chen et al. 2019.
The code provided for the main experimental setup contains some documentation about what has been done.

Weaknesses
Although the problem is clearly stated, because the replication work could not be completed, it does not have any quantitative assessment of the performance of the method in its current form. It could help to understand how far off the results are w.r.t. the original submission. e.g. distribution of accuracy, or any other performance measure as it related to the original work of Chen et al. 2019.
The code provided (downloadable from Gitlab) has some documentation of what it does in one of the scripts. However a couple of files, and particularly the spectral normalization one, have no code documentation.
Although the evaluators communicated with the authors, there is no information in the document about the insights obtained or any unresolved doubts they may still have.
Finally, I suggest the evaluators to describe what in particular is hard about the replication task? i.e. what aspects of the original paper they still find hard to follow or are unclear? what assumptions they had to make when building their code? what suggestion do they have to the original authors with respect to notation, variables, hyper parameter search descriptions, model configuration, etc. that may need to be more precisely described in order to replicate the results.",19-203.txt
57,https://openreview.net/forum?id=Bye09vnGpB,0,7,5,"This report attempts to replicate the results of the paper Unsupervised Object Segmentation by Redrawing. The paper provide the details in ""MASK EXTRACTOR NETWORK"" and ""IMAGE GENERATOR AND DISCRIMINATOR NETWORKS"", which shows the effectiveness in the two core module.",19-204.txt
58,https://openreview.net/forum?id=Hye_mTczTS,0,8,4,"This report reproduces the approach, SWAG-Gaussian, proposed in (Maddox et.al, 2019). Code was implemented from scratch in a different platform (Tensorflow instead of Pytorch). The authors performed some hyperparameter search for the proposed new baseline (for the parameter alpha). The report authors communicated with the authors from the original paper for testing reproducibility. 

Overall, the report is quite complete, and brings interesting insights on the original paper. The authors were able to qualitatively reproduce the reported gains in the original paper, despite the implementation differences. However, the authors in the report also implemented an additional naive baseline which outperformed all variants of SWAG-Gaussian. This is an interesting and novel insight on the original work, and make one wonder on the utility of the original paper. It also leads to a better understanding and new hypothesis for why SWAG-Gaussian was performing better than others.

Some major comments on the report:
- results for transfer learning mentioned at the end of Section 3, but no supporting results.
- In Eq. (4), why do the authors pick the covariance matrix proportional to the mean? Since alpha is already an additional degree of freedom, why not adopting an isotropic Gaussian centered at \theta_SGD and alpha*I? 
- Finally, code in the git repo is not fully reproducible. Since this is a report in a reproducibility challenge, I would expect all the commands described in the README to execute appropriately, but that was not the case. The code looks correct, only, some care needs to be added in the testing and README files.

Other minor clarity points that could be improved in the report:

- Page 2: add references for calibration approaches.
- Eq (1): a reader unfamiliar with calibration would not understand Eq. (1). Add more textual information. Same for reliability diagrams, to make the report more self-contained.
- Caption of Figure 2: what is the take-away observation? This is scatterly explained in the main text, but it would be nice to summarize in caption.
- Some of the results in the main text correspond to an earlier bug of the authors, whereas corrected results are currently in Appendix A. It would make more sense to switch both. (Still, the reported results with the bug are insightful on the behavior of the algorithm, so should be kept in Appendix).
- specify what you exactly mean by ""vanilla SGD"" versus ""SGD"".
- Explanation for Figure 6 missing.
- ideally, should use same data split as original paper.

- Fig (4a): where is SWA? This should be mentioned somewhere in the caption or main text.
- Fig (4): can the authors comment on why SGD-noise is very well-calibrated in Fig 4a) but is the most overconfident method for CIFAR-100? This indicates to me that different datasets might actually require different alpha values. The authors should add a note about this.",19-205.txt
58,https://openreview.net/forum?id=Hye_mTczTS,0,6,4,"This looks like a thorough enough attempt to reproduce the results of the original NeurIPS paper. 

The biggest pros of the report:
1. The report presents the work in the replication track where the task is to implement the code from scratch. A reader can be ensured that this is the case for this report because the authors implemented the original methods in the different framework.
2. The authors of the report not only did a fair attempt to reproduce the original results, but also presented a new baseline, which  outperformed the contributed method from the original NeurIPS paper. Although this is not the main goal of the task of this challenge I find it a very nice bonus.
3. The authors of the report describe what details of the implementation they lacked in the paper and what they managed to find in the original implementation provided by the NeurIPS paper's authors. This would be a good lesson for everybody how to ease reproducibility of their work and how important is to provide a code with a paper submission.

The report however suffers from couple of big issues:
1. It seemed that the authors first submitted the report where they could not reproduce the SGD results and then resubmitted the report when they were able to resolve this issue. Note, however, that reviewers were assigned to reports after the resubmission had been done, therefore, I, as well as any potential new reader of this report, was unaware of the initial version. In these circumstances the authors’ referrals such as “An error in our implementation meant” in Introduction and “we do not achieve convergence” in Section 3.1 where we already know from footnote 1 that the authors did achieve the convergence are unclear and quite confusing. The authors have chosen to focus on those readers that had read the initial version of the report, rather than those readers who would read only the later version with the new results, however, it seems that it should be other way around.
I also do not quite agree with the authors’ decision how to incorporate the new results in the report. If I am not mistaken (please forgive me if I am wrong) there is no space limit for a report, therefore it is unclear why not to include new results, which are actually reproducing the results from the original paper, into the main body of the report rather than leave it in the appendix. In any case, explanation about resolving the initial issue with reproducibility should be in the main text rather than a footnote on page 3. It seems that it would be easier for a report reader if both new and initial results were presented together with clear description of the difference between them.
2. From the report, discussion on OpenReview and github it does not look like the authors have engaged too much with the authors of the original NeurIPS papers as suggested in the challenge call. It is especially important since the authors of the report have found a new baseline that outperforms the method proposed in the NeurIPS paper and this seems to contradict to conclusions from the original paper. It is slightly out of scope of the challenge but it would be very interesting to see what are thoughts of the authors of the original paper about this finding. It also seems that the authors of the original paper could have been of help to resolve the initial issue of reproducing the SGD results and the authors of the original paper suggested another issue with the reproduced implementation, which was left unanswered by the authors of the report (at least according to the public information available).

Other issues/suggestions:
1. Not clear from Introduction which track this report is about
2. Section 2.1. “the ground truth correctness likelihood” – too complicated
3. Eq. (1) - \hat{P} and p are not defined
4. Section 2.1 would benefit if more references are used
5. In Algorithm 1 \hat{D} and p(y^*|Data) are not initialised, and NUM_COLS, REMOVE_COL and APPEND_COL could have been described at least briefly in the text
6. I find a structure a bit unusual when first the experiments are described in Sections 3 and 4 and their results are presented only in Section 5. I am missing at least a sentence at the end of Section 3 that the results would be presented later. 
7. Section 5. “our SGD-Noise method” – not defined. Based on referral to the parameter alpha it seems to be the method described in Section 4, but it should be named as this there then.

Minor:
1. Section 3.1, second to last paragraph. “… we found that the diagonal covariance matrix … to produce negative values …” – bad wording
2. Section 3.1, last paragraph. “the models ware all trained” – typo “ware” -> “were”
Hi Victor and Josef,

Thanks again for choosing our paper for the NeurIPS reproducibility challenge. After starting to look into your implementation (thanks again for providing the codebase), we think that you're including a randomly initialized model in the SWA solution causing worse performance.

I opened a git issue (https://github.com/DD2412-Final-Projects/swag-reproduced/issues/1 ) mentioning this issue along with a couple of others a couple of weeks ago and was wondering what your thoughts were on that.

Best,

Wesley",19-206.txt
59,https://openreview.net/forum?id=SkegBa5zTH,0,7,4,"Good reproducibility effort, well written paper which adds to the understanding of the original paper. However due to space constraints and relative impact, AC is unable to recommend this paper for the journal. Although, the AC acknowledge and agree with the reviewers on the contribution of the reproducibility effort and hope the readers and original authors gain useful insights from the findings.",19-207.txt
59,https://openreview.net/forum?id=SkegBa5zTH,0,8,4,"This paper has two major contributions. First, the paper reproduces from scratch the optimizer proposed and one of two experiments from the original paper ""Rethinking Binarized NN Optimization"". Second, the paper closely examines a claim from the original paper: the approximation claim.

The reviewed paper shows a great amount of work reimplementing and analyzing claims of the original. The most interesting part is the experiments concerning the approximation claim. While the original has only vague mentions that the common perception in the literature is incorrect. This is the perception (let's call it ""approximation hypothesis"") that the binarized networks approximate real-valued vectors. The reviewed paper conducts multiple experiments and conducts proper hypothesis testing. These experiments don't show that the approximation hypothesis is incorrect. This work shows that we need more work in this direction to understand this phenomenon and should be insightful for the community.

The second part reimplements the optimizer and tests it with CIFAR dataset. The experiments seem to be conducted well and show that the original work can be reproduced. This part could benefit from the comparison with the baseline reimplementation. It would be interesting to see if the difference of the results applies to the baselines as well as the Bop algorithm. It is also notable that the learning curve in the original work looks smoother than in the reimplementation. 

The paper is generally written well. There are several typos (e.g. let -> led) and the paper can benefit from another round of proof-reading. I found it difficult to read the sections on methodology and results because I needed to jump back and forth to check which experiment is which. Perhaps, it is better to structure the writeup differently or allow some repetition in the Results section.

To sum it up, this is a very good work on reproducing other's research and an example of scientific critical thinking. The source code is clean and easy to follow, but I haven't had a chance to run it myself. There are some points to improve in writing and experimentation. I recommend accept.",19-208.txt
59,https://openreview.net/forum?id=SkegBa5zTH,0,7,4,"(Apologies for the delay in submitting the review)

Firstly, I would like to thank the authors of the report for participating in the challenge and making the effort to reproduce the results from the original paper.

The report tries to replicate the results from the original paper where the authors propose a novel optimization algorithm, BOP, to train binary neural networks. While the original implementation of the optimizer and associated modifications to layer architectures were done in tensorflow, the authors try to reproduce the results by re-implementing the setup in PyTorch in addition to verifying the results based on the codebase released by the authors. Furthermore, the authors also dig deeper into the experimental motivation centered around the “approximation viewpoint” in the paper.

- The report is well-written and is easy to understand. The authors do a good job of first highlighting the approach presented in the original paper briefly and then succinctly describing the setups they try to verify / replicate.

- Upon further investigating the approximation viewpoint presented in the original paper, although the authors find inconclusive results in their attempt to disprove the claim they do point out the fact that conducting the same set of experiments on deeper architectures and different datasets would probably provide a holistic understanding of the claim. I like the fact that the authors delved deeper into this aspect and conducted experiments surrounding this claim in the original paper.

- The experiments conducted by the authors are thorough and clearly highlight the takeaways surrounding -- (1) the role of batch-normalization in training binary neural networks (with BOP or otherwise), (2) a suggestion to interleave batch-normalization in optimizer updates and (3) the role of hyper-parameters described in the original paper.

In summary, I think the report is well-written and highlights the takeaways succinctly based on the experiments conducted. Furthermore, the authors clearly highlight portions of the implementation with which they faced issues, describe the subtle underlying implementation-level differences involved while porting over results from one framework to another and provide feasible hypotheses for differences in performances compared to the original paper wherever applicable.",19-209.txt
59,https://openreview.net/forum?id=SkegBa5zTH,0,7,5,"This work aims to reproduce the paper ""Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization"", specifically the Bop (Binary optimisation) under the framework of PyTorch. The reviewer has checked the code, which has good coding style and organization. Although the code of this work is light-weight, the authors do try to study the hyperparameters with ablative study (Sec 3 & 4). The results are well-discussed (Sec 4 & 5). The draft is well-written and easy to follow. Overall I think this work if among Top 20% in all reproducibility challenge reports.",19-210.txt
60,https://openreview.net/forum?id=HJeWSpczTS,0,8,4," 
I provide my review following the Reproducibility Challenge Reviewer Guidelines as follows.

- Problem statement: The problem in the original paper is clearly described. I would like to suggest the authors draw a figure to illustrate the ReDO network.
- Communication with original authors: It has been done.
- Hyperparameter Search: all hyper-parameters specified in the original paper are searched.
- Ablation Study:  There is no ablation study in the paper. In the original paper, there is also no ablation study.
- Discussion on results: There are many interesting discussions on the results.
- Recommendations for reproducibility: In Section 3.4, the findings are useful to improve the original publication.
- Overall organization and clarity: The paper is well organized and the writing is good.",19-211.txt
60,https://openreview.net/forum?id=HJeWSpczTS,0,7,4,"This report clearly states the problem statement and approach proposed in the original paper. By communicating with original authors, the report points out several inaccuracies of the original publication including incorrect model parameters. Based on reproduction results. it showed that one of three data sets can be reproduced. It also discussed the reasons why the original results on LFW and CUB data sets could not be reproduced. I agreed with the statements in this report that reproducing results would be difficult due to vagueness in referenced works and missing information about model parameters in detail.  This report is in good writing and it is easy to understand. ",19-212.txt
60,https://openreview.net/forum?id=HJeWSpczTS,0,6,4,"Annotating image at pixel-level for object segmentation is an interesting but challenging task in Computer Vision. This work is to examine the adversarial model ReDO developed by Chen et al. that is capable of unsupervised segmenting foreground-background of small images. This report understands the problem statement of the original paper and reproduces some of the results of the original study, while also providing TensorFlow 2.0 implementation of ReDO.

The author reproduces some parts of the original author’s repository using TensorFlow 2.0. However, there are only results from one dataset. The results from the other datasets such as LFW, CUB are not tried out.

This report does mention fair communications with the original authors for testing reproducibility. There are several rounds of discussion on OpenReview panel. The original paper author has also updated their arxiv paper according to the communications.

The paper has done its due diligence to try out the hyperparameters sweep and more. It is interesting to see that the report has found that different loss function such as sigmoid + cross-entropy makes the system performs slightly better than the original one.


This report does not provide very comprehensive ablation studies though the report has produced close results like that in the original paper.


The report contains a detailed discussion on how to reproduce the original paper using TensorFlow 2.0 on Flowers dataset but missing the results from other datasets LFW, CUB, Colored-2-MNIST.

The authors of this report have provided very good communication and recommendation to the original authors to improve the reproducibility of the paper, which is exactly this challenge is for. The original authors have even updated their papers and code based on the discussions here.


Grammatical issues / organization / proper plots

I think this report would be improved with the following points.
1. The figure 2 does not provide clear visualization. It would be better to provide the difference map between the prediction mask and ground truth one. It would be also great to provide the difference map between the prediction mask from the original paper and the ground truth one.
2. It is a bit messy to mention a few datasets in the paper, while in the end there are no experiments about these datasets at all. It would make the paper better to exclude those datasets that are not considered.
3. The current report does not provide a very detailed explanation about how to train the networks. For example, the learning rate, and how to search for the hyperparameters are not discussed and presented in the report. It would require readers to go over the code to find those hyperparameters that are already working, but it would be difficult to find out what has been tried out.
4. This report is not about semantic segmentation, but rather foreground-background object segmentation. There are a lot of very different literature review papers to be included and referred to. ",19-213.txt
61,https://openreview.net/forum?id=H1lPS65z6B,0,4,4,"The report belongs to the Reproducibility track.

Problem statement: the report clearly states and understands the problem statement of the original paper.

Code: the results are reproduced from scratch. The code is in Keras, as the official PyTorch & Tensorflow code is available. The authors provided the code. However, the code is not very well organized in a reasonable structure, including many replications. The README is somewhat too simple.

Communication with original authors: The authors do not seem to communicate with the original author. However, I guess it is OK since they have achieved decent results.

Hyperparameter Search: the authors used a simplified hyper-parameter search due to lack of computation power, which could potentially lead to different results. I think this is not a proper practice since the method itself is quite simple. I would like to see more from the hyper-parameter search.

Ablation Study: no new ablation studies. The authors did compare with the baseline (Adam&SGD) but did not mention the results from the original paper, making the report less clear to read. Furthermore, since the original paper is a novel optimizer, more tasks and datasets are needed to back the conclusion. It is hard to tell if the improvement is consistent on a larger scale dataset or other tasks.

Discussion on results: contains some level of discussion, but I think it is not very new from the original paper.

Recommendations for reproducibility: Little.

Overall organization and clarity: Not so good. The report is clear but somewhat simple. The reproduction results are not enough to back the method. Many of the contents are directly taken from the original paper. I would also like to see a better presentation of the plots and tables.",19-214.txt
61,https://openreview.net/forum?id=H1lPS65z6B,0,5,4,"The participants have successfully reproduced the LookAhead (LA) optimizer, and they released their code in https://github.com/shv07/lookahead-optimizer. 
The reimplemented the LA optimizer in Kears due to the original authors have released the code in Pytorch and Tensorflow. In this version provided by participants, they understood the algorithm and made a clear reproduction (though there are some differences, e.g., the definition of 
k
). Therefore, overall speaking, reproduction is good. 

As for the experiments, the participants conducted on cifar10 and cifar100, due to lack of time and computational power. In these two tasks, they have shown the fast convergence and better results of LA optimizer. Though I understand the limitation of time and computation power, with only 2 seeds for each experiment and the similar cifar10 and cifar100 experiments, I still feel little concerned to make a clear answer that the experiments can be reproduced in both image classification (cifar10) and text processing tasks. Especially the differences are big between text and image processing tasks. 
The hyperparameter search and ablation study is not too necessary since this report is in the Replication Track. 

I suggest the participants do a further experiment on PTB language modeling task since this is not a heavy task. The dataset is small, and the training should be faster. If LA performs better, the convergence will also be quick. I would be happy to increase the score if this is answered. ",19-215.txt
61,https://openreview.net/forum?id=H1lPS65z6B,0,6,4,"For reference, the review below follows the structure suggested here: (https://docs.google.com/document/d/1Fj4E1IHFBGdXp19A93_sI06SlpPEAw4YMWcFDMAtb5U/edit)

Problem statement. In this report, the authors aim to replicate (without using the original authors’ code) a subset of the results reported by Zhang et al. in the work “Lookahead Optimizer: k steps forward, 1 step back”. The authors understand the problem statement of the original paper (which is essentially to design a better first-order optimizer that operates well in the deep learning setting).

Code.  The code is clear and readable. However, I was a little surprised when reading the code to find that: 
(1) The authors have implemented an algorithm that is somewhat different to the algorithm described in the original paper.
(2) Their algorithm still works remarkably well!

The difference relates to the steps over which the lookahead applies. In the original paper (Algorithm 1), k minibatches are sampled and used to perform updates with the inner optimizer. This is followed by a single update of the outer optimizer via weight interpolation. By contrast, In the implementation supplied with the replication report (and described in the report itself), k full training epochs (i.e. many, many minibatches) are completed prior to performing the weight interpolation for the outer optimizer. Of course, this can still be viewed as the algorithm described in the original paper with an incredibly large value of k, but given the difference I would expect qualitatively different behaviour. Nevertheless, the authors demonstrate that this too produces very good results on CIFAR-10. It can perhaps also be interpreted as a form of stochastic weight averaging [1] without cyclic learning rates. 
On a more minor note, in the original paper the authors recommend learning rate step changes at epochs 60, 120 and 160. However, in the code associated with this submission, the changes take place at epochs 80, 120 and 160 (i.e. there is a discrepancy in the first step epoch).

Communication with the original authors. NA.

Hyperparameter Search. The authors note that due to limited computational resources, they perform a simplified hyperparameter search, but they don’t specify precisely what these are. E.g. Does this search include learning rate, batch size etc. as well as k value? However, since this is for the Replication track, rather than the Baseline or Ablation tracks, this point is perhaps not critical.

Ablation studies. The authors perform an ablation by comparing their implementation of Lookahead to standard baseline (the Adam solver) on CIFAR-10. This seems appropriate for a Replication Track paper.

Discussion on results. The authors note that (their implementation of) lookahead proves effective on CIFAR-10 (though shows less improvement on CIFAR-100).  In that respect, it provides evidence in support of the original paper.

Recommendations for reproducibility. The authors make two suggestions for reproducibility, advocating for the inclusion of lighter network architectures and additional seeded runs.  
On the first point, while there I agree that there is a benefit to reducing computational cost in supporting reproducibility, I also consider it valuable that Zhang et al. demonstrate the effectiveness of Lookahead in the large-scale regime (e.g. ResNet-50/152 on ImageNet). Additional seeded runs, however, are certainly always welcome.

Overall presentation and clarity. The overall presentation is reasonable. It would have been helpful to include training/validation curves for CIFAR-100 (as is done for CIFAR-10), so that the reader can get a sense of the consistency of the trends.

Recommendation. As noted above, I don’t believe that the accompanying codebase implements Lookahead as it is described in the original paper. Nevertheless, the fact that it still works at the epoch-level is an interesting result in itself. Since this may prompt good discussion, I would recommend that the submission is included at the workshop (but the AC should feel free to override this if appropriate). 


[1] Izmailov, Pavel, et al. ""Averaging weights leads to wider optima and better generalization."" arXiv preprint arXiv:1803.05407 (2018).",19-216.txt
62,https://openreview.net/forum?id=SJxcS6qG6B,0,4,5,"Problem statement

The problem is well introduced and motivated. There is not enough information on the notation used in DILATE objective functions however. 

Code

Source code from original paper was used. No additional code was submitted by the replicators.

Communication with original authors

The replicators report contacting the original authors with no success.

Hyperparameter Search

A coarse evaluation of gamma is conducted with 6 log-space values between 10^-3 and 10. I doubt this evaluation is informative however because it is done while only measuring the DILATE loss, while gamma is the coefficient of the shape term. Scaling up the coefficient will inevitably affect the DILATE loss directly. The metric to evaluate the effect of the coefficient should have been a side-effect, otherwise we are measuring the coefficient.

The evaluation of alpha is more informative as it is reported with MSE, comparing against TDW and TDI, the two terms of DILATE. I note however that this evaluation was already part of the original paper and is thus more a replication attempt than a more exhaustive hyperparameter search. The discrepancy with respect to original results is surprising. The original authors explain from the same experiment that alpha=0 leads to exploding MSE/shape losses because the time loss (TDW) is only meaningful in conjunction with the shape loss. The replication does not corroborate this.

Ablation Study

I did not see any ablation study in the report. To be fair, I don’t see how we could conduct an ablation study on the DILATE loss function beside varying alpha to turn one part or the other off, and this experimentation was already part of the original paper.

Discussion on results

The discussion of the results is too brief. The results on the replication of results with respect to different alpha required more discussion because of the discrepancy. Also, original results should be reported as well to ease comparison. I only realized the discrepancy of results in figure 4 after looking at the original paper.

Recommendations for reproducibility

There are no recommendations for reproducibility in the report.

Overall organization and clarity

The report is well organized and the text is overall clear, but there are several slips. The plots are visually clear but lack information which makes it difficult to interpret them. The table is difficult to read and I do not understand why 1 line in particular is bold.",19-217.txt
62,https://openreview.net/forum?id=SJxcS6qG6B,0,7,4,"The authors provided an ablation and replication study of the DILATE method. They used the code provided by the original paper and made an attempt to contact the DILATE authors to receive additional materials from the supplementary. They replicated the original study (though with 5 instead of 10 trials) and evaluated performance on an additional dataset. A hyperparameter sensitivity study was also performed to assess the impact on the different choices of target metrics (MSE, DWI, TDI). Some summary conclusions and recommendations were made, namely that DILATE may not be worth the computational cost given at best marginal performance gain.

Overall, this is a good investigation into the stability, reliability, and performance of the DILATE method. I would have liked a little more intuitive conclusions being drawn as I felt the presentation was a bit scattered and hard to follow. Nevertheless, the authors have made a real contribution here.",19-218.txt
62,https://openreview.net/forum?id=SJxcS6qG6B,0,4,4,"The Authors replicate DILATE and compare with with MSE for two time series forecasting architectures.

I think the contributions in the paper are not sufficient to warrant a workshop paper and the paper writing is not high quality:

- It is not clear how hyperparameters are optimized for different loss functions. Ideally, one needs to reoptimize the learning rate parameters for a new loss function for a fair comparison.

- The impact of \alpha shows random fluctuations, it is better to present averaged results.

- Since the paper does not introduce any novel technical content, I would expect to see more experiments. Comparisons on only two architectures are insufficient to draw strong conclusions. 

- ""We could not reproduce the Hausdor distance and Ramp score as there was no access to the supplementary material"" - this is another weak point, I would expect to see more analysis on the reasons of this. ",19-219.txt
63,https://openreview.net/forum?id=B1xxUaqGpr,0,4,4,"This report assesses the reproducibility of the NeurIPS paper Fast AutoAugment ( Lim et. al.) which learns augmentation policies using efficient methods.  The original paper show their methods works on different models and image classification datasets.
 The reproducibility report basically asks whether  the  augmentation policy learned for  ImageNet dataset will transfer to other datasets. 

Clarity: The report also has a lot of grammatical errors and the writing is not clear in many places.

Pros:
They find that compared to non-augmenting the model, using policies learned from FAA helps in 4 different datasets.
Cons:
Need to improve writing and explaining key concepts.
Their experiment compares cutout augmentation with FAA augmentation policies learned for ImageNet. However, it is not clear that what augmentations were useful for other datasets. Since MNIST is binary while ImageNet  has RGB images.
While the idea of the original paper is to find dataset specific augmentation policies faster, I am not sure if transfer of the polices across dataset is the main theme since it involves many other analysis such as domain difference etc. Also the original paper also performs such experiments in Table2 and 3.",19-220.txt
63,https://openreview.net/forum?id=B1xxUaqGpr,0,1,5,"- Problem statement - The submission makes a very brief attempt to introduce the problem solved by the paper in question, but does not go into any depth or relates it to the issues being studied.
- Code: Was reworked and submitted
- Communication with original authors: No communication with authors was mentioned or apparent on openreview
- Hyperparameter Search: No details of the hyperparameter search are mentioned at all, which is particularly problematic as this paper is submitted under the ablation track. 
- Ablation Study: The presented experiments are on transfer learning which, 1) is not a purported issue of the paper in question and 2) no components of the paper are ablated and then tested.  It seems that this is not an ablation study at all, which is concerning as it was submitted under the ablation study track.
- Discussion on results: There is none
- Recommendations for reproducibility: Minimal, but technically correct.
- Overall organization and clarity: No plots, clearly written and does not follow the structure of a paper, completely lacks related work section does not describe the study in any depth does not motivate the study well, and seemingly does the wrong study. ",19-221.txt
63,https://openreview.net/forum?id=B1xxUaqGpr,0,6,3,"This paper conducts four experiments to evaluate the FAA algorithm. The authors employ the augmentation policies searched on the ImageNet dataset, and compare them on four small datasets with Cutout augmentation. The experimental results show the searched policy on ImageNet is also effective when transferred to other datasets. 
The paper gives detailed experimental settings, and provides possible scope of further improvements. I have two concerns about the paper:
(1) It would be better to evaluate the transferability of ImageNet policy on Cifar-10 and Cifar-100. The original paper of FAA conducts policy search on these two datasets. Comparing the transferred policy with searched policy can help better understand the transferability.
(2) There are some typos. For example, in the second line of section ‘Scope of Improvement’, ‘This formulation takes a makes a strong assumption’.",19-222.txt
63,https://openreview.net/forum?id=B1xxUaqGpr,0,4,4,"This report performs ablation study for the paper ""Fast AutoAugment"". It only performs a very simple comparison between ""only augmenting using cutout"" and ""using the learned auto augmentation policy"", and points out that the learned policy works transfer to smaller dataset.

Overall I find the ablation study performed in this report very simple. It does not touch the components of the ""policy search"" phase, but only compared the learned policy with the cutout.

I would expect that the authors to perform ablation study on the fast auto-augment policy search algorithms. For example, comparing against a simpler search space, varying K value in the K-fold data separation and etc.

The current report, although verifies certain claims of the paper, does not provide further insights on top of the original paper.",19-223.txt
64,https://openreview.net/forum?id=BJlz8pqz6S,0,4,4,"This paper tests the baselines and the replication of the main results of the original paper, verifying both, using their own implementation of the models from the details in the paper without referring to the original authors’ code. In addition, they test variation of certain hyperparameters (fixing of the learning and adaptivity rates and weight dropout). They also test the performance of the algorithm on new problems. 

Nonetheless, I find this paper to be of unacceptable quality and hence my recommendation is a clear rejection. The paper suffers from poor organization, ad hoc experimentation, unsatisfactory presentation of the results and the discussion thereof, and many grammatical mistakes. 

Poor organization: The authors do not mention the track (baselines, ablation or replication) the paper belongs to. It doesn’t lay out the motivation (need) of the study and the scope of the proposed work (and how it’s grounded in the original work).  

Clarity: The paper suffers from many grammatical mistakes. Section 2.2 needs to be improved. 

Ad-hoc experimentation: 
(a) The original paper carries out a hyperparameter search for \gamma and \tau for the experiment on CIFAR 10 and presents a good analysis of the results. Further, they mention in Section 6 Discussion that hyperparameter schedules would be a useful direction to investigate. In this report, the authors test the original settings against scenarios when the learning or adaptivity rates are fixed (as against the original variable rates) and find that the performance suffers. These experiments have no value since it’s common knowledge that variable rates are almost always better than fixed ones. 
(b) Similarly, dropouts with BNNs are known to be an issue (conceptually) - original authors acknowledge that in Section 6. This study doesn’t add much value either.
(c) SVHN dataset is larger but of the same order of magnitude with same number of classes. I’m not sure how well this dataset tests the scalability of Bop on BNNs.
(d) Image segmentation on the Oxford-IIIT PET dataset: This is a much harder problem but the results are garbage and there is no reason why the hyperparameters tested out should work on UNet and why hyperparameter search would not be needed.    

Inadequacy of results and discussions: is a major issue with the paper. Since experiments are the backbone of this work, I expect more care to be taken in this section. I’ll provide a few examples of the loose handling of this part of the work below:
(a) Section 3.1 Replication of Original Results: Since the model was run thrice, the difference of means can’t by itself be used to deduce similarity/ dissimilarity. The spread needs to be reported as well. 
(b) For clarity, the parameters for the baseline should be mentioned together with parameters for the proposed Bop algorithm (ref. Table 1).
(c) It’s hard to make out in Figure 1, if BoP indeed improves the training accuracy from 91.02% to 91.6%.
(d) Section 4.1: Changing the learning rate from variable to fixed achieves training and validation accuracies of 96% and 91% respectively. However, for the variable learning rate, only the test accuracy is mentioned (91.02%). There is no way to compare.
(e) That fixed learning rate regimes may saturate the learning is well known. I’m not sure what is the supposed takeaway from “Although the results are not really conclusive, the understanding here is that a lot of learning occurs in the first 100 epochs of the training phase which causes constant flipping of the gradients …”. 
(f) Section 4.2: “However, there is a sudden change in loss/accuracy after the 100th epoch which is quite surprising. The magnitude of this change decreases after the 100th epoch, but still persists on the 200th epoch, and fades away during the later stages of the training phase.” And Figures 4,5: It is not clear if the learning/ adaptivity rate decays every 100 epochs (presumably so, as those are the default settings). If so, then that very well may explain the above behavior. Why is it surprising then?
(g) “This is due to the fact that, dropout randomly introduces zeros into the network which violates the basic principle of BNNs.” This is highly speculative. Though one may argue (philosophically or even from an implementation POV) about dropout (setting weights to 0) on a BNN (weights take values +1 or -1), from an empirical point of view, it shouldn’t be an issue. And, the violation of this basis principle of BNNs is clearly not the cause of the drop in the performance.
(h) Section 4.4 SVHN Dataset: The reported accuracy of 92% for baseline and 93.2% for Bop (test accuracy?) doesn’t agree with Figure 8.
(i) Section 4.5 Image Segmentation: The results in Figure 9 just show that with the chosen hyperparameters, the training was unsuccessful – for the baseline, BNN and the proposed Bop algorithm. Since there is no reason to expect that the chosen hyperparameters to work, the entire discussion in the section is meaningless.

Also included below is the feedback on metrics suggested by the reproducibility challenge.

Problem statement: It understands and the problem statement of the original paper is ok.

Code: For this report, the code is reproduced from scratch without referring to the authors’ repository. 

Communication with original authors: There’s no indication that the authors have communicated with the authors of the original paper.

Hyperparameter search: Hyperparameter search is not carried out in this report. Learning rate, weight decay and dropout have been experimented with. However, these choices seem ad hoc and the conclusions drawn are unsatisfactory (and necessarily inconclusive). It is important to note that the original paper does a better job of hyperparameter search and in explaining the behavior of the network as a function of changes in their values. 

Ablation study: No ablation study carried out in terms of the core architecture, or core aspects of the algorithm (or tricks thereof). 

Discussion on Results: This is one of the main drawbacks of this report (details above). 
 
Recommendations for Reproducibility: This study tests the reproducibility of the baseline and main results of the paper using their own implementation of the author’s code and are able to replicate the results. They don’t have any further recommendations to the authors. 

Overall organization and clarity: The overall organization and clarity of the paper is poor.",19-224.txt
64,https://openreview.net/forum?id=BJlz8pqz6S,0,7,4,"The paper is an attempt to reproduce the results in the ""Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization"" paper. The authors have put in significant effort to not only reproduce part of the results in the paper, but also tested this on various different datasets and tasks.

Here are some of my comments: 

1. The authors made a significant effort in replicating the results in the original paper. The code for the original paper was published, but the authors implemented the code themselves regardless. I appreciate this effort, this eliminates the chances of overlooked bugs in the original implementation.

2. The authors reproduced the results in the original paper on Cifar10. The authors also run the experiments on different learning rate schemes. 

3. The authors implemented the tested the results on extra tasks and datasets, such as SVHN for image classification and the Oxford Pet III dataset for image segmentation. 

A few things that I think could be improved on.

1. The motivation of the dropout experiment is not totally clear to me. It could help if the authors could write a little more about the motivation behind these experiments. 

2. The authors could be a little more rigorous on experiments on different hyperparameter settings, such as selecting a range of learning rates and run experiments for various learning rates within the range. 

3. The report could be slightly better written. It is not totally clear to me why the authors performed certain experiments (for example dropout or why not run the algorithm on different learning rates).

Overall, I think the authors made a genuine effort towards reproducing this paper. Not only they were able to reproduce some of the results in the paper, but they further extended the work to test this method on various other datasets and tasks.",19-225.txt
64,https://openreview.net/forum?id=BJlz8pqz6S,0,2,4,">>> Overall comment
The goal of this track was to work on the baselines of the original paper to check if they were fairly tuned.
Possible ways to achieve this goals included:
 - Performing an hyperparameter search on the hyperparameters of the baseline
 - Do ablation studies to better understand which aspect of the baseline could affect its performance and explain the allegedly good results of the main paper.

This paper focuses mainly on modifications to the approach (Bop) proposed by the original authors (OA). This is different from what the baseline track required.

Positive points:
The Reproducibility Authors (RA) coded the baseline and proposed approach algorithms from scratch, even though they were not required to as the code from the OA was available.
They managed to reproduce the results of the OA.
They conducted several additional experiments, and particularly evaluated the OA proposed approach and the baseline on a different dataset, and with a different architecture (the Unet).
This can be time consuming, and I believe that the efforts to apply the technique should be noted.

Negative points:
Besides the off-topic argument, the paper overall lacks clarity:
- on the denomination of the models
- on the experiments the RA exactly performed

Most importantly, the baseline and its hyperparameters are never precisely defined.
It is unclear what the initial learning rate of the baseline is, as well as its other hyperparameters (beta1, beta2, epsilon). 
The OA mention that they use Xavier learning rate scaling, which is not mentioned in this report. There is no mention of the batch size either, and the scheduling is unclear.

In the experiments section, the motivation and methodology are not consistently stated. As a consequence, the conclusions of the experiments are unclear, since it is not obvious what aspect is being evaluated by the RA.

I would suggest to rework parts of this report, especially to  be more precise with the terms that are used, and clarify the experiments section. 
In my opinion, this would greatly improve the quality of this report.

>>> Detailed review

> Motivation:
The motivation of the original paper is well stated.
The description of the base mechanism is not always clear (especially Eq.3) and could use some additional sentences to detail the procedure.
The RA do not define formally the « latent weights » in Sec. 2.1, despite being in the title, and the denomination « weights » is used both for the latent weights and the binarized weights, which is confusing.

The behaviour of the OA proposed optimizer seems to be understood but the explanations need clarifications or additional formula since it’s the central part of the original paper.

> Organization of the report:
- The structure of this report was well-organized.

> Hyperparameters search and ablation studies on the baseline(s):
The RA did not study the effects of the hyperparameters of the baseline (which had some!).
The RA applied the OA approach and the baseline on a dataset not originally tested, which is an interesting study to make. However, the analysis of the comparison is short and some mistakes make it hard to follow (the right plot from Fig. 8 plots the ""testing accuracy"" but the caption of Fig. 8 mentions the validation accuracy).
Some of the conclusions are not clearly explained/backed by plots or data (regularization, overfitting of the baseline) and the metrics reported in the text (92%) do not match the plots' metrics.

> Reproducibility check:
The RA managed to reproduce the results from the original paper from scratch, which is positive and a nice effort.
However, there is no detailed analysis of this reproducibilty besides the mention that the metrics were matched.

> Grammatical issues:
- Some sentences are missing a verb (Sec 2.1, opening sentence), several are missing words. 
- There is an extensive use of the passive voice, which is confusing when trying to understand who did what.
For instance, in Sec 4.4: « the binary model was also tested on SVHN dataset [8] by Stanford University »: tested by whom? The OA ? The RA ? Stanford university ? After comparing with the original paper we understand that the RA did that experiment, but this should be clear from the first lecture. Use rather: « We tested the binary model on SVHN [8] ».
- The results are « stochastic » (Sec. 4.2): the idea that the RA want to express is not clear.
- The RA often refer to « the model » (Sec. 3 intro, Sec 3.1, Sec 4.1, …) which is unclear: do the RA refer to the baseline ? To the OA proposed approach ? To the OA version ? To the RA version ?
- The OA proposed approach’s abbreviations (Bop) is not consistent: BoP (abstract), bop (Sec. 4.4), Bop (Sec 4.5), plus variants with the use of italic. In Sec. 4.4 for instance, the OA proposed approach is spelled 4 different ways.
- The baseline is only really defined in the caption of Fig. 1 (Adam baseline), and referred before and after simply as « the baseline », despite that the baseline should be the central part of this study.

> Plots:
The plots themselves were overall clear, besides Fig. 8 where the caption contradicts the y-axis label.
The captions could be more descriptive to highlight what is noticeable in the graphs.
It would be better to generate them in a .pdf / .ps format to make them more readable and allow for zooming.

>>> Additional comments on the experiments section

> Learning rate
The original architecture learning rate schedule is not described precisely before the experiments section or in the learning rate section (Sec. 4.1).
As a consequence, the modifications that the R.A. performed compared to the original architecture are not understandable. 

Moreover, it is mentioned for the first time in Sec 4.5 that the learning rate was « dropped by 0.01 » every 100 epochs , « as per previous experiments ». At this point, I don’t know if « as per previous experiments » includes the OA proposed approach, the baseline, the RA implementation’s of the proposed approach, the RA’s implementation of the baseline, or any combination of those.
Moreover, a quick look at the Original Paper seems to indicate that the original decay was of 0.1 and not of 0.01.

The RA study the learning behaviour over the training epochs, but I am not sure to understand what is the point of this study, since the motivation of this experiment (Sec 4.1) was not clearly stated.

The RA mention that there is no degradation of performance when compared to the baseline model, but there is no clear comparison made: for instance, the testing metrics are not plotted nor given.
Moreover,  it seems that the RA did not change the baseline in this experiment, which was the point of the baseline track.

- «  The model achieves » -> The RA’s version of BoP ? Of the baseline ?
- «  A lot of learning » -> imprecise wording.
- « Although there is a difference of 6% in accuracy […] » VS reported training accuracy 96%, validation accuracy 91%, which gives a 5% difference.

> Dropout
The motivation of this experiment is not clear.
The learning rate (of which model?) is said to be kept constant, which is in contradiction with the statement that the learning rate is decreased every 100 epoch (Sec. 4.5).

Note: could the change in accuracy after the 100th epoch be due to the decay of the learning rate after 100 epochs?

> Weight decay
The motivation and methodology of this section is clear. 
The presence of a conclusion on what was evaluated is positive.
However, the modification affects the OA proposed approach, and not the baseline, which is not mentioned in this section.

> SVHN dataset
Evaluating the method on another dataset is an interesting experiment to compare the performance of the proposed approach and the baseline on a new test case.

However, the motivation stated is a bit confusing: the RA mention that they want to test the performance of the proposed approach in a large dataset, after stating that the SVHN dataset is similar to the CIFAR10 one.

The RA mention that the training lasted for 300 epochs instead of 500. A justification (time constraints, for instance) would be helpful.

The authors use the same hyperparameters that for CIFAR10. One possible interesting study would be to try to change the hyperparameters to see if they can get the baseline to outperform the proposed approach.

> Image segmentation
The RA evaluate the proposed approach on a different type of architecture.
There is however no clear comparison between the baseline and BoP.
The RA rather focus on a comparison of the two binarized methods with the original Unet approach, which, while interesting, does not seem to be the purpose of the baseline track.",19-226.txt
64,https://openreview.net/forum?id=BJlz8pqz6S,0,6,5,"The report has clearly understood and stated the BOP methods of the origin paper. The provided code reproduced from scratch, and gived baseline experiments on CIFAR10, extended experiments for ablation study and experiments on other tasks.

The results of CIFAR10 experiments in the report are similar to those in the origin paper, with even better performance. The hyperparameters such as learning rate, dropout, weight decay, also have been investaged to verify the performance of the methods. The experiments of hyperparameters show that the BOP methods need carefully tuned hyperparameters for good performance. The experiments on SVHN and image segmentation also show that the BOP methods with simple hyperparameter settings cannot perform better than BNN.

There are still some weaknesses of the report.
- The experiments on ImageNet have not been reproduced, which is important to practical applications.
- The hyperparameter experiments were only conducted with 1 or 2 values. The detailed ""Hyperparameter Search"" is neccessary.
- The training and testing accuracy in image segmentation experiments are consistently low and unstable. I wonder whether the models were well trained and converged.",19-227.txt
65,https://openreview.net/forum?id=BJeEIT9fTB,0,6,4,"The report clearly summarizes the problem statement of the original paper ""Making AI Forget You: Data Deletion in Machine Learning"". The authors have chosen the replication track for their reproducibility study. Consequently, they focus on reimplementing the three algorithms (baseline k-means, Q-k-means, and DC-k-means) and rerunning some of the experiments from the original paper.

The reimplemented code is provided as jupiter notebook on Github. It is written in a readable way with comments documenting the return values in the base class and computation steps in longer functions. As the notebook also describes how the code was used for replicating the experiments this amount of documentation is sufficient. As expected for a independent reimplementation there are differences in the code: most notable the original code stores a cluster index for each data point, while the reimplementation uses lists of data points to describe each cluster. This may explain some differences in the run time of the two implementations, but, of course, the choice of hardware has an effect, too.

The report does not mention communications with the original authors. For the replication track hyperparameter search and ablation studies are not required and the authors did not do any.

The results support the claims of the original paper and the differences are discussed in sufficient detail. However, the comparison in section 7.2 could be improved further by showing the results of the original paper side-by-side with the results of the reproduction (similar to section 7.1). And I recommend to add the standard deviation of the loss ratio.

As the reproduction was successful, the authors did not provide any recommendation to the original authors for improving reproducibility. The report is well written and clearly organized.",19-228.txt
65,https://openreview.net/forum?id=BJeEIT9fTB,0,7,4,"The manuscript presents a replication study on the Ginart et. Al paper “Making AI Forget You: Data Deletion in Machine Learning” which addresses the problem of efficiently deleting individual data points from trained machine learning models by proposing two deletion-efficient approaches: a quantisation-based and a Divide and Conquer tree-based k-means. The authors of this report have been able to back up the results from the original paper. 

- The report clearly defines and describe the experimental setting of the original paper. The overall organisation and clarity of the document is excellent.
- The authors were able to successfully implement both the proposed algorithms from the description of the algorithms in the original paper/appendix (they didn’t need to contact thus with the original authors for testing reproducibility). Also, the implementation of all algorithms/baselines is now provided in a github repository, which is good.
- The report contains a summarised discussion on the state of reproducibility of the original paper. Results obtained are very close to the values reported by the original authors (for some experiments, results are even better).

Itemised comments:

- Why the authors use 5 out of the 6 original datasets when analysing clustering performance? “Gaussian” dataset made from a Gaussian mixture model is missing. Authors should have contacted to Ginart et al.
- Table 4 and 5 should be just one table, as Table 1 in the original paper.
- Not running all the datasets when analysing the amortised times makes the analysis a bit incomplete. Authors should have look for other compute resources (https://reproducibility-challenge.github.io/neurips2019/resources/)
- Any recommendations to the original authors for reproducibility?",19-229.txt
66,https://openreview.net/forum?id=SkgPIpcGar,0,5,3,"Good reproducibility effort, adds to the understanding of the paper. However, overall reviews are not quite good enough for the AC to recommend to the journal.",19-230.txt
66,https://openreview.net/forum?id=SkgPIpcGar,0,7,3,"This submission reproduces an experiment in ""When to Trust Your Model: Model-Based Policy Optimization"" and also fixes some minor issues in their theoretical analysis.
Due to resource limitations, the submission only reproduces the training curves in figure 2 of the original paper. This leaves out the model generalization (fig 1), ablation (fig 3), and model exploitation (fig 4) experiments.
The authors report difficulty in making the baseline methods to work. In particular, they could not PETS to train.
To make MBPO run as described in the paper, they had to make several choices that were unstated in the original paper.

The quality of the writing is sufficient, and I believe it replicated the original paper well.

pros
- the replicated results have a similar tendency to the original paper
- the authors make an observation that was not highlighted in the original paper: while MBPO is sample-efficient, it takes the most time per step.
- they correct lemma B.4 of the original paper, which leads to slightly different statements for theorems 4.2 and 4.3

cons
- the reproduced experiments seem to show single runs of each algorithm; these results would be more reliable if they aggregated multiple runs and displayed error bars.
- while the authors pointed out the relative time-inefficiency of MBPO, they did not numerically compare the time-efficiency of MBPO to its baselines; I believe this simple comparison would have made this argument much more convincing.",19-231.txt
66,https://openreview.net/forum?id=SkgPIpcGar,0,6,4,"This report is about the replication of “When to Trust Your Model: Model-Based Policy Optimization” under the track of “Replicability.” 

The report includes clear statements about the understanding of the task and the background of the problem. Before introducing the details of the replication, the authors give detailed explanations and analysis of the theory in the original paper. Some improvement suggestions (with proof) for the original theory statement are given, as shown in Section 4.3, which may be helpful for the replication work. The authors also introduce replication setup and difficulties. The implicit assumptions for replications are introduced in Section 4.4.3 in detail. 
The authors give recommendations based on the theoretical and empirical studies. 

The authors show the main results of the replication in Figure 2 and 3, which show similar trends as the curves shown in the original paper (shown in Figure 1). However, I still can see many differences between the reimplemented results and the original results, especially about the training stability/variance of MBPO. Detailed analyses and discussions on the replication results are absent. The authors describe the modifications and assumptions in the implementation in Section 4.4.3. I think some results showing the effects of the implementation tricks or assumption would make the report more informative. Ablation studies or hyperparameter studies are not included. 

I did not find the code from scratch or any way to access the codebase mentioned in the report.

The organization of the report can be improved. Many detailed and informative details are distributed in different sections in the whole report. ",19-232.txt
66,https://openreview.net/forum?id=SkgPIpcGar,0,4,3,"This replication report explains an attempt in replicating the methodology and results of the Model-Based Policy Optimization (MBPO) algorithm. The authors re-implement the model and replicate the main findings of the original paper. They run some experiments and, for the most part, replicate the main results in the paper with some minor failures in exact replication. Authors also point out that while MBPO can be efficient in terms of fewer steps, it can take more time, thus not suitable for tasks that require fast learning, which is an interesting insight. 
While most of the report is well-written, I think it lacks important details as a replication attempt. The most important one, in my opinion, is the lack of comparison with the original paper’s method. The code for the original paper is available online and any replication report should make an attempt to run that code and compare it with their own implementation (more details below). 

Major comments:
* It is important and necessary for authors of this replication report to make their code available online and reference to it in the paper. 
* It would have been (and would still be) helpful for authors to get in touch and communicate with the original paper authors about their questions and concerns. I couldn’t find any evidence of existing communications in OpenReview and there was no mention of communication in the report.
* The authors don’t compare their replication with the results generated by the available code online. One crucial result that should be included in figure 2 is the curve from the original paper’s code. Additionally, there seems to be a lot of fluctuation in the re-implemented curve, which requires further investigation and analysis.
* It would be helpful to include a section in which authors make explicit recommendations on how to improve the original paper. Authors point at recommendations implicitly throughout the paper (e.g., specification of each task, adding details on the hyperparameter search) but having a separate section would be ideal.
* Authors of the report should clarify how they specify hyperparameters in their own implementation. This is an important detail that the original paper misses and as a replication report, it’s important to include it. 
Minor comments:
* Section 2, PPO: authors should expand on the claim on why MBPO has “little connection with PPO”. 
* Authors should expand MuJoCo and add a reference.
* Y axes fro plots don’t have a label.",19-233.txt
67,https://openreview.net/forum?id=rkedITqfpB,0,6,4,"Note: The link to the NeurIPS paper ID in the description is incorrect; it should link to the following paper from NeurIPS 2019: /forum?id=HJlr9NBgUr . This does not pose a problem for the replication report.

Overview: The authors of the replication study present a replication of the NeurIPS paper ""A Meta-MDP Approach to Exploration for LifelongReinforcement Learning"", which proposes to use a meta-learning algorithm to derive exploration strategies that have the potential to generalize across environments.

Code: The authors of the replication provided code to reproduce their replication experiments.

Communication with the original authors: The replication authors did a great job of inquiring about missing experimental details in the original paper, including defining the task sampling procedure, and the feature representation being the Fourier basis.

Discussion on results: The replication study focusses on the Cartpole environment. While this is the simpler environment of the two in the original paper, the study is nonetheless thorough and brings to light that it is difficult to replicate the improvement claimed by the original paper on this simple environment. The most informative result from the replication study is that the baseline algorithm (in which no meta-learning advisor is used to provide the exploration policy) can be tuned to reach reward similar to what is reported as the proposed method in Figure 3(a) in the original paper. The replication study of Figure 3(b) reaches similar conclusions.

Recommendations for reproducibility: The authors clearly describe their difficulties in reproducing the results of the paper, and highlight incomplete tuning of baselines as well as underemphasised sensitivity to hyperparameters as possible causes.

The overall organization and clarity of the reproducibility study is very solid. However, specific parameter setting are missing from the paper (although this omission may be mitigated by the fact that code has been provided).

Minor:
- It is not clear in the reproducibility report whether the clarification from the authors details in Section 4.2 aided in the reproduction.
- I was surprised by the memory footprint of the method (40GB GPU memory) on the simple Cartpole environment. How much of this footprint is due to implementational choices that are specific to the reproduction?",19-234.txt
67,https://openreview.net/forum?id=rkedITqfpB,0,5,3,"The report described the authors' attempt to reproduce the work ""A Meta-MDP Approach to Exploration for LifelongReinforcement Learning"". The report does a reasonable job in describing the problem to be solved and the approach proposed by the original paper. 

The authors implemented the original paper from scratch and performed experiments with one of the problems in the original paper, the cartpole problem. The authors have reported several communications with the original author, through which implementation details are clarified. However, the reported experiments did not match the ones presented in the original paper. The authors provided some discussions regarding the possible reasons behind this.

I think in overall this is a reasonable effort in reproducing the original work. However, given that the reported experiments did not match the original one, and that the authors of the original paper have published their codebase, it would be valuable to contrast the two codebases and investigate the cause of the discrepancies in the performances. In addition, the authors mentioned that they tested different hyper-parameters in their experiments, yet no details or results were presented in the report. It would be great to have more details about the hyper-parameter search results.",19-235.txt
67,https://openreview.net/forum?id=rkedITqfpB,0,4,4,"Problem statement:
The report understands the original paper's algorithm and problem setting correctly. 

The report mentions communications with the original authors about the policy model with a Fourier basis. However, no more implementation details are mentioned. What are the training hyperparameters chosen in the original work and in this report remain unknown.

Although, the report claims ""experimented with a variety of ranges for each parameter"". However which parameters are searched and the ranges are not explicitly mentioned in the report.

The reproduction results are incomplete. The report only contains results in the CartPole environment (continuous control task) and ignores the Animat environment (discrete control task).

Apart from claiming ""the original algorithm is very sensitive to hyperparameters"", the report contains no detailed discussion on the state of reproducibility of the original paper and does not provide any recommendations to the original authors for improving reproducibility.

Overall, I do not recommend the acceptance of this report.  
The indexing to the original paper is confusing.
/forum?id=HJlr9NBgUr is directing to another paper titled ""A Meta-Analysis of Overfitting in Machine Learning "" with the pdf. 

However the abstract, the author and the code are for the paper ""A Meta-MDP Approach to Exploration for LifelongReinforcement Learning"".",19-236.txt
68,https://openreview.net/forum?id=S1xcL6qfpr,0,4,4,"Thank you for taking the time to reproduce this paper! Hopefully it was an interesting and constructive process!

This study does a good job of summarizing the motivations behind the original work, and the high level methodologies. It could be more clear on the lower-level methodologies, including a discussion of how easy (or not) it was to replicate, sticking points during the replication, and decisions that the authors made that were different from the original authors. 

Ultimately, it was difficult to make an apples to apples comparison between this work and the original paper. I think rewriting the paper and perhaps regenerating plots to make the comparison more clear would make this work a lot more defensible. I would reject this paper on these grounds, but hope that the authors revise this work so there is a clearer comparison. Reproduction studies have the opportunity to clarify and produce dialogue, and I’m excited to see the authors rework this paper to make both papers more clear.


What is the relationship between epochs, cycles, and episodes? It is difficult to compare figures 3 and 4 and figures 5 and 6. We don’t know what 1000 cycles means vs. 2*1e6 steps. We also don’t know how the two y-axis instructions/episode vs. reward/cycle) differ. This makes it hard to interpret which region of the graph we should analyze.

Figure 4 would benefit from including plots in addition to HIR 600. Either the same plots figure 3 includes (non-compos 600, HIR 10k+, or no HIR 600), or the one-hot encoded representation model if that is the relevant comparison.

Figure 6 would benefit from following the same color code as figure 5 to make it easy to compare between the two charts. It would also benefit from the same labels—switch out “one hot 1 bin” for “onehot 600” for consistency. 

This may have also been a recent change, but the original authors updated their version of figure 3. If revisions are made, please update to the new graph.

Additionally, since we’re hoping to compare figures 3 and 4, and figures 5 and 6, it would be better to put both plots next to each other instead of vertically spread on the page. 

Qualitatively, it appears that the results for onehot 600 is very different in the original study versus this replication, but there are many differences between the plots that make it hard to draw conclusions. 

Since the authors only test the low-level policy, it’s unclear to me what high-level policy they are acting against. Please provide an example high-level policy. Additionally, the paper would benefit from examples of the instructions.

I didn't see where the ""minor modifications as to better reproduce the results from the paper and possibly provide some improvement techniques"" are but please update the paper to include them. That would be incredibly valuable.
The last paragraph of section 3 would be valuable to expand on and possibly reach out to the original authors about. 

The paper makes an attempt to calculate the amount of time it would take to run the original setup, but the conclusion is vague. A better approach would be to include the cost to reproduce, including the amount of time the authors spent reproducing the code, the time/monetary cost to training, and engagement with the original authors. 

More questions & nits:
* Include why the original authors never reached a total of 12.5 million training steps even though they intended to train that long.
* Write out the acronym the first time it's used. For example HIR in the abstract or MDP in section 2. 
* Please fix the subscripts in the description of the algorithm in section 3. As a suggestion, you can annotate on top of figure 1 to help clarify the algorithm (Which matrix is P? E?)
* ""scheme. It was still for the..."" --> ""scheme and unchanging""
* “any training if the authors did not stop training early. We” —> “any training for the full 12.5 million intended training steps. We”",19-237.txt
68,https://openreview.net/forum?id=S1xcL6qfpr,0,4,4,"Summary:
• Problem statement
The authors show a good understanding of the paper’s problem.

• Code: 
The authors reimplemented the model themselves in pyTorch.
Codebase is submitted at the end of the report as Supplementary Material.

• Communication with original authors
They did not communicate with the authors, although they needed some implementation details. 

• Hyperparameter Search
They tuned the number of bins per representation for the oneHot encoding setting. 

• Ablation Study
Not relevant for this work

• Discussion on results
The authors just reported their findings but without any meaningful discussion. (See details below)

• Recommendations for reproducibility
The conclusion of the report included some suggestions; however, it would have been more useful if some of these suggestions were tested by the authors.

• Overall organization and clarity
The report is well-written, but the plots are very confusing are the main weakness of this work. 

Details: 

Strengths:
1- The authors have a very good understanding of the problem. 
2- The language is very clear. The report is detailed and well organized. 
3- The authors tested a key hyperparameter in the algorithm, the bins for the oneHot encoding of the questions. 

Weakness:
1- The main problem of this report is the clarity of the figures.
a. When the authors explained the graph of the model, they used the same figure that was used in the original paper. That’s good, but in their explanation of the figure, it is not clear where the networks F1 and F2 are (in the figure), also the figure has f_3 that is not mentioned in the authors' description. 
b. When reporting the results, the authors used different terms for the axes and reported different values than what the original paper reports. Specifically, for Figures 3 and 4, the original paper used “steps” and “instruction/episode” while the authors used “cycles” and “reward/cycle”. These two figures should be easily comparable, however, given the differences, it is not possible. As a reviewer, I cannot tell if the two figures are similar or not. 
c. The same problem with Figures 5 and 6, they are replicating the same figure but using different names for each curve. They do match the names in the paragraph, but I cannot see a reason why not just use the same names. 

2- One of the authors' findings show that with more instructions (Section 5.2) the authors saw a better performance, in contrast to the authors' findings, where the performance decreased. The authors did not discuss this result beyond reporting it. Their result is also counter-intuitive to me, as one expects the performance to degrade as the low-level policy must learn a more diverse set of instructions. This seems like a problem in the authors’ experiments that should have been, at least, commented on in more detail. 

3- In terms of run time, running an experiment for 2-days on a CPU is not considered a long time in modern-day. I get that the experiments take time as reported in the original paper, but also, I think it is doable for a replication project. 

Thank you for your efforts. ",19-238.txt
68,https://openreview.net/forum?id=S1xcL6qfpr,0,6,2,"In the attempt of reproducing the results of the original Jiang et al. paper, ""Language as an abstraction for 201 hierarchical deep reinforcement learning. CoRR, 2019"", mixed partial results are obtained.
On the one hand, part of the claims in the original paper are confirmed:
*following the paper, *able* to train and confirm the low-level policy
*changing the bin size *does* in fact affect the agent’s ability to learn
On the other hand, the full results and many of the claims in the original paper are yet to be reproduced. It is said partly due to computational limit and partly due to the missing of few w key implementation details.

Overall, it seems a promising reproducing effort is presented. But much work (probably a bit half of the total goal) is yet to be reach to draw a complete picture of the reproducibility of the original paper. ",19-239.txt
69,https://openreview.net/forum?id=Bylj86qzTB,0,7,3,"This work belongs to two tracks: the baseline track and the replication track. In the baseline track, the authors tried to reproduce part of figure 4 of the original paper, which shows the performance of one-hot encoding with instruction sets. In this part, the authors showed a result that is in sharp contradiction of the NeurIPS paper. 

Additionally, in the replication track, the authors tried to reproduce the low-level policy training results. The authors show the HIR-model outperforms the baseline, which is consistent with the NeurIPS paper. However, the authors weren't able to reproduce the full result due to incomplete information about the experimental settings in the NeurIPS paper. ",19-240.txt
69,https://openreview.net/forum?id=Bylj86qzTB,0,6,3,"The original NeurIPS paper introduced the architecture of using language as the abstraction between the high-level policy and low-level policy in the hierarchical reinforcement learning. All the authors of this paper are from Google Research. So the computation of the original paper was backed by powerful resources. The reproducibility study of this NeurIPS paper is conspicuously challenging. Efforts in this direction deserve encouragement and applause.

Meanwhile, I find some problems from this report.

1. Is the word ""Step"" in the original NeurIPS paper equivalent to the word ""cycle"" in this reproducibility report? If so why two words, or otherwise what is the difference? This is very confusing. Let me guess that they are equal for my further reading.

2. In the original plot, the ordinate variable is ""instruction / episode"", while in the report the variable is ""Reward per Cycle"". Again, are they the same? Do they have the same unit? This looks confusing.

3. Since in the original paper, the minimum unit of ""Step"" is 100K, while the total number of ""cycles"" the report finished is just 1k, the results seem not comparable, and I feel confused about any conclusions drawn based on this comparison.

4. In subsection 4.4, the laugnage seems vague, in the sense that while the authors of this report admitted that the original paper used 2 days for 2M steps of computations, the authors did not explain why the Google computation is so prohibitive that they managed only to finish 1000 cycles. --- perhaps I am wrong, if ""cycle"" does not equal to ""step"".",19-241.txt
69,https://openreview.net/forum?id=Bylj86qzTB,0,4,4,"1. Problem statement - 
The report shows an understanding of the original paper’s goal and problem.

2. Code -
The code does not have any Python version or package information documentation aside from general links to some libraries used. No requirements.txt or Pipfile are provided and the repository does not have any documentation in the readme about how to run the code. This codebase is extremely poorly documented and difficult to run.

3. Communication with original authors -
I found no evidence of communication between the report authors and the paper authors.

4. Hyperparameter Search -
The report authors state they followed the hyperparameter search provided by the original authors. This is sufficient for Track 3.

5. Ablation Study -
The report authors did not conduct an ablation study. This isn’t problematic for Track 3.

6. Discussion on results -
The report shows their ability to reproduce a portion of the paper’s results that focuses on the primary value of the paper. The report discusses their lack of ability to reproduce certain results of the original paper providing some details on what was missing from the original paper’s provided materials and methods, namely paraphrasing and synonym substitution. 
It is important to note that this reproducibility study did not attempt to reproduce the high level policy portion of the original paper. 

7. Recommendations for reproducibility - 
The report briefly mentions missing portions of resources that seemed necessary to reproduce the work. 

8. Overall organization and clarity -
The report contains some grammatical errors and is brief with regards to details about their own experiment, understanding and related work. Overall, the report is poorly written. The report’s organization is generally okay. Plots and images have a large amount of spacing between them making the report seem light on content. Some plots are provided and are relevant but take up too much space and could use more descriptions. Plots from the original paper should be referenced instead of having a screenshot of the plot taking up a large portion of a page in the report.",19-242.txt
70,https://openreview.net/forum?id=H1x3LpqzaB,0,7,4,"The reproducibility report is very clearly written, and easy to follow.

To being, there is a clear problem statement, of both the original paper and the goals of the reproducibility study.  Then the experimental methodology is detailed with an appropriate level of detail.  Section 7 is particularly insightful, regarding some discrepancies, weaknesses or gaps in the original work that came to light during the reproducibility study.

It is worth noting that lack of computation limited the reach of the reproducibility study, in particular limiting experiments to just a few runs.

But overall, I feel this is a strong example of a reproducibility report that provides an insightful view on a recent model.",19-243.txt
70,https://openreview.net/forum?id=H1x3LpqzaB,0,4,4,"The replication paper documents the learning process as the replication study was performed.  One key point the authors identify is the lack of clarity regarding the annotations. Another contribution of the replication study is section 7.4.1. I concur that the average F1 is not meaningful do to the varying entropies. It may be complementary to use average mutual information as that would provide more bits to higher entropy variables. The sentiment in section 7.5 is well founded.

However, many of the statements are not germane to general replication, but rather were points that required careful implementation (e.g., the discussion of negative and positive examples for the noise contrastive estimation, and lack of background on mutual information). 

In terms of a replication study, the experimental results are hindered by seemingly large discrepancies in training procedures and limited scope of the replication (a single game). This replication study seems preliminary and in its current form incomplete. While the replication need not be exhaustive, it is unclear what the experimental results contribute.

Other specifics: 

Section 4's comments about clarity should be revised with regard to the camera ready version. 

Section 4.3, clarify if the ""main challenges"" are the replication steps or issues. Also there is a missing word ""each image in the was""

""matrix multiplying the result with the other feature"" -> ""taking the inner product with the other feature vector""

Section 4.4, should clarify if the 256 values are treated as different output classes or if this was a regression task. From the paper it is clear that it was the former, but in this work the relationship between the layer form (soft-max?), the byte values, state variable, and the categories is not clear from the text alone.  

In Section 4 there is no mention of training epochs, etc. In Section 6, 'a single run' is mentioned. It would appear this is a random initialization of the networks, but should be clarified. 

In section 6.2 the graphs show that the supervised learning is not gaining any information regarding the probes, except a peak for ball location. As the original authors open review comment point out, the original paper used early stopping. This may also highlight a deficiency in the training set.

Overall, the replication study highlighted points of ambiguity and discussion of appropriateness of performance. In terms of replication itself the results are incomplete and limited. I would encourage the authors to distill their discussion for a future publication or discussion piece. ",19-244.txt
70,https://openreview.net/forum?id=H1x3LpqzaB,0,5,3,"Summary
========
In this report, the authors replicate the paper “Unsupervised State Representation Learning in Atari” from scratch. The authors discuss the challenges they experienced while attempting to reproduce the proposed method, as well as point out some errors/solid suggestions to improve the original paper. Due to limited compute resources and time, they were only able to run experiments on one of the 22 Atari games examined in the original paper.

Detailed comments
=================
Overall, I find the report well-organized and easy to follow. The authors appeared to have gone to great lengths to understand the proposed method and examine the new benchmark. However, I’d like to point out that, unlike what the authors state in Section 1, the original authors did not claim ST-DIM is able to facilitate transfer learning and increased efficiency (albeit it is generally agreed upon that better representations should help).

One of the major issues I have with this report is that it does not contain any link to the code, despite it being in the replicability track. I believe opensourcing the code would shed more light as to why the results couldn’t be reproduced and help the original authors improve their paper.

Another concern is the lack of results presented and the seeming lack of communication with the original authors. With these, it seems difficult to fully determine whether the original paper is reproducible or not. That being said, I do appreciate the authors’ explicit mention of the time spent on data collection and training, and the hardware they use. This can help interested practitioners to have a better understanding of what to be expected if they were to train the model.

Finally, here are some comments/questions:
- Can you provide a link to your code?
- I noticed that the original authors have addressed some of your concerns in the comment section. Out of curiosity, now that the official implementation has been released, have you checked the reference code and pinpointed what could have caused your results to be different?
- It is puzzling that the supervised baseline has a lower score than Random-CNN? Any new thoughts on this?
Hi team,

Thanks for taking the time to read and reproduce our paper.

It seems you were working off an earlier arxiv version. We want to note that we fixed some errors and improved our exposition significantly in the camera-ready version and the latest arxiv version. We realize this was unavoidable for you since the reproducibility challenge started before the camera ready deadline.

- You correctly point out that we didn't elaborate enough on ST-DIM in the original arxiv submission, we received the same feedback from the reviewers and improved the clarity of that section in the camera-ready / arxiv update. We hope that the updated version of the paper, along with our reference implementation (https://github.com/mila-iqia/atari-representation-learning/blob/master/atariari/methods/stdim.py ) is now sufficient to reproduce ST-DIM numbers. To further make ST-DIM clear, we created a new figure for the global-local loss (https://i.imgur.com/6BMQVRW.png) that more elaborately explains the method. 

- For probing, could you verify if you used the same hyper-parameters as in Table 5? We notice you mention you used the same number of steps for training and probing, but we used 35k probe training steps and 70k encoder training steps. We also note that there was error in our earlier submission mentioning that the probe learning rate was 5e-2, however we actually ran all our probe experiments with a learning rate of 3e-4 with the Adam optimizer and early stopping. This was also similarly fixed in an arxiv update and the NeurIPS camera ready submission.  

- The supervised learning numbers were the most puzzling to us. For the same classification task, the supervised learning scores should be higher than random-cnn. Our guess there might be a bug in the probing setup or there's overfitting on the training set (we use early stopping to prevent this). 

- We'll take a second look at the ram variables you mentioned, thanks for pointing them out. For Breakout however, we have 30 different ’block_bit_size’ variables because each variable corresponds to the presence of a block in the game. When collecting data with a random agent, most if not all blocks are not affected during the game and thus their values doesn't change. This is why the entropy pruning procedure ignores these variables during probing. 

We once again thank you for taking the time to reproduce our paper and provide feedback on different parts of the paper. We'll use the feedback to improve the clarity of writing in the paper wherever possible. 

Finally, we want to point the readers to our open-sourced implementation of different methods and the probing benchmark used in the paper:https://github.com/mila-iqia/atari-representation-learning",19-245.txt
71,https://openreview.net/forum?id=HJlCUp5M6H,0,4,3,"This paper is a replication of the paper ""Goal-conditioned imitation learning"". The paper describes a re-implementation of the goalGAIL algorithm, and tests it on the Fetch Pick and Place environment from the original paper.

Strengths:
- This paper does a good job of describing the goalGAIL algorithm, and uses considerable space to do so. 
- The paper goes into detail describing how they approached their reimplementation
- The paper describes some anecdotal difficulties into the reimplementation (e.g. using rllab which has been discontinued), which I find helpful

Weaknesses:
- The main weakness of this paper is that the scope of the replication isn't clearly defined. Once we get to the results section on page 6, we find that the authors were only able to run goalGAIL on one environment, for one seed, for part of the training time of the original model. I totally understand that computational limitations make it hard to completely replicate a paper. However, based on Figure 2 there seems to be a lot of variance, so all that can be said is ""it seems like goalGAIL learns a bit faster in steps 15000-20000"". Even this might be okay, if this was very clearly stated by the authors at the beginning of the paper by describing the scope of the replication.
- I found it difficult to distinguish between which parts of the algorithm description were in the paper, and which were inferred by the authors when doing their replication. Thus, it's hard to figure out how reproducible the original paper was in terms of giving appropriate details for implementing the model correctly
- There is little analysis of how well the original paper did in terms of making their work reproducible. The paper states some hyperparameters were 'originally unspecified' in the original paper, but I wasn't able to find which hyperparameters these were

Overall:
Given the above weaknesses, my current recommendation is on the side of rejection. 

Small fixes:
""It a supervised learning problem""
Formatting could generally be improved, by making more use of subsections (also, underlined text for headers is pretty non-standard)
It'd be nice for Figure 1 to be higher quality",19-246.txt
71,https://openreview.net/forum?id=HJlCUp5M6H,0,4,4,"== Problem statement ==
I think the paper does a good job at clearly stating the main problem tackled by the paper: to combine the fast convergence of GAIL and the sample efficiency of HER to obtain an agent that can quickly learn a goal-conditioned policy that is able to reach any goal and that can outperform the expert.

== Code ==
I couldn’t find the code in OpenReview. I also tried looking for the authors’ repositories, but I couldn’t find it. This is disconcerting since the sole purpose of this track is to reproduce the original results from scratch, which is not possible to validate if no code is included. 

== Communication with original authors ==
It is not clear if there was open communication with the authors. I couldn’t find any discussion in OpenReview either. 

== Hyperparameter Search ==
This is not applicable since the report was submitted to the replication track (Track 3). The experiments in the report used the same hyperparameters as in the original paper.

== Ablation Study ==
This is not applicable since the report was submitted to the replication track (Track 3).

== Discussion on results ==
The report contains useful information about the lack of maintenance of RLLab, which is used for the environments in the original paper. This could potentially hinder replicability for anyone that is trying to obtain comparable results in the same environments.

Moreover, although not framed as a comment about reproducibility, the authors of the report expressed that it was difficult to replicate the experiments due to how computationally demanding were the environment and the algorithms. The authors are even upfront about this issue by mentioning that the number of steps used for their replication is clearly less (at least 10^3 times less!) than in the original paper. I don’t hold this against the authors of the report since demonstrations in our field are getting incredibly big; that in itself greatly hinders reproducibility. 

On the other hand, the conclusion section of the report is not well justified since the conclusions are based on just one run of only 20k environment steps, which is 10^3 times less than in the original paper. The report should limit the scope of its conclusions to the reproducibility of the paper and not the performance of the algorithms.

== Recommendations for reproducibility ==
The report does not include any recommendations about improving reproducibility.

I have a comment nonetheless. I consider that if we want to improve reproducibility in our field, authors should be encouraged to also provide empirical evaluations in small domains that anyone could run on their desktop or laptop. For example, experiments in environments such as mountain car or cart pole are very light and don’t require any GPUs to run. It is very likely that authors are already running their algorithms in these light environments as a proof of concept for their algorithms, so we should encourage authors to also include those results in an appendix so that other researchers can quickly validate the results of the paper. 

== Overall organization and clarity ==
The organization of the paper is hard to follow due to the lack of numbering for the subsection. This makes it very hard to understand which sub-subsection corresponds to each of the different subsections or if they all are supposed to have the same hierarchy. 

There are several typos and grammatical errors in the paper. Moreover, some of the equations have notational or formatting errors. For example, the weights of the discriminator network of GAIL are denoted first with w and then with psi. ",19-247.txt
72,https://openreview.net/forum?id=rkgbDp5z6H,0,6,3,"Overall, this report has reproduced the core findings of the original paper, Distributional policy optimization: An alternative approach for continuous control. The original paper tests GAC on a variety of continuous control tasks in the MuJoCo control suite[1], but the replicated code was only tested on two OpenAI Gym[2] environments. Although the test environment in the replicated code is different, it successfully achieves similar results compared with the original paper.

The reproducibility report figures out the main contribution of the original paper, a novel reinforcement learning algorithm called the Generative Actor-Critic (GAC) for continuous control problems. The authors reimplement the proposed GAC algorithm and reproduce an experiment on the MuJoCo Humanoid task to verify the core idea. The experiment is the same as that in the original paper. The reviewer thinks the paper is clearly presented and captures the key technology of the original paper.

This report provides a high-quality open-source repository ( https://github.com/gwbcho/dpo-replication) with detailed comments and installation guide in Github.  The authors adopt TensorFlow as the basic network framework, which is different from the original PyTorch implementation.  The repository can be considered as reproduced from scratch.


One main shortage of this reproducibility challenge report for the replication track is, as they claimed in their report line 2-4, paragraph 1, sec.1, that they only reproduce one of the six experiments of the original paper. The limited experiments may reduce the credibility of the report.

The other shortage is that this report claimed several interesting observations but didn’t provide detailed results to support their observations, for example, 

1) In paragraph 2 of Sec.,  the report claims that experiments running on the replicated algorithm actually achieved higher rewards than that in the original paper. The reviewer is hard to justify the observation.

2) The report states that the TD3 algorithm used in this experiment achieved substantially greater rewards than the original paper.  It lacks sufficient justification for this result.

3) In paragraph 3 of Sec. 5, the report claims that running without certain components (regularization of the critic function), the algorithm was unsuccessful at reproducing the results seen in the original paper’s experiments.  It is hard for the reviewer to get such a conclusion.

4) The paper also claims that the implementation could converge faster than the original paper after implementing Gaussian noise with a standard deviation of 0.2 during the training phase consistent with line 6 in Algorithm 2.  More explanations can be added to depict such observations. 

[1] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026{5033. IEEE, 2012.
[2] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",19-248.txt
72,https://openreview.net/forum?id=rkgbDp5z6H,0,7,3,"# Summary
The reproducibility report reproduces some of the results in Tessler et al (2019), namely Figure 4 (humanoid) in Tessler et al (2019). The authors conclude that the overall result of the proposed method can be reproduced. Although the authors show that other benchmarks, such as TD3, are equally good as the method proposed by Tessler et al (2019). This indicates that there is a difference between the reproduced and the original work concerning the comparisons/benchmarks with, at least, TD3.

# Quality
The reproducibility report is in general of good quality, although the report only replicates one Figure/result of the original paper.

# Clarity
The report is clearly written and easy to follow.

# Originality 
It is hard to judge the originality of a reproducibility report.

# Significance
I think the reproducibility report sheds new light on the Tessler et al (2019) paper.  The results of the proposed algorithm can be reproduced, but it does not seem to beat state-of-the-art, something that differs between the paper and the reproducibility report.

# Questions
1. Do the report authors know why there would be such a large difference between the reproducibility report and the original paper when it comes to TD3? 

# References
Chen Tessler, Guy Tennenholtz, and Shie Mannor. 2019. Distributional policy optimization: An alternative approach for continuous control. arXiv preprint arXiv:1905.09855.",19-249.txt
72,https://openreview.net/forum?id=rkgbDp5z6H,0,4,3,"This report is intending to reproduce a paper by Teassler et al. 2019 titled Distributional policy optimization: An alternative approach for continuous control. The code to reproduce original paper's results is available online. The report is in the replicability track.

Overall, I think that the report makes some interesting observations and it is good to see that researchers are putting so much effort in trying to ensure the reproducibility. However, in its current stage I would not recommend this report for acceptance at a journal. For detailed justification see below.

(1) Problem statement:   
- The paper clearly states what parts of the original paper it is attempting to reproduce (Figure 4, Humanoid-v2). 

(2)Code: 
-   The code used to report the results in the report is available online. 

(3) Communication with original authors  : 
- I could not find any evidence of interaction with the authors of the original paper. 

(4) Hyperparameter Search:   
- The report would benefit from clearly stating all the hyperparameters used to obtain final results. There is no mention about hyperparameter sweep in the report. 

(5) Ablation Study:   
- The report introduces some new results that are not original present in the paper: LunarLanderContinous plot where the authors alter the roll out period. The report also experiments with different levels of Gaussian noise (line 6 in Alg.2 in the original paper).  

(6) Discussion on results:   
- It hard to say much about the successful reproduction of code findings just by looking Figure 1 in the report and comparing it to the plots from Figure 4 in the original paper. From the reading of the original paper it seems that the authors used ""5 training procedures with a randomly sampled seed"", however, the report seems to plot only a single seed. Did the authors try running training for more seeds? Given current presentations of the results in the report I do not find sufficient experimental evidence to support the claim of ""this project was successfully able to reproduce the core findings of the original paper's experiment."" 

(7) Recommendations for reproducibility:   
- In the reflection section, the authors of the report provide some details that are missing in the original paper. 

(8) Overall organization and clarity: 
   - Plots in Figure 1 are the main results of the report - the plot should be made easily comparable with the plots reported in Figure 4 of the original paper by using same scales on the axes and reporting results over multiple runs (mean + std) as in the original paper. 
- The report would benefit from writing improvements and proofreading.",19-250.txt
73,https://openreview.net/forum?id=B1lQvpqzpr,0,6,2,"The reproducibility report is extremely well written, and is easy to understand and follow. My current evaluation is based on the following observations:

1. Report states that the reproducibility attempts were only marginally successful, however, the original authors have posted a detailed comment on why that might be the case, and the report authors have not yet posted their side of the story, without which it is hard to get an ""overall picture"".
2. There is no code available at the provided link (https://github.com/attraylor/reproducing_bsr), which further makes the evaluation harder.",19-251.txt
73,https://openreview.net/forum?id=B1lQvpqzpr,0,4,3,"This reproduction replicates the algorithm from the original paper, which is a complicated one. In the interest of time, the authors perform different experiments, for which code is already available.

Ideally, the number of steps would also be included, but I think it's fine to look at cumulative reward instead of number of steps: the only reason an algorithm may fail to obtain the reward of +10 is that it used more than 75 steps. However, Tamas Madarasz raises a very important concern: cumulative rewards of up to 700000 are impossible! This needs to be explained.

The link for the code is broken, you maybe need to set the GitHub to public. I cannot recommend this review for acceptance without available code. This would go some way to clearing up why the reward goes to 700k, but the reason should be explained in the paper too.

It is surprising, but not impossible, that the Q-learning baseline heavily outperforms the BSR algorithm in Figure 2, especially given the learning rate of 1. If this is a genuine find, it's amazing: finding good baselines that should have been tested is one of the purposes of this competition. Given the clear error in the previous figure, though, I'm skeptical of it.

Overall I think this is a decent reproduction, with some fatal omissions that should be easily fixable. I am also pleased to see you have been in contact with the authors and have made several good recommendations to clarify the original paper. 

I am looking forward to a friendly back-and-forth with the authors of this reproduction, to fix all these things! I hope to be able to recommend this reproduction for acceptance after that.",19-252.txt
73,https://openreview.net/forum?id=B1lQvpqzpr,0,5,4,"The authors replicate the experiments in Fig 2a and 2c from the corresponding paper (for SSR and BSR). Seems to be Track 3. 

Fig 2a shows the cumulative number of steps over that an agent takes to achieve a goal. The goal changes each episode. Lower is better. Grid-world task.

Fig 2c shows the cumulative return when using different exploration bonuses. Higher is better. Puddle-world task.

The authors make an excellent effort, and their work will clearly be helpful in future replication attempts, although it is not clear their work is above the acceptance threshold.

I use the rubric and provide an out-of-5 score for each category as guidance, although the final score is not a function of these intermediate scores. Rubric: https://docs.google.com/document/d/1Fj4E1IHFBGdXp19A93_sI06SlpPEAw4YMWcFDMAtb5U/edit

# Problem Statement

The authors explain BSR and its motivation well. (4/5)

# Code

Nice to use an existing library like simple_rl, although can not review code because provided link is not accessible. https://github.com/attraylor/reproducing_bsr (1/5)

# Communication with original authors

Did not contact, and failed to replicate. Would suggest following up with authors to clear up whether interpretation was false or due to instructions from the original work. (1/5)

# Hyperparameter Search

Uses only the best settings from the original work, and provides a new baseline (Q-learning). (3/5)

# Ablation Study (n/a)

# Discussion on Results

The authors note they can not reproduce the algorithm, although it is not clear whether this is a failure in implementation (i.e. a bug in the code), or a failure from the original work. In the authors' own words, discrepancy in results ""may be due to difficulty in the re-implementing the BSR inference step"". This is despite the authors' otherwise clear understanding of the work, and their documentation of the steps in reproducing the algorithms are likely useful in future replication work. (3/5)

# Recommendations for reproducibility

The authors did not provide recommendations on the original work, although one could infer as much from their notes in Sec 2. They mention future dialogue with the original work authors would be helpful, but could have provided more specifics. (3/5)

# Overall Organization and Clarity

The work is easy to follow with clear plots. It would have benefited from plots, results, and analysis that make it clear to what degree the original work was reproducible. (3/5)
First of all, I would like thank the participants of the reproducibility challenge for taking an interest in our work and spending time working on replicating the project.

I am particularly grateful that they spotted some mistakes in the text that evaded us during the proofreading process, such as the typo in the smoothing kernel, as well as swapped labels for two of the control algorithms in panel c of Figure 2. These are important mistakes that have now been corrected. 

Having said that, I am somewhat concerned that the experiments run in the replication effort only partially resemble the original experiments at best, with hard to explain numerical results, making the data provided difficult to interpret. 

Most prominently, the maximum possible total reward in the experiments replicated (Experiment 1 and 2 in the original paper) is 10 for each of the 4500 episodes, or 45000 total. However, the authors of the reproducibility challenge report cumulative rewards of up to 700000 and 250000 respectively, which is impossible to achieve, and is off by an order of magnitude. There seems to be no obvious explanation that accounts for this discrepancy (such as e.g. summing rather than averaging runs), and in private communication with the authors of the report they were also unable to offer a satisfactory explanation. Given the failure the replicate the details of the simple multi-task grid-world setup, the replication of the algorithmic details becomes very hard to parse. However, the following differences from the original paper still stand out for being conspicuous.

In Experiment 1 in the original paper the reward function is always provided (as indicated in the section heading and at several points in the text), and there's no learning of the reward weights w. This allows e.g. setting the epsilon-greedy exploration parameter to 0 for best performance for our main algorithm, BSR. We regret if this wasn’t clear enough from the text and the experiments, and while we find this surprising, will try to underscore this by explicitly giving the learning rate for w as 0 when listing hyperparameter values. As the reward function was not provided in the report for the for the algorithms in Experiment 1, the study of BSR becomes highly problematic, as e.g. it now needs to find unknown goals with a 0 exploration rate.

Further, if Experiment 1 is properly set up, cumulative, undiscounted reward, as given in the report, is not a helpful measure of performance, as all the algorithms almost always manage to reach the goal eventually (as can be seen in Fig. S2 in the original paper). The experiment was specifically set up to see which algorithm finds the best shortcuts/shortest paths, and as such episode lengths or returns should be used as an informative measure, as done in the original paper.

Less significant but still considerable differences include the omission in the report of an initial phase of high exploration for all algorithms (as referred to in the original main text, and descried in detail in the Supplementary Information). Similarly, while the authors of the report choose to also omit replay altogether, they then proceed to use the results of the explicitly described hyperparameter search conducted while using replay in their experiments. Both of these omissions are problematic for several reasons, but at the very least, in case major changes are made to the algorithms, the parameter search should be repeated accordingly, to get comparable qualitative results for those given in the original paper.

There were a number of further issues that concerned us in the report, and we hope to keep the dialogue open with the authors of the as well as the wider community so that we can further clarify details of our method and satisfactorily answer any queries regarding the paper or the accompanying source code. 

Taken together, these issues with the experimental setup have made it quite hard for us to assess how successfully the different algorithmic details were able to be replicated, but we hope that we were still able to make several positive changes in the manuscript as a result of the report.  Again, we are thankful for the authors for their time and their comments, and hope it was an interesting and enjoyable experience for them, just as it was for us.",19-253.txt
74,https://openreview.net/forum?id=rkgLDa5zpr,0,7,3,"This replication reruns some of the main experiments from the original DAC paper. I think it's largely well-done, with good comments / implementation details / minor critiques on the original paper's reproducibility.

A few small questions and suggestions:

1) Plot comparability: It might be nice to show your results side-by-side with the original paper's results (and ideally with the same legend colors), so it's easier to see how they compare.

2) Code reuse: The guidelines for the reproducibility challenge say that ""participants should not use any code which were released by the authors."" If I'm understanding your repo correctly, it looks like your main implementation of DAC is done independently, but some of your baselines (e.g. https://github.com/DAC-Prime/supreme-waffle/blob/master/dac/ppoc.py) are based on Shangtong Zhang's original code. I think that's probably ok since it's only a baseline, but it does mean that your replication wouldn't correct errors in the original paper's implementation of the baseline that incorrectly made it seem worse than it truly is. This might have been related to some of the challenges you mention in Section 3.

3) Minor grammatical issues:
l9: the hierarchical reinforcement learning area -> hierarchical reinforcement learning
l18: option -> options, action -> actions
l19: algorithm -> algorithms
l27: boose -> boost
l82: Our code for experiment -> Code for our experiments
l96: replicate -> replication
l113: output -> outputs
l118: sigmoid as -> sigmoid as its
l164: that large margin -> as large a margin than
l182: underneath -> underlying

Overall this seems good though!",19-254.txt
74,https://openreview.net/forum?id=rkgLDa5zpr,0,6,3,"This work replicates the experiments for single task learning setup of the original work where Double Actor-Critic architecture is used in combination with Proximal Policy Optimization algorithm on OpenAI Mujoco tasks. Along with DAC+PPO, experiments are run for 4 out of 6 baselines - PPO, DAC+A2C, OC, PPOC. Appropriate results with relevant discussions are reported. 
.
The report is very clearly structured and presented. 

Of the two competitor baselines - AHC+PPO and vanilla PPO, this work replicates PPO and observes similar performance on two tasks - HalfCheetah and Hopper. However, the observations for Swimmer and Walker differ marginally. This work does support the thesis that DAC architecture is able to utilize PPO and A2C algorithms.

I would be interested in understanding what other potential reasons might have led to higher stochasticity and the large margins between PPO and DAC+PPO on Swimmer-v2.

Small remarks:
Since Figure 1 is adopted from original DAC work, it might be useful if the caption to Figure 1 includes a mention or citation of the original work. 
Typos:  line 27 boose -> boost
Thanks a lot for your effort in replication. 
Just a minor comment on that random seed bug:
That bug shouldn't have made any difference as I always start a new docker container for each run of each algorithm.",19-255.txt
75,https://openreview.net/forum?id=Hygvw69zpB,0,8,3,"Paper tries to reproduce the results that show that goalGAIL achieves a better policy than GAIL and also converges faster than HER with DDPG. Paper gave a clean explanation and demonstration of DDPG, HER, GAIL and GoalGAIL. They noticed that outside 10^6 (regime not shown in original paper) steps HER outperforms goalGAIL. Their experiments  confirm the results in last two tasks whereas for one task, they were unable to replicate the success of GoalGAIL and reason that it could be possibly because of contamination in their initial setup. The answer to observation about convex to concave in page 4 and experiment results in first two tasks seems worth exploring more.

Paper has some spelling mistakes such as convax in page 4 and ""we takes"" and obverse in page 7.",19-256.txt
75,https://openreview.net/forum?id=Hygvw69zpB,0,7,4,"Authors tried to reproduce the original work and put a lot of emphasis into trying to explain differences. In the end the authors show that part of the results can be explained but that the choice of seed is extremely important. Indeed, they show that changing the import order of modules can affect the use of the seed, which can result in different behavior. Also important is that the authors use as much computation time for their experiments as the authors of the original work did.

This was a very good attempt to try to reproduce results in a hard-to-reproduce domain of work. One way to improve the quality of the replication effort would be to reach out to the original work's authors when there is confusion about reproducibility; the authors seemed to compare against the code released alongside the original work without much communication.",19-257.txt
75,https://openreview.net/forum?id=Hygvw69zpB,0,5,5,"The authors re-implement goalGAIL by combining several open-source implementations, specifically those of Hindsight Experience Replay and Generative Adversarial Imitation Learning.

The re-implementation presents mixed results - while in some domains they observe similar behavior as the original paper (i.e., goalGAIL performs better than the rest of the methods), in others they see behavior which is much different than that of the original authors. This is also seen in Figure 2 which simply runs the original code provided by the authors.

My main concern is:
""However, with limited computing resources and time, we only complete the experiments with one of the three original random seeds.""

Although replicating code and evaluating it is no easy feat, and reporting the results should be commemorated, the main issue is that this has to be performed at a level which is at least on-par with that reported in the paper. As the original authors presented the average across seeds and did not present single-seed performance, it is plausable that they observed similar results to the those in the re-implementation; but when they averaged across 5 seeds (not 3 as is stated in the re-implementation report) the results improve. For instance, it would be better to focus on 1-2 domains and perform better evaluation over a single seed over many domains.

Also, they note that:
""The authors imported and modified several baselines from other public repositories so that different
parts can be better combined and easily invoked. In GAIL, the reward function was modified from its originated form − log(1 − sigmoid(x)) to log(sigmoid(x)) which changes the backbone function from a convax function to a concave function. It could be a factor which makes a difference in the final results since it changes the probability distribution and expectation of the reward, however, this is not mentioned in the paper.""

I think that such an observation should be tested by selecting a single environment and evaluating how this change effects the performance.


Overall I think all authors which decided to tackle the reproducibility challenge should be applauded for their hard work and dedication. This is a huge contribution to the community and I think should continue to happen in future conferences.

However, solely on the merit of this re-implementation, I would vote for rejecting on the grounds of which they did not re-implement all the code from 0 but rather combined several code-bases and the evaluation (even though costly) was performed only over 1 seed.",19-258.txt
76,https://openreview.net/forum?id=BJgtDa9GaH,0,3,4,"This submission is an attempt to reproduce the paper ""Towards Interpretable Reinforcement Learning Using Attention Augmented Agents"" by Alex Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo J. Rezende. There are some inaccuracies in the submission; for example, in the first paragraph of the introduction the phrase ""computation of this optimal behavior"" is not correct - in fact, deep RL normally finds high-quality policies, not optimal ones, given the complexity of the domains involved, which is what justifies the use of deep RL in the first place. The first sentence of Section 3 cites [4], rather than [14]. The last sentence of the submission states that ""the results...indicate the validity of the authors' work"" which does not appear to be the case, given the limited scope of the replication. There are a few minor typographical errors as well. It is unfortunate that resource constraints prevented the authors from a complete study of even one game -- the original paper studied 57 games -- and I wonder whether the authors could have used an alternative RL framework to IMPALA that does not require as many computational resources at the cost of lower agent performance and could even be done without GPU resources. Overall, the code is a good step towards being able to replicate the results of the paper, and contains some interesting observations (e.g., the note in lines 203-204 about choice of padding), but is not sufficiently documented. It would be nice if the authors also included the code for generating the example videos. However, the submission is fundamentally not complete, and it is difficult to infer whether more training time would indeed be sufficient.",19-259.txt
76,https://openreview.net/forum?id=BJgtDa9GaH,0,7,2,"This reproducing paper is in the replication track. The authors briefly introduce the topic and the novel idea of the original paper clearly.  Unlike the original paper, the authors reimplement the algorithm in PyTorch using torchbeast. The resulting program executes roughly 20x slower than the reported runtime. The authors give a detailed analysis of this issue. Although this paper reimplements the algorithm of the original paper, the author didn’t fully reproduce the results in the original paper due to the budget issue. However, the authors give some reasonable analysis of this and conclude that the algorithm in the original paper is valid. Overall, the paper is clear and the organization is good. ",19-260.txt
76,https://openreview.net/forum?id=BJgtDa9GaH,0,5,4,"The manuscript accurately summarizes the main contributions of the paper being reproduced, and includes relevant related work. Figure 1 is particularly helpful for understanding the model architecture being implemented.

The report states that there was no communication with the original authors. Authors forked an open-source implementation of IMPALA (TorchBeast) and implemented the attention-based architecture proposed in the paper. The report includes a link to the code used for training the agent ( https://github.com/cjlovering/interpretable-reinforcement-learning-using-attention ), but I could not find the code used to visualize the attention maps.

The main weakness of the report is the fact that results in the original paper could not be reproduced due to the lack of computational resources. This prevents the authors from properly analyzing the trained agent, which mostly generates static attention maps that do not seem to depend on the current observation (c.f. https://imgur.com/a/SGLPQvQ ). I would suggest to run the same code on a different environment (e.g. Pong or Breakout), as it will be possible to learn a competitive policy much faster than in Seaquest (c.f. Figure 8 in the Appendix of https://arxiv.org/pdf/1906.02500.pdf ). ",19-261.txt
77,https://openreview.net/forum?id=BJg2waqGar,0,5,3,"Dear authors,

Thank you for your efforts to reproduce this paper! I enjoyed reading your report. Below is my review:

---

The authors reproduced the paper ""A Family of Robust Stochastic Operators for Reinforcement Learning"" by Lu, Squillante, and Wu. The original paper proposes Robust Stochastic Operators (RSO) as a new operator for Q-learning, proves that it preserves optimality and increases the action gap, and shows empirical results on classic OpenAI Gym environments. 

The authors of the report repeat the empirical work and compare their results with the results from the original paper. It seems like the final version in the NeurIPS 2019 website differs from the version the authors used for their reproduction, as the has several curves for RSO with different distribution for 
βk
. I assume that the authors reproduced version 2 in the arXiv. From here on I will refer to the arXiv version as the ""original paper.""

The authors tried to reproduce every figure in the original paper. 

1. In the Acrobot environment, the authors reproduced the training graph, although the curves have more variance. I could not understand how they calculated the action gap: more explanation would be welcome.
2. In the Mountain Car environment, the authors reproduced similar training curves but found that RSO and consistent Bellman had very similar performance. To investigate, the authors decrease the number of training episodes and find that RSO outperforms Bellman and consistent Bellman operators when the number of episodes is small. 
3. In the Lunar Lander environment, the authors evaluate 9 agents in contrast to 3 in the original paper. I did not read a clear comparison between the results in the original paper and the reproduced result.

Overall, the authors thoroughly reproduced original work and gave analysis in parts where the reproduced results differed from the original paper. However, there are some unclear parts in the report that could be improved. Furthermore, the authors do not provide the code they developed to reproduce the results.

Here are the major fixes that I recommend:

M1. I could not find the link to your codebase. Please make it available so I can review your code also.
M2. In Section 3.2 Paragraph 1 (""This OpenAI...""), the first citation [1] seems to be wrong: perhaps you wanted to cite ""Efficient Memory-Based Learning for Robot Control"" by Moore, corresponding to [15] in the original paper?
M3. In Section 2 Paragraph 4 (""As we will be...""), you should use ""Bellemare et al."" instead of ""Bellemare"" when citing [2]. 
M4. In Section 3.1 Paragraph 2 (""We attempted...""), please elaborate on how you calculated the action gap and how the replicated action gaps are consistent with the original.
M5. In Section 3.3, please add more analysis for Figure 3.

Here are other minor fixes that I suggest:

m1. The author info of this report seems to be that of the original authors, not yours.
m2. In Section 1 Paragraph 2 (""In 'A Family...""), I recommend writing out RSO before using it the abbreviation.
m3. In Section 3.2 Paragraph 3 (""The authors...""), I recommend adding a table for the numbers, as it is difficult to compare the numbers inside the sentences.
m4. For the tables, it would be easier to read and analyze if the best results are highlighted bold. Please look at Figure 2 (b) of the original paper for an example.

---

I am giving an initial rating of 5: Marginally below acceptance threshold for this report, but with a few modifications, I believe it deserves a higher rating.",19-262.txt
77,https://openreview.net/forum?id=BJg2waqGar,0,7,4,"This replication report successfully reproduces most of the empirical results of the original authors (Yingdong et al.), and shows by extending the original set of experiments that the proposed Robust Stochastic Operators (RSO) are mainly beneficial when there are fewer training episodes. The arguments provided in cases where there were minor discrepancies between the original and replication seem reasonable, for example acknowledging differences in learning rate annealing, or moving average smoothing schemes.

The replication differed with respect to two specific claims made in the original work, of which it appears they contacted the authors regarding one of these issues and received a response.
1. Yingdong et al. claimed that 
ϵ
 in the 
ϵ
-greedy setting did not affect the relative performance ordering of the three operators considered: Bellman, Consistent Bellman, RSO. This work found a non-trivial dependence on 
ϵ
, but after contacting the authors it appears a different softmax method was reported instead.
2. It was also claimed that a higher variance for 
βk
 --- which weights the difference between the value and Q function --- improves final test score performance, yet in the Mountain Car experiment no relationship was found between the variance of 
βk
 and test scores, rather the mean of 
βk
 was more important. 

General comments:

- I suggest you use ""authors"", if at all, to refer to the authors of *this work*, and use Yingdong et al. to refer to the authors of the work being reproduced. This is consistent with guidelines here (Dietterich slide 42 http://web.engr.oregonstate.edu/~tgd/talks/new-in-ml-2019.pdf)

-  This error may have been propagated from the original work, but essentially all of the Tabular results contain spurious precision. Uncertainties should be reported to 1 or 2 significant figures. The mean value then contains no more decimal places than the uncertainty, consider rounding 
129.93±32.68
 to 
129±32
, 
119.43±3.68
 to 
119±4
. Reporting results in this way isn't just a formality, it makes it easier to spot real differences versus noise. 

- This again may have bee propagated from the original, but you should aim to distinguish data series in Figures using lifestyles or markers rather than only by color. Makes work more accessible and legible in case printed in BW.

Comments by section:

Introduction

- careful not to switch between present and future tense, e.g., In section 2, we will ...
- it would be helpful to first cite [5] after providing the title to connect these.
- RSO undefined in the introduction

Section 2 Properties of RSO

- ""even training in the continuous sense is possible with loss of optimality"", I think you mean *without* loss of optimality?
""the papers also states"" grammar
- missing bracket when introducing the value function

Section 3.1 Acrobot

- It would be helpful to describe this task in a bit more detail and clarify the difference between ""experiment"" vs ""epsiode"" for non RL folks, and how the 20 experiments differed from each other if at all. 
- Fig 1, it would be helpful to use the same y-axis scale when comparing your figure versus the original. Also, Figure captions should be standalone, it is currently a fragment.

Section 3.2 Mountain Car

- ""The only hurdle was realizing a softmax distribution over actions"" -> this caught me off guard due to the casual nature of the sentence, try to be more explicit to show that realizing is not meant in a technical sense here, for example ""it was unclear from the original work that a softmax distribution over actions was used to produce these curves""

- An explanation re why Figure 2 b) continues downward after 2000 training steps, whereas Figure 2 a) plateaus would be helpful. 

- ""In line with our prior"" -> again, this is a bit casual, consider revising ""the authors prior belief that""

Section 3.3 Lunar Lander 

- Only the agents w/ learning rate 0.1 ""did well"" <- try to be more specific, e.g., achieved a low enough score, converged quickly enough, ...

- Figure 3, the legend overlaps with some of the data series, it should be moved outside the plot

- Is there a difference between ""Lunar Lander"" and ""LunarLander-v2""? Could this possibly account for the discrepancy in reward distribution observed in Figure 4? 

Conclusion 

""although it does not seem to completely dominate the other two agents"" This isn't competitive wrestling :) I doubt Yingdong et al suggested this, consider rephrasing, ""although RSO exceeded the benchmarks in the low training epsiode regime, its benefit was less universal than Yingdong et al suggest. ",19-263.txt
77,https://openreview.net/forum?id=BJg2waqGar,0,7,4,"First of all, I am confused about the entry here on openreview. It states the wrong authors. It lists the original authors and original abstract. This review is for:
""Re: A Family of Robust Stochastic Operators for Reinforcement Learning""
by Ajay Balaji, Rigel Galgana, Isaiah Liu, Andrew Park, Benjamin Vu, James Wang (as in the pdf)

The replication report correctly states the main message of the original paper and provides a summary of the original findings.
The replication effort was to replicate the results from scratch (Track 3). The results were reproducible in general, but some discrepancies and missing information were found.
Some of which was communicated to the original authors and got confirmed (softmax action selection instead of eps-greedy).
The replication also did some additional hyperparameter analysis (epsilon and beta distr. mean and variance).

The experiments are well motivated and discussed. The presentation is good (better than in the original paper, but this is not a compliment in itself).
Thanks for reproducing the original paper and shedding more light on the method.

I do not see any link to code in for the replication. This needs to be added.

Details:
page 1, par 2: Add the citation after the title of the paper.
Text before Table 1: ""fact that our replication performed slightly better overall"": where is that supported? Fig 1 shows the opposite.
Table 1: where do the original values come from, I cannot find it in the original paper (including appendix), the same holds for Fig 4 (a). Is there a different version of the paper?

Table 2: how can the test scores be so much lower (meaning better here) than the training scores, it seems there is a difference in evaluation here right?

Text below Table 5: ""the performance seems to decrease as well"": do you mean score decreases and performance increases? At least the following sentence suggests this. However, the table does not contain any line with mean > 1.",19-264.txt
78,https://openreview.net/forum?id=SygawpcMaS,0,5,4,"The authors selected the ""No Press Diplomacy: Modeling Multi-Agent Gameplay "" for their reproducibility challenge. In a nutshell, the selected paper is a computationally demanding one, which did not allow for extensive experimentation across the different models introduced in the original paper.

The reproducibility paper has been well written and clear and describes concisely the step undertaken to reproduce the results. As acknowledged by the authors, selecting this specific paper might have been a poor choice as the authors of the original paper have decided to publish their code as well. This meant that the paper did not include every single detail of the hyperparametrisation and other bits and bobs therefore making it harder to reproduce from the paper alone. The reason for this is that the authors of this challenge did not have access to the code.

However they contacted the authors of the original paper for clarifications, which helped to some extent but not to a level that all aspects could be reproduced.

Overall it has been a decent effort but the computational demand has been a barrier given the nature of the game.",19-265.txt
78,https://openreview.net/forum?id=SygawpcMaS,0,6,4,"The paper focuses on the No Press version of a strategic board game called the Diplomacy and aims to replicate the following 2019 NeurIPS paper: Philip Paquette, Yuchen Lu, Steven Bocco, Max O. Smith, Satya Ortiz-Gagné, Jonathan K. Kummerfeld, Satinder Singh, Joelle Pineau, Aaron Courville. No Press Diplomacy: Modeling Multi-Agent Gameplay. 

I provides immense value to the community by explaining how to resolve replication issues by good communication with the original authors; by resolving problems together with the  original authors, the authors of the submitted manuscript were successful in working through the many problems and constructively criticise the original paper. The lessons learnt were clearly classified as implementation ""successes"", ambiguities, and difficulties. The authors' merits also included releasing their code on GitHub and documenting their results and providing actionable advice on what to keep and what to improve when writing experimental machine learning papers in the future. Finally, they reflected in a relatable way on how to (better) choose a paper for a replication task.

However, the paper's structure and writing could be clarified. For example, its tense made it often hard for me to follow the narrative. Please use past tense consistently to describe what the original authors did and what your materials and methods were. Most importantly, I felt uncertain if the results were as they were expected because they were somewhat scattered between Sections 2-4; it should be the authors and not the readers responsibility to gather the evidence together (as a proper plot/table in Section 4 with body text to explain to the reader the main aspects of this numerical outcome). Finally, often the paper felt more like a lab notebook with listings of what was and what should be done rather than a carefully written and comprehensive (top-tier NeurIPS)  paper with clear structure and storyline to tell.

Regardless of leaving room for improvement in its writing, I wish to congratulate the authors of their persistent, good work. I believe the manuscript is a valuable contribution to the community, and should be published after major revisions. That is, I do not have problems with the core content of the paper but rather its writing.",19-266.txt
78,https://openreview.net/forum?id=SygawpcMaS,0,4,4,"Despite a significant effort from the authors to discuss with the original authors, this submission suffers from a major limitation: the computing resources used are far lower than the one used in the original paper.  Thus, this limits the authors to train their model on only 1.5% of the data. Since they obtained a lower performance, it is not possible to draw conclusions from whether this comes from a lack of reproducibility of the original paper or from the computing limitations. 

As a side note, the authors wonder in the Discussions section whether the choice of the paper was right since a number of details of the original work are only accessible in the code, which they do not want to access because they chose the Replicability track.  The reviewer is left a bit puzzled by this comment. 

On the other hand, the authors spent a great deal of effort understanding the implementation details of the original paper as well as the computing needs.  Therefore, this paper carries information and would be worth publishing if they had access to the right computing resources and if the authors did the appropriate hyperparameter search study. 

As a conclusion, despite the good will exposed by the authors and the deep technical work they did to understand the original work (in collaboration with the authors), this paper brings more questions than answers on the original work and may induce unfair negative perspectives on it. 

 

Problem statement: good. 

Code: good.  The authors provided their full code.  The code is reasonably commented. 

Communication with original authors: good 

Hyperparameter search: weak.  No hyperparameter search is described in the paper. 

Ablation study: weak.  No ablation study is described in the paper. 

Discussion on results: weak. The Results section is limited in size and scope. The discussion does not cover results but rather the original paper. 

Recommendations for reproducibility: good. The paper provides a number of insights in terms of how to reproduce the original paper, both in terms of computing needs and in terms of implementation details. 

Overall organization and clarity: good ",19-267.txt
79,https://openreview.net/forum?id=Bkee_pcM6H,0,5,4,"Overall, the authors demonstrate a clear understanding of the paper and provide useful insights into the methodology of the authors.  The top areas of improvement are the inclusion of code with documentation for the experiments and analysis of the variability in performance across runs, ideally at the matching number of runs (50 instead of 20). 

Problem statement 

The authors seek to replicate the experiments on the stochastic operators introduced by Lu et al. on the Mountain Car task.  The summary of the paper provided by the authors suggest that they understand the problem statement of the original paper.

Code

The camera ready for Lu et al. does not provide code; neither do the authors provide.  Only a partial snippet is shared in the preprint of Lu et al.  Inclusion of the code to replicate the experiments provided is necessary to fully assess the work.  The authors mention that they use the same software library as Lu et al., basic_reinforcement_learning, but neither Lu et al. nor the authors describe how the library was used in detail. Without the specific scripts to show how the library is used, much of the context is missing and the claim that the authors “strictly followed [Lu et al]’s methodology” cannot be independently confirmed.  Ideally, code and documentation would be included.
 
Communication with original authors

No communication with Lu et al. is described either in the report or on OpenReview.

Hyperparameter Search and Ablation

The authors provide an independent exploration of hyperparameter values.  In particular, they perform an additional exploration of epsilon decay, which is not described in Lu et al. Furthermore, they perform a study of the effect of parameter values of epsilon, which is discussed very briefly by Lu et al., but no results are provided to support their claim.

For each hyperparameter setting, the authors run their experiment 20 times and report the average score for 1000 episodes following training.  While this information is promising, in replicating the results presented by Lu et al., the authors do not provide an estimate of the variability of these 20 trials such as standard deviation or range.  Furthermore Lu et al. mention the use of 50 trials each, not 20.  While this difference may not lead to statistically significant differences in result, it still is a difference in the scale of the number of trials.  A similar comparison of 50 trials to 50 trials could also help improve measurements of the variability of performance across trials.  Lu et al. do not do this, but providing these variability estimates to the community would be of interest.  

Furthermore, the authors do not fully explain why the parameter values for each parameter were included in testing.  For example, the authors do not make clear why only a single value for alpha and gamma were tested.  They also do not explain why the three values of epsilon and two values of epsilon decay were chosen.  Additional explanation would be appreciated.

Finally, Lu et al. describe 9 different methods of selecting Beta_k in their camera ready.  The authors only use two methods of the nine methods: setting Beta_k to 1 and sampling Beta_k Uniform(0,2). The authors say that these two methods are selected to demonstrate they can replicate the result that Beta_k Uniform[0,2) is optimal.  However, this comparison of a single deterministic value to a single distribution does not fully replicate the result provided by Lu et al. and show that Uniform[0,2) is optimal.  More comparisons of instances of Beta_k would need to be shown to demonstrate this result is replicable. It is important to note that the pre-print version does not include all methods of selecting Beta_k and this discrepancy in the similarity of the results may be because the authors based their replication on the preprint. Given the presence of the camera ready at the time of the submission deadline for this replication challenge, additional effort to match the experiments in the camera ready would be appreciated. 


Discussion on results

The authors provide a discussion of the level of reproducibility of Lu et al.  They find that epsilon decay has a notable effect on the results, leading to different conclusions about the effectiveness of RSO. However, their results are limited to both the domain and parameter settings tested.  Moreover, the claims by the authors are less strong than they initially appear to be due to the lack of reporting of variability metrics such as range or standard deviation. Claims such as “significant margin” of improvement in using RSO, and that their deterministic setting of Beta_k “substantially outperforms” Uniform[0,2) often imply the presence of these metrics.  

The authors suggest that they have replicated graphs from the original paper.  However, they only refer to the preprint version of the paper and not the camera ready.  The authors do not make clear that their replication is based off the pre-print.  Due to the existence of the camera ready, it would help the reader if the authors could distinguish between discussion of methodology and results presented in the preprint and the camera ready and to take additional steps to replicate the results presented in the camera ready, especially with regard to RSO instances. 

Recommendations for reproducibility

The authors explore the use of epsilon decay by actively testing the effect of epsilon decay on results. They also recommend the inclusion of results showing the effect of different values of epsilon in the appendix. 

Overall organization and clarity

Organization and clarity are generally good.  Stylistically, more effort could be used to improve the quality of the plots.  For example the plot used in the preprint of Lu et al. and included in the paper is of poor image quality.  Axes for the plots would make them easier to read.  Matching scales in the plots, specifically the plot from the Lu et al. preprint and the authors’ replication of the plot would be easier to compare if they had the same scale.",19-268.txt
79,https://openreview.net/forum?id=Bkee_pcM6H,0,4,4,"+ Summary:
The authors replicate some of the experiments performed in [1], in particular those using the Mountain Car environment from OpenIA Gym ([1], section 4.2). The original paper, proposes a family of (robust) stochastic operators (RSO) as an alternative to the Bellman operator, when there are approximation errors in a Markov Decision Process (MDP), in particular when a continuous-state, continuous-time MDP is modeled with discrete states and time. 
These operators follow the idea presented in [2] , where the so-called ""action gap"" (difference between the Q-value of the optional action and the second-best action) is maximized to reduce the effects of the approximation errors. 
Experiments in [1] (and this replication study) compare the RSO family against the traditional Bellman operator and the consistent Bellman operator, introduced in [3].

+ Problem statement:
This replication study clearly states its goal (replicate the experiments in [1] performed in the Mountain Car environment of OpenIA Gym), and briefly explains and summarizes the ideas presented in the original paper. 
[Score: 1.0/1.0]

+ Code:
Both [1] and this replication study use the open-sourced code from [4]. The authors from [1] did not provide their modified version of the code, as far as this reviewer can tell, but provided only pseudo-code to implement the Bellman, consistent Bellman and Robust Stochastic operators. Since there's no mention that the authors of this study have communicated with those of [1], it is fair to assume that the authors of this replication study had to implement the pseudo-code by themselves.
The authors of the replication study should publish their modified version of the source code in [4]. This is very important since the absolute results from this replication study highly differ from the original paper, and it's close to impossible to tell where the differences are. 
[Score: 0.2/1.0]

+ Communication with original authors:
As far as this reviewer can tell, there has not been any communication by the authors of this study and the original authors of [1]. There are no messages in OpenReview (as of Jan. 11, 2020) between them, and there are no mentions in the report of any private communication. 
Moreover, section 3.2 of the report is fully dedicated to clarify whether the original authors used epsilon decay in the epsilon-greedy algorithm or not. 
In addition, the authors of the study suggest that the differences in the results could be explained by different choices of epsilon, yet the authors of [1] clearly state that they used the ""default"" value of epsilon (which as of Jan. 11, 2020 is 0.1). These two issues could have been quickly solved by contacting the authors of [1].
[Score: 0.0/1.0]

+ Hyperparameter Search:
The authors of the study perform diligent hyperparameter search over the value of epsilon and epsilon decay. The latter was not used in the original paper, but this study shows in section 3.2 that epsilon decay can greatly improve the results of the traditional Bellman and the consistent Bellman operators, which reduces the gap in performance between these two and the proposed RSO. The original authors of [1] should have done the same, and this reviewer appreciates the effort of the authors of the study. However, given that they only sweep over 6 different configurations, this reviewer would suggest to increase the upper range of epsilon to 0.5, since they find 0.3 to be the best for RSO, which is the upper limit of epsilon in their experiments. This would only increase the sweep to 8 different configurations.
The authors of the study also explore with beta_k = 1.0, which they find to perform better than the stochastic version, using beta_k ~ U[0,2), which is the value that the authors of [1] recommend.
One negative point is that, contrary to [1], the authors of the study do not report any confidence intervals, and perform 20 trials for each hyperparameter configuration, instead of the 50 performed in [1]. Given that the variance across implementations of the same operator (between [1] and this study) is much larger than the variance between different operators, this variance study is mandatory.
[Score: 0.5/1.0]

+ Discussion on results:
The report contains a discussion of the replicability results in section 5. They discuss how the differences in the results could be explained. Despite the absolute differences in the results, the conclusions presented in [1] still hold, in the sense that RSO is shown to improve over the traditional Bellman and consistent Bellman operators. The only claim from [1] that the study finds to be wrong is the fact that using beta_k ~ U[0,2) is optimal for ""Mountain Car"".

This reviewer also wants to point out that the ""Authors' score"" column in Table 4 does not match the results of the proceedings version of [1]. For instance, the authors of [1] report a score of 131.2 +/- 0.23% for the Bellman operator, while Table 4 states 129.93. Perhaps, this has changed since the preprint version that the authors of the study has used, but this reviewer has no way of checking that, since I only have access to the versions of [1] from:
- /pdf?id=B1M-ASSgUS
- https://papers.nips.cc/paper/9696-a-family-of-robust-stochastic-operators-for-reinforcement-learning.pdf
[Score: 0.4/1.0]

+ Recommendations for reproducibility:
The authors of the study suggest [1] to make explicit whether or not they used epsilon decay, and the actual value of epsilon. It is worth mentioning that, as said before, the authors of [1] do report that they use the ""default"" value. Although this value could have changed, since the code in [4] is not necessarily static, notice that [4] has not been modified since July 2018. Thus, it's very likely that indeed the authors of [1] used epsilon = 0.1, which bears the question again on why the results of [1] could not been reproduced. 
[Score: 0.5/1.0]

+ Overall organization and clarity:
The paper is clearly written and well structured. There are only a few minor comments:
- Equations 1, 2 and 3 are missing the expectation over the next state in the operators. If the authors want to express that the next state is sampled following the distribution P, they should state it explicitly.
- Also, this reviewer would suggest to use x and x' to denote the current and next states, as the authors of [1] do.
- Add x/y labels in plots of Figure 3.
- Please, fix reference to consistent Bellman operator in page 2. It was presented in [3], not in [2].
- Make bibliography style consistent (e.g. use either ""Name Lastname"" or ""N. Lastname"" consistently).
[Score: 0.8/1.0]

+ Final comments:
I am willing to increase my score if the authors of the study address my concerns adequately.

[1] ""A Family of Robust Stochastic Operators for Reinforcement Learning"". Y. Lu, M. Squillante, C. Wu. NeurIPS 2019.
[2] ""Action-gap Phenomenon in Reinforcement Learning"". A. Farahmand. NeurIPS 2011.
[3] ""Increasing the Action gap: New Operators for Reinforcement Learning"". M. G. Bellemare, H. Ostrovski, A. Guez, P.S. Thomas, and R. Munos. AAAI 2016.
[4] ""Basic Reinforcement Learning"". V. Mayoral Vilches. https://github.com/vmayoral/basic_reinforcement_learning",19-269.txt
79,https://openreview.net/forum?id=Bkee_pcM6H,0,5,4,"This is a replication of the paper « A Family of Robust Stochastic Operators for Reinforcement Learning », that proposes to replace the classical Bellman operator with a stochastic one chosen from a family of action-gap increasing operators (shown to preserve optimality). This report focuses on the Mountain Car experiments, attempting to replicate results showing the superiority of the proposed operators over both the classical Bellman one, and a previously proposed « consistent Bellman operator ». Overall it is found that these results « were largely replicable », though validation scores were higher (across all methods) compared to the original papers, and the replication results contradicted the authors’ claim regarding the optimal parameterization of the proposed stochastic operator (RSO).

This is definitely an interesting study, highlighting the challenge of reproducing existing results even when it seems relatively trivial (clone [4] then replace the Bellman operator following the 3-line pseudo-code at the end of the original paper). After reading it, I actually think the conclusion saying that results « were largely replicable » does not really reflect the content of the report: although there are indeed similar trends overall, in my opinion some numbers are different enough to worry about whether this is a faithful replication (see e.g. the difference in axis range in FIg. 1, the differences in scores in Table 4, and the fact that the deterministic version beta=1 performed consistently better than the stochastic beta ~ U(0, 2)). I would encourage the report’s authors to rewrite the conclusion to emphasize first these significant differences, before acknowledging that at least these results support the conclusion that RSO operators are superior to the two baselines.

Given that some details of the original experiment settings where unclear, especially regarding exploration, I believe it would have been a good idea to try and contact the paper’s authors, which does not seem to be the case here. Since there was another replication attempt on the same paper, I took the liberty to look at it as well, and noticed that:
1. They contacted the authors regarding exploration, and it turns out that Boltzmann exploration was actually used instead of epsilon-greedy (which could be one important reason why this report’s results are significantly different). It would thus be important to re-run the experiments with Boltzmann exploration.
2. Surprisingly, the replicated results from the other report (Fig. 5b) show a very different behavior without epsilon decay compared to this report (Fig. 3b). One of them has to be wrong somehow, so I encourage this report’s authors to get in touch with the other group and figure out what is going on.

One important limitation of this replication attempt is that it focuses only on the Mountain Car experiments, while the original paper reported results on 3 other environments. Given how RL algorithms are known to exhibit highly performance behavior across tasks, and Mountain Car also being a somewhat « special » case (see e.g. https://www.reddit.com/r/reinforcementlearning/comments/emb02o/new_to_rl_and_looking_for_help_to_solve_mountain/), I think it would have been worth trying to replicate results on the other environments as well (note that the other replication report found additional discrepancies in these other environments).

It also seems like the report was based on the arxiv version of the paper, while the camera-ready NeurIPS version has some differences (I have not tried to compare them in details, but some numbers have changed, and the assertive superiority of stochastic RSO mentioned in section 4.5 of the arxiv paper seems to be gone). Ideally this report should be based on the final version, but if not possible at least please mention the meaningful differences in the report, to avoid confusion.

Other remarks:
- There is no link to the codebase used for the replication, hopefully this is just an oversight
- Please specify (when referencing Fig. 1) that lower score is better (it is a bit confusing if you have not read the original paper first)
- Fig. 3a has negative axis labels
- The numbers of runs performed to obtain the various graphs should be mentioned
- Please add some measure of uncertainty to Tables 2 and 4
- Table 2 is particularly interesting since it is the closest to the « true » performance of these algorithms in practice. However the methodology (as I understand it) is a bit biased since the performance associated to the best hyper-parameters seems to be reported directly. Ideally, it would be better to re-run each algorithm with its best hyper-parameters configuration on a different set of seeds, to get rid of this potential bias (it is hard to say how important this is without a confidence interval)",19-270.txt
80,https://openreview.net/forum?id=r1x-_acfaH,0,6,3,"Good reproducibility effort, with proper published code. However, paper has conflicting reviews and overall reviews are not quite good enough for the AC to recommend to the journal.",19-271.txt
80,https://openreview.net/forum?id=r1x-_acfaH,0,4,3,"- Summary: The report reproduces 1 baseline and the proposed method of the original paper on 1 of the 4 datasets used in the paper. The report also expands the ablation study of the original paper (which is only on 1 of the 4 datasets in the paper) to another dataset. 

- Problem statement: The report clearly describes the problem statement of the original paper. 

- Code: 
    -- The released code from the original paper has been used. It seems like the only addition over the original code is the Jupyter notebook. Since everything is in the same place (including installation, generating data, training, etc.), the notebook is not very easily readable. It would have been better if either the codes for generating data, training, etc. were separate or if non-informative output (e.g., output of generating training instances) was minimized to make the notebook more readable.
    -- It is also unclear whether/what part of code is novel. 

- Communication with original authors: The report mentions good communication with the original authors. The communication is not through OpenReview. 

- Hyperparameter Search: The report uses the same hyperparameter settings as the original code. The report mentions that the reason is excessively high running time. 

- Ablation Study: The report expands the ablation study of the original paper (which is only on 1 of the 4 datasets in the paper) to another dataset. Although useful, I think the novelty is on the low side. 

- Results: The report reconfirms the conclusions from the original paper. However, I have the following concerns with the results in the report:
    -- The numbers reported as “Original” in Table 1 are not the same as the original paper.
    -- In Table 2, although the order between the baseline (RPB) and the proposed method (GCNN) stays the same as the original paper, there are some significant differences between the original and reproduced numbers: the number of wins for GCNN for the “Easy” case is substantially higher in the reproduced results, the numbers of nodes for both RPB and GCNN for “Hard” case are substantially higher in the reproduced results. There doesn’t seem to be any clear explanation for these discrepancies. On a smaller note, the reported nodes for “Medium” case for RPB and GCNN have been mistakenly exchanged.
    -- In Table 4 (Ablation Study), the results are not reported for the “Hard” case which, according to the paper, is the only case where the proposed method clearly outperforms the ablations. The Jupyter notebook seems to contain partial results for the “Hard” case. It’s unclear why that’s the case.
    -- Section 5 also mentions that some problems were encountered in the replication. But these problems are not expanded on. It would be good to include these problems in detail.   

- Recommendations for reproducibility: The authors provide two suggestions to improve reproducibility:
    -- Providing the actual datasets used for training and evaluation along with the code to generate them. I think this is very important because it seems that the exact results are not reproducible using the dataset generated by the provided generation scripts because of randomness. It is also unclear how the randomness comes into play if using the same installation instructions and random seeds as the original paper.
    -- Keeping the codebase up-to-date.  

- Overall organization and clarity: The report is clear and easy to read.",19-272.txt
80,https://openreview.net/forum?id=r1x-_acfaH,0,6,3,"This paper replicates the results of a subset of baselines reported in the original paper, as well as one of the original ablation studies. Overall, it is encouraging to see that the authors were able to replicate the original results they tried to replicate and reporting it has value by itself. It is unfortunate that only one baseline was replicated though, not allowing the reader to draw stronger conclusions from this report. To me, more baselines would drastically improve the value of this paper. Moreover, the write-up can be improved; it is sometimes repetitive, it makes claims stronger than the data it presents to support them, and it fails at providing better insights into the implemented algorithm. The bullet points below should elaborate on these topics.

- With respect to making strong claims, the paper starts stating “We conclude that the paper is reproducible.” However, only a small set of methods and experiments were replicated. I find the quoted sentence misleading and it is a much stronger claim than the results presented allows to conclude. I’d encourage the authors to tone down their claim.

- In terms of clarity, I think it should be made clearer (early in the paper) how much the original code provided by the authors influenced the development of this paper. Clearly there is a new repository with the code used, but the discussion will be much clearer if the authors explicitly discuss the impact the original code had in theirs.

- I don't understand the necessity of a Related Work section, which is probably 10% of the paper. It mostly restates what the original paper discusses and it doesn't add much to the reproducibility discussion per se. This is a different type of paper  and I think it requires a different presentation style as well.

- I missed a more clear discussion about how much was reproduced, what are the claims that were validated and the ones that were not. More importantly, I missed a more explicit discussion about the lessons learned. What were the insights gained by the authors when replicating those results? What was not stated in the original paper? Some of this is discussed in the paper but based on my personal experience, generally there's much more to be said than what is in the paper at this moment. To make it concrete, I really like the discussion about running time and hardware, which is something often absent in the original papers. Something else that could have been further highlighted as it is often not considered is the fact that libraries change and this impairs one's ability to reproduce results. It is not realistic to expect authors to constantly maintain every code base they have, keeping them up to date, but it might be interesting to make everything self-contained. 

- The results in Table 1 are surprisingly close! For Table 3 it might be possible to achieve similar results if you also report the ratio between both methods for [RE] and [OR]. This would show relative speed and it wouldn't be so impacted by the hardware being used.

- I would report, for all tables, the values reported in [OR].

- It is said that not every result in the paper was reproduced due to ""lack of time"". Honestly, I don't find ""lack of time"" a very compelling explanation. Moreover, it is said that some experiments have a prohibitive cost but the most expensive ones run for 24 hours. How prohibitive is that? Often ML papers have experiments that take much longer to run (e.g., RL papers that report Atari results need a computer per game for almost a week). Am I missing something or was 24 hours the prohibitive cost?

- Typos:
 * ""decisions and extract(S)""
 * ""The authors found that the the"".
 * ""small instances can generate (generalize?)""
 * ""auction benchmark and our results ?""",19-273.txt
80,https://openreview.net/forum?id=r1x-_acfaH,0,8,4,"tl;dr: Great work. I will increase my score if there is a tiny bit more of hyperparameter search OR ablations OR plots.

This is a submission for the Baselines track.

I will be reviewing using these guidelines:
ttps://docs.google.com/document/d/1Fj4E1IHFBGdXp19A93_sI06SlpPEAw4YMWcFDMAtb5U/preview

Problem statement:
The problem statement is clear and found on lines 1-3 of the report.

Code:
The link is shown in the abstract: https://github.com/audreyanneguindon/NeurIPS_2019
The authors re-use some code, for example ""01_generate_instances.py"" to generate instances appears to be the same as the original author's one.

Communication with original authors
The authors communicated with the original authors, it is mentioned in section 3.4 "" The authors shared with us that [...]"" and section 6 ""the authors were willing to answer our questions and guide us""

Hyperparameter Search
I agree with the authors that in this setting the computational cost of policy evaluation is high, discouraging hyperparameter search, section 3.2 ""Hyperparameter search can be quite difficult for these benchmark problems due to the prohibitive time it takes to solve MILP problems"".
As an alternative, the authors could try doing hyperparameter search (and ablations) for the accuracy metric.

Ablation Study
The authors repeat the ablations done by the original authors.
I encourage the authors to try other architecture variants and benchmark them with the accuracy metric.
For example, trying the GCNN with some features remove/masked out, or removing the edge information, these would allow us to know if the features or edge information are possibly important (if accuracy gets reduced by a lot) or possibly not important (if accuracy does not get reduced)

Discussion on results
The report includes a discussion of results in section 5.

Recommendations for reproducibility
The report includes a recommendations for reproducibility in section 5.

Overall organization and clarity
The report is readable and the organization is good.
Adding some plots would be nice (as addition to tables but not instead of tables), for example in table 3 the solving time seems correlated with instance size, so comparing the ratios of medium_time/easy_time (or similar) for the original and the replication seems like a good idea.



Typos and other recoverable errors (I didn't take these into account for the rating):

The data in Table 5 is incorrectly labeled, as currently it is the accuracy@1 @5 @10, the corresponding code is 
https://github.com/audreyanneguindon/NeurIPS_2019/blob/master/final_notebook.ipynb see ""In [83]""
acc = float(results[results.policy_ == policy][f'acc@{i}_mean'])

The ""reported wins"" column in Table 2 does not sum to 100/100 in the easy setting as LMART has 75/100 and is not included in the replication (source Table 2 of https://arxiv.org/pdf/1906.01629.pdf)",19-274.txt
81,https://openreview.net/forum?id=H1gU_Tcz6r,0,5,3,"Good reproducibility effort, adds to the understanding of the paper. However, overall reviews are not quite good enough for the AC to recommend to the journal.",19-275.txt
81,https://openreview.net/forum?id=H1gU_Tcz6r,0,7,4,"## Summary

* The report attempts to replicate the results in “Experience Replay for Continual Learning”.
* The focus is on implementing the CLEAR method for experience replay.
* No code provided by the authors of the paper.
* Oddly, the authors of the report have decided to provide the code only on request. I hope that they open-source the code.

## Significance

* Kudos to the authors (of the report) on enlisting the possible installation issues. This often becomes a major blocker.
* The report describes the various design choices that the authors had to make when replicating the original paper. This will be useful for anyone wishing to work on this problem.

## Scope for improvement

* I am not convinced that the report replicates the results from the paper. It is not fault of the authors - the experiments require a large amount of compute. And reading the report does indicate that the authors' implementation is correct.
* It would be helpful if the authors had use good resolution/scale images when reporting results. For example, figure 9 (left) is barely readable.
* Many of the issues/errors are pasted as images. While it makes it easier for the reader, it would be even better if the authors linked to the issues/gists(with the error messages).


6 our investigation, we found that the authors did not release a codebase of their
7 contributions, and only the baseline, IMPALA [Espeholt et al., 2018], was available.
8 We also found that the CLEAR method required many modifications to the baseline
9 code with vague details not well described in the paper. To this end, we include our
10 solutions to fixing the IMPALA codebase and how we adapted it for the CLEAR
11 method. Lastly, we describe our attempts to implement the CLEAR method with
12 and without behavioral cloning (Figure 2 in the original paper) and report our
13 replicated graph of IMPALA for sequential task learning.",19-276.txt
81,https://openreview.net/forum?id=H1gU_Tcz6r,0,5,4,"The report provides a detailed analysis of the attempt made to replicate the work ""Experience Replay for Continual Learning"". The attempt covers all the aspects, from the installation of the codebase to the obtained results from the paper. I found the report a bit weak because most of it covers the set up of the codebase and little space is left to the actual method and its main contributions (e.g. continual learning experiments, behavioral cloning). 

Positive aspects:
- The problem statement is very well-written, with a clear description of the problem and approach here analyzed.
- Detailed effort to reproduce the codebase

Negative aspects:
- The report does not provide too many insights on the reproducibility of the continual learning experiments (or even baselines), which was the main contribution of the analyzed paper. 
- While the authors describe the difficulties encountered in running experiments for the continual learning scenarios, no communication is reported with the original authors on this point. 
- In my opinion, too much space is dedicated to the set up of the docker which I found a bit out of scope. Also, the various errors encountered in setting up the memory replay buffer could have occupied less space, since it is something related to the codebase and not to the contribution of the paper only. 

Overall, I like the fact the author detailed the errors encountered in every single step, but I found the details too much focused on the docker and the codebase and not to the actual target method. Indeed, since the focus of the original paper was sequential multi-task learning, not having any insights on those results but only a partial replication of a baseline makes the report lacking important analysis on the actual reproducibility of the approach. Moreover, no discussion with the original authors has been reported, thus this weakens the justification for the lack of experiments on sequential training scenarios. ",19-277.txt
81,https://openreview.net/forum?id=H1gU_Tcz6r,0,5,2,"The paper appears some advantages:

1. It points out that the published code of the being produced paper does not contain the core part of its algorithm, and the implementation details described in the original paper is not clear. These declarations show that authors have studied the original paper and its corresponding code seriously.

2. The process of reproducinng indicates the clear thoughts of authors. Specifically, they try to reproduce the baseline and then the core part of the algorithm. The reproducing of the core part is also seperated into three parts, which is reasonable.

3. The authors give many attempts to reproduce the core part of the algorithm, though all fails.

However, the paper still suffer from some defects: As I understand, the reproducibility challenge aims to check the effectiveness of a proposed algorithm, which means the algorithm described in the paper should be implemented at least. However, the authors fail to implement the algorithm described in the original paper, let alone to assess its effectiveness.

I still have a question: It seems that the dilemma during reproducing is in the implementation of ""buffer"", if it is possible to simulate the buffers with simple data structures in Python or Tensorflow, giving up implement it by using C++.

Totally speaking, the authors give many attempts to reproduce the paper but fail. It seems that the failure is partly due to the incomplete official code, and the authors of this paper fail to complete it. I feel sorry but I think the paper is below acceptancd threshold.",19-278.txt
82,https://openreview.net/forum?id=r1xJPpqzTr,0,7,3,"The paper is well written and organized. The discussed demands on the original paper well chosen.
My impression is, that these considerations and comments would have improved the original paper quite substantially.

The only suggestions I can make are some missing ""."".

At Eq. (7)
At preposition 1
Before section 2 ""motivation to use 
\hat{\gamma}}
\hat{\gamma}}
Before subsection 2.2 ""authors: 
λ1=0
In subsubsection 2.2.3 ""derive our own implementation""",19-279.txt
82,https://openreview.net/forum?id=r1xJPpqzTr,0,8,2,"Summary
The paper aims at replicating central empirical outcomes of the NeurIPS 2019 paper ""Generalized Off-Policy Actor-Critic"". The authors focus on the published results for MuJoCo Hopper v2 Robot Simulation, for which they re-implemented the propose d algorithm, Geoff-PAC, as well as the used baselines, Off-PAC and ACE. The authors are able to mostly replicate the results, but (1) show a smaller gap in empirical performance between Geoff-PAC and Off-PAC as well as (2) give reason for the need of a deeper discussion with respect to parameter 
gamma^
, balancing the off-policy gradient and the accurate objective function, originally coined counterfactualness.

Pros
* Thorough, good motivation of the NeurIPS paper and the rigor for further investigation
* Good evaluation setup, showing results for more varying values of 
gamma^
Cons
* Minor, but missing results for the two-circle MDP, where 
gamma^=0.9
 as successfully evaluated

The quality and clarity of the replication is paper is good, as the original paper with its central contributions are thoroughly introduced. The paper also suggests a meaningful objective in its replication endeavor, namely further varying 
γ^
 for MuJoCo Hopper v2 Robot Simulation, which was not done in the original paper. The authors give sufficient detail to how they approached re-implementing Geoff-PAC as well as the Off-PAC and ACE baselines. Finally, the paper starts an interesting discussion based on the found subpar performance of Geoff-PAC when $\hat{\gamma}$ is large, which goes against the intuition of the original paper.",19-280.txt
82,https://openreview.net/forum?id=r1xJPpqzTr,0,5,3,"# Overall
Reimplementation of the Hopper experiment seems well done, and it is interesting that performance of OffPAC improved, though it would have been better to see results on additional envs. New code should have been linked to in the report, and care should be taken to ensure the correct algorithm are implemented (ACE). The discussion of what was hard to reproduce seems useful. The theoretical discussion, however, seems a bit misguided to me, details below. The demonstration of the performance of 
γ^=0.99
, whether or not based on the flawed theoretical discussion, is still useful to see.

# Problem statement - whether the report clearly states and understands the problem statement of the original paper.

The reproducers reimplemented the proposed algorithm Geoff-PAC, as well as the two baselines Off-PAC and ACE
(though apparently the wrong ACE, according to an OpenReview comment from the paper authors).
The reproducers then reproduced the Hopper experiment, finding that their
version of OffPAC did somewhat better than the version reported in the paper,
though the difference was not enough to invalidate the paper's claims.
Finally, the reproducers experimented with a value 
γ^=0.99
 that was not tried in the paper.

The reproducers spend a lot of space talking about this issue of the chosen values of 
γ^
 not being motivated by the paper, but I don't agree with this. The paper simply presents a continuum of objective functions, with the excursion objective on one end (
γ^=0
) and the alternative life objective on the other (
γ^=1
). And in fact I think the paper actually avoids providing any theoretical motivation for any particular choice for 
γ^
; they mention 
γ^=1
, but I think it's clear that the authors understand that this is a poor choice, and that presumably nearby choices (e.g. 
γ^=1−ϵ
, for small 
ϵ
) will suffer nearly equally from the same problems that plague 
γ^=1
.

So basically, I think that what the paper is saying is:

""here is a parameterized continuum of objective functions, with well-known objectives on either end; let's test empirically whether using some of the in-between objectives gives better performance""

whereas what the reproducers are taking the paper to say is

""the alternative life objective is what we should be using, but we can't actually use it, so we came up with a way of getting a set of objective functions that are asymptotically close to it"".

This explains why the reproducers are (incorrectly, imo) surprised that the paper is choosing values of 
γ^
 far from 1.

With that said, I think the main paper could have been clearer about this,
(the paper talks about the counterfactual obj. being designed to *approximate*
the alternative life obj., which seems like a big source of the confusion)
and would have done well to include a theoretical discussion about the effects
of different values of 
γ^
, especially the downsides of choosing
γ^
 close to 1.


# Code: whether reproduced from scratch or re-used author repository.

This was a replication track submission; the reproducers re-implemented the algorithms
but made use of the authors' experimental framework. The reproducers do not appear
to provide the code that they used for reproduction.


# Communication with original authors

The reproducers mention communicating with the authors about data used to
produce the graphs in the paper, which was provided by the authors but
ultimately not used by the reproducers. They appear not to have communicated
early on with the authors about the theoretical issues mentioned previously,
which may have helped clear some things up early on. The paper authors have a
small rebuttal in the comments.


# Hyperparameter Search

The reproducers appear not to have performed a hyperparameter sweep in their
reimplementation, they mention using the values used in the paper.
In Section 2.2.2 they do try out an additional value 
γ^=0.99
.


# Ablation Study
No ablation studies were performed.


# Discussion on results
Whether the report contains detailed discussion on the state of reproducibility of the original paper

The reproducers note 3 pain points when it came to reproducing the paper:
1. Incomplete specification of the way the ""normalization loss"" introduced by
   (Gelada & Bellemere, 2019) was used.
2. Difficulty implementing gradient manipulation required by the algorithm in the
   synchronized RL setting used in the paper.
3. Complexity of the authors' experimental framework.


# Recommendations for reproducibility
The reproducers do not provide explicit recommendations for improving
reproducibility, though there is some advice implicit in their thorough
discussion of the pain points.

# Overall organization and clarity
Good.

# Grammatical issues / organization / proper plots
Good.",19-281.txt
82,https://openreview.net/forum?id=r1xJPpqzTr,0,4,4,"The authors replicated the experiments of the original paper in 1 out of 5 settings. They were able to improve the performance of a baseline in the original paper, and they conducted interesting experiments about one crucial hyper-parameter. However, their implementation is not from scratch but heavily based on the code of the original paper. 

Strengths:

+ In their replication experiments, the authors find that (with minor modifications) a baseline can have better performance than reported in the original paper. 

+ The authors communicated with the authors of the original paper via email and OpenReview.


Weaknesses:

- The authors didn't provide code for the replication experiments. From the report, we know that the authors' implementation was largely based on the code of the original paper. However, the replication track requires participants to implement the replication from scratch.

- The authors have only replicated the experimental results on 1 out of 5 environments in the original paper.

- Sec. 1.1: ""algorithms based on a policy gradient"" is not grammatical. 

- Sec. 1.1: ""Zhang et al. seek an off-policy objective function for the policy gradient to facilitate exploration"". The original paper does not claim exploration to be one of their merits.

- Sec. 1.1: ""Off-Policy Gradient Policy"" should be ""Off-Policy Actor-critic""

- Eqn. 1: This is not the exact formulation used by Degris et al.  The interest function was introduced much later in Sutton et al. [A]

[A] Sutton, Richard S., A. Rupam Mahmood, and Martha White. ""An emphatic approach to the problem of off-policy temporal-difference learning."" The Journal of Machine Learning Research 17.1 (2016): 2603-2631.

- In the abstract, the sentence fragment "", which unifies existing objectives for off-policy gradient algorithms in the continuing reinforcement learning setting .."" is almost identical to a sentence in the abstract of the original paper without proper quotation. In addition, I think ""off-policy gradient algorithms"" should be ""off-policy policy gradient algorithms"", even if that's what is in the original paper.

- Sec. 1.2 doesn't summarize the experiments of the original paper well.
Thanks a lot for your effort in replicating GeoffPAC.
Just some minor comments:
1) The ACE baseline corresponds to Imani et al 2018 instead of Zhang et al 2018. They used the same name ""ACE"" by accident.
2) I think the main issue preventing us from using a large \hat{\gamma} is that when \hat{\gamma} becomes large, the density ratio/covariate shift learning becomes very unstable. In Gelada's work, they show contraction only for small enough \hat{\gamma}. So in the one hand my paper suggests a large \hat{\gamma}, in the other hand, to learn the ratio stably, we need a small \hat{\gamma}. So I think this is the trade-off we need to make.
3) I agree synchronizing gradients across 10 workers is very annoying. I think it is easier to implement GeoffPAC based on A3C instead of A2C so we don't need to synchronize gradients. I was too lazy to find a A3C implementation :<",19-282.txt
83,https://openreview.net/forum?id=Byg5D65GTS,0,5,3,"(I only read the paper and skimmed the code of graph 4+5 https://github.com/vedasunkara/EBUReproducibilityChallenge/blob/master/graph4%2B5/ and ipython notebook https://github.com/vedasunkara/EBUReproducibilityChallenge/blob/master/graph4%2B5/graph.ipynb )

I think the authors made a serious attempt at reproducing EBU, but missed some details. EBU is roughly episodic DQN (one has to mix backward-propagated values from the episode with the existing Q values, with a beta diffusion factor), which makes bootstrapping values to early samples in successful episodes much faster (this seems due to the SGD nature of it, as if new samples were trained on until convergence with one-step DQN, theoretically the values would be propagated as well).

In the reproduction, the authors mixed steps and episode because the EBU paper was unclear: it matters a great deal to only count steps (or frames, but something consistent) for all methods. It is also important to mesure batches consistently across methods in the same unit.

The authors seem to have used BreakoutDeterministic-v4 (from Gym, defaults at https://github.com/openai/gym/blob/master/gym/envs/__init__.py#L635-L641) instead of ALE directly, which may differ slightly (in sticky actions) from ALE's Breakout.

I do not think that this reproduction is correct enough to be published. However, this reproduction work is still worthwhile for may reasons:
 - It unveiled some unclear points from the EBU paper.
 - From the current status of the reproduction effort, they are landed not far from being able to test EBU independently. With modifications, this could be published as reproduction (even if the numbers from EBU don't reproduce once the steps menntioned above are taken).
 - Similarly, it is a starting point for testing EBU further, or extending it.
 ",19-283.txt
83,https://openreview.net/forum?id=Byg5D65GTS,0,7,3,"Problem statement: This report clearly understands and concludes the problem statement and main contributions of the original paper.

Code: The authors implement the algorithm of the original paper from scratch and submit the code.

Communication with original authors: Limited communication with the original paper authors through OpenReview.

Discussion on results: the report evaluates and discusses the reproducibility of the original paper in detail, including Fig. 1 and 2, as well as subsections of Fig. 4 and 5, and analyzes the reasons why the results of Fig. 4 and 5 are not reproduced. The report also gives some suggestions about improving reproducibility, e.g., establishing apt hyperparameter configurations about sampling, batching, memory buffers.

Overall organization and clarity: The report is well written and easy to follow, despite some minor grammar errors, e.g., ""we able able to""in Section 4.2. ",19-284.txt
83,https://openreview.net/forum?id=Byg5D65GTS,0,8,4,"The reproducibility report authors (henceforth referred to as rep-authors) appear to understand the motivations of the EBU paper along with providing a solid attempt at reproducing the code from scratch. rep-authors communicated with Lee et al (referred to as EBU-authors) during attempts to reproduce work as well as in OpenReview.

Pros:
- overall a very solid reproduction effort. Given the difficulties of reproducing some results, it seems that the authors put in a lot of effort to making it work and match hyperparameters and numbers exactly.
- The report and ensuing discussion reveals the sensitivity of the EBU algorithm to replay buffer hyperparameters and minibatch size (for example, the requirement of beta decay used to encourage decorrelation of minibatch statistics and the episode vs. transitions size for memory buffer in MNIST Maze) that was not readily apparent in the paper. I believe the difficulty of reproducing the experiments should warrant an EBU manuscript update that clarifies that these parameters are quite important to get right, and the EBU-authors might want to consider even citing this tech report to show what happens when ""fixed # transitions in replay buffer"" is swapped for ""fixed # episodes in replay buffer"".
- Finding that EBU paper's Nature DQN Atari baseline is substantially worse than the baseline performance reported in the Nature DQN paper (could explain why reproducing DQN performance numbers was difficult) is interesting and to my knowledge the ebu-authors have not responded as to why their baseline’s average test score on Breakout is so low compared to the Nature paper.

Cons:
- rep-authors claim ""We used Tensorflow version 1.14, but believe that there is a possibility that we would get different results with a different version of Tensorflow"" and discrepancies in Fig 1 might be due to different library versions. While such performance regressions are not unheard of, these are generally uncommon and the burden of proof rests on rep-authors to show that library versions have caused differences in performance (a proof by example would be sufficient).
- EBU-authors used the Nature ALE environment implementation while rep-authors used gym’s BreakoutDeterministic-v4 environment (https://jair.org/index.php/jair/article/view/11182). Although rep-authors could have done better to reproduce results by using the exact same ALE environment, it’s still interesting to see that considerably different qualitative results are obtained when switching environment dynamics slightly (gym).
This is SuYoung Lee, the author of Episodic Backward Update.
First of all, we greatly appreciate the authors of reproducibility challenge for a careful reproduce and constructive comments for future work.
We apologize for some missing descriptions about the episodic buffer and batching that caused difficulties in reproducing. 
Although most of these experiment setups can be inferred from our code online, we strongly agree to the authors of the reproducibility challenge that Episodic buffer and batching should have been discussed more in detail in the manuscript as below.
Episodic buffer: sample episode without the very first and last episode in the buffer --> we may have a replay buffer with a fixed step-size
Batching: followed the Nature DQN, divide the sampled episode into pieces of equal size, size=minibatch size (350 for the MNISTMaze, 32 for Atari). The last minibatch of each episode less than the minibatch size was filled with transitions from another episode to make each minibatch size equal.

---------------------
We would like to share some concerns (mostly caused because of the lack of descriptions in our paper) we have about the reproduce methods.

1. MNIST maze:
As discussed by the reproduced report, the buffer size is a very important hyperparameter, we should fix it for all baselines to compare.
Two types of fair comparison can be made by fixing buffer size in steps or in episodes.
1)one-step DQN ( buffer size=30,000 steps), EBU (buffer size=30,000 steps), n-step DQN (buffer size = 30,000 steps)
2)one-step DQN ( buffer size=170 episodes), EBU (buffer size=170 episodes), n-step DQN (buffer size = 170 episodes)

Our manuscript only makes a comparison in terms of 1), I think it would be also interesting to compare using 2).
However, the reproduced results compare the three algorithms in the following configuration.
3)one-step DQN ( buffer size=170 episodes), EBU (buffer size=30,000 steps), n-step DQN (buffer size = 170 episodes)

DQN may have outperformed the other baselines because of relatively larger number of steps in the replay memory at the end.
Using average step length 175 throughout the entire training time is unfair because at the end of training, the episodes length reduces to about 20~30 for DQN. Then DQN contains about 1000~1200 episodes in its buffer (EBU and N-step Q will have buffer size far less than 10,000 steps in general).
170 episodes will be more biased to return biased value estimates to sub-optimal policy. 

Normally all methods should converge to relative length of 1.
The problem not converging to 1 is most likely due to the stability issue caused by too small replay buffer.

2. Atari
As stated multiple times in the manuscript and the appendix, we used the exactly same set of hyperparameters and evaluation methods that are used in Nature DQN.

It is very important to match the version of Atari games. 
We stated in the manuscript we used the Arcade Learning Environment (ALE) by Bellemare et al.
Using ALE Atari returns a significantly different result compared to the gym Atari.
The Nature DQN uses the ALE version, so it is important that we use the same ALE Atari instead of gym Atari.

The randomness for evaluating robustness is also important, whether to use no-ops evaluation or the sticky-key evaluation.
Since the the Nature DQN with ALE only uses 30 no-ops evaluation, it is clear that no sticky key evaluations are used.
Breakout-v0~Breakout-v4 is for the gym environment so we don't have to consider about sticky-key evaluation methods.

In the Nature DQN paper, it says The number of frames over which the initial value of epsilon is linearly annealed to the final value = 1M.
The authors of DQN actually meant ""Steps"" for ""frames"".
The authors of Nature DQN uses ""Steps"" and ""Frames"" interchangeably in the hyperparameter section which causes a huge confusion.
So it is highly recommended to look at the original code by the authors which is uploaded along with the Nature DQN paper (written in LUA).
You can check the epsilon decaying curriculum of the authors that uses 1M STEPS (4M frames) to decay.

Since we used the same hyperparameters of Nature DQN, as stated in the appendix, the batch size is also 32  (same number of updates).
We found that the reproduced code uses a minibatch size of 5000 that is too large compared to 32 which will malfunction in most cases.
Such a large step-size induces varying size of batches at every update, where most practical deep learning methods use a fixed-size minibatch size.
In the beginning of the sampled episode or in the early epochs of training (all episodes are very short in length, much shorter than 5000 frames) we may have sample trajectories with length shorter than 5000.


---------------------
Once again, we greatly appreciate the authors of this challenge for their work despite the difficulties and some missing explanations about the experimental setups.
We will definitely refer to your constructive comments for our future work.",19-285.txt
